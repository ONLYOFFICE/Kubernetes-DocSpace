# Default values for subchart Onlyoffice Docs
# ref: https://github.com/ONLYOFFICE/Kubernetes-Docs/blob/master/values.yaml
#
docs:
  # docs.enabled Enables Onlyoffice Docs subchart installation
  # Set to `false` if you plan to use the already installed Onlyoffice Docs or install DocSpace without it
  enabled: true
  connections:
    # Onlyoffice Docs Database connection parameters
    # By default, the same Database connection is used as for DocSpace
    # docs.connections.dbType The Database type
    dbType: mysql
    # docs.connections.dbHost The IP address or the name of the Database host
    dbHost: mysql
    # docs.connections.dbUser Database user
    dbUser: onlyoffice_user
    # docs.connections.dbPort Database server port number
    dbPort: "3306"
    # docs.connections.dbName Name of the Database database the application will be connected with
    # The database must already exist
    dbName: docspace
    # docs.connections.dbExistingSecret Name of existing secret to use for Database password
    # Must contain the key specified in `docs.connections.dbSecretKeyName`
    dbExistingSecret: mysql
    # docs.connections.dbSecretKeyName The name of the key that contains the Database user password
    # If you set a password in `docs.connections.dbPassword`, a secret will be automatically created, the key name of which will be the value set here
    dbSecretKeyName: mysql-password
    # docs.connections.dbPassword Database user password
    # If set to, it takes priority over the `docs.connections.dbExistingSecret`
    dbPassword: ""
    # Onlyoffice Docs Redis connection parameters
    # By default, the same Redis connection is used as for DocSpace, except for `docs.connections.redisDBNum`
    # docs.connections.redisHost The IP address or the name of the Redis host
    redisHost: redis-master
    # docs.connections.redisPort The Redis server port number
    redisPort: "6379"
    # docs.connections.redisUser The Redis user name
    # ref: https://redis.io/docs/management/security/acl/
    redisUser: default
    # docs.connections.redisDBNum Number of the Redis logical database to be selected
    # ref: https://redis.io/commands/select/
    redisDBNum: "1"
    # docs.connections.redisExistingSecret Name of existing secret to use for Redis password
    # Must contain the key specified in `docs.connections.redisSecretKeyName`
    redisExistingSecret: redis
    # docs.connections.redisSecretKeyName The name of the key that contains the Redis user password
    # If you set a password in `docs.connections.redisPassword`, a secret will be automatically created, the key name of which will be the value set here
    redisSecretKeyName: redis-password
    # docs.connections.redisPassword The password set for the Redis account
    # If set to, it takes priority over the `docs.connections.redisExistingSecret`
    redisPassword: ""
    # docs.connections.redisNoPass Defines whether to use a Redis auth without a password
    # If the connection to Redis server does not require a password, set the value to `true`
    redisNoPass: false
    # Onlyoffice Docs RabbitMQ connection parameters
    # By default, the same RabbitMQ connection is used as for DocSpace
    # docs.connections.amqpType Defines the AMQP server type
    amqpType: rabbitmq
    # docs.connections.amqpHost The IP address or the name of the AMQP server
    amqpHost: rabbitmq
    # docs.connections.amqpPort The port for the connection to AMQP server
    amqpPort: "5672"
    # docs.connections.amqpVhost The virtual host for the connection to AMQP server
    amqpVhost: "/"
    # docs.connections.amqpUser The username for the AMQP server account
    amqpUser: user
    # docs.connections.amqpProto The protocol for the connection to AMQP server
    amqpProto: amqp
    # docs.connections.amqpExistingSecret The name of existing secret to use for AMQP server password
    # Must contain the key specified in `docs.connections.amqpSecretKeyName`
    amqpExistingSecret: rabbitmq
    # docs.connections.amqpSecretKeyName The name of the key that contains the AMQP server user password
    # If you set a password in `docs.connections.amqpPassword`, a secret will be automatically created, the key name of which will be the value set here
    amqpSecretKeyName: rabbitmq-password
    # docs.connections.amqpPassword AMQP server user password
    # If set to, it takes priority over the `docs.connections.amqpExistingSecret`
    amqpPassword: ""
  # docs.service.port ONLYOFFICE Docs service port
  service:
    port: "80"
  # docs.license.existingClaim Name of the existing PVC in which the license is stored
  # Must contain the file `license.lic`
  # By default, a PVC is connected, in which a license is added when using DocSpace Enterprise
  license:
    existingClaim: "docspace-data"
  # docs.jwt.existingSecret The name of an existing secret containing variables for jwt
  # If not specified, a secret named `jwt` will be created
  # By default, the jwt secret is used, which will be created with values from the jwt DocSpace
  jwt:
    existingSecret: "docspace-jwt"
  # docs.docservice.image.repository Docservice container image repository
  # Depending on your license type, add the suffix "-de" - Developer Edition or "-ee" Enterprise Edition
  # By default - Community version
  docservice:
    image:
      repository: onlyoffice/docs-docservice
  # docs.proxy.image.repository Proxy container image repository
  # Depending on your license type, add the suffix "-de" - Developer Edition or "-ee" Enterprise Edition
  # By default - Community version
  proxy:
    image:
      repository: onlyoffice/docs-proxy
  # docs.converter.image.repository Converter container image repository
  # Depending on your license type, add the suffix "-de" - Developer Edition or "-ee" Enterprise Edition
  # By default - Community version
  converter:
    image:
      repository: onlyoffice/docs-converter
  # docs.upgrade.job.enabled Enable the execution of job Docs pre-upgrade before upgrading
  # Set to `false` when upgrading to version `3.0.0` from earlier
  # When installing `3.0.0` and also when upgrading from version `3.0.0` to a later one, this parameter should be set to `true`
  upgrade:
    job:
      enabled: true
  # docs.clearCache.job.enabled Enable the execution of job Docs Clear Cache after upgrading
  # Set to `false` when upgrading to version `3.0.0` from earlier
  # When installing `3.0.0` and also when upgrading from version `3.0.0` to a later one, this parameter should be set to `true`
  clearCache:
    job:
      enabled: true

# Default values for DocSpace

## product.name Specifies name of the product
## This is a service variable. You don't need to change it
product:
  name: onlyoffice

# DocSpace common parameters
# This block defines common parameters for all DocSpace Apps services
#
# Connection parameters to services
connections:
  # connections.envExtension Defines whether an environment will be used
  envExtension: ""
  # connections.installationType Defines solution type
  installationType: COMMUNITY
  # connections.migrationType Defines migration type
  migrationType: STANDALONE
  # connections.mysqlDatabaseMigration Enables database migration
  mysqlDatabaseMigration: "false"
  # connections.mysqlHost The IP address or the name of the Database host
  mysqlHost: mysql
  # connections.mysqlPort Database server port number
  mysqlPort: "3306"
  # connections.mysqlDatabase Name of the Database the application will be connected with
  # The database must already exist
  mysqlDatabase: docspace
  # connections.mysqlUser Database user
  mysqlUser: onlyoffice_user
  # connections.mysqlPassword Database user password
  # If set to, it takes priority over the `connections.mysqlExistingSecret`
  mysqlPassword: ""
  # connections.mysqlExistingSecret Name of existing secret to use for Database passwords
  # Must contain the key specified in `connections.mysqlSecretKeyPassword`
  mysqlExistingSecret: mysql
  # connections.mysqlSecretKeyPassword The name of the key that contains the Database user password
  # If you set a password in `connections.mysqlPassword`, a secret will be automatically created, the key name of which will be the value set here
  mysqlSecretKeyPassword: mysql-password
  # connections.redisHost The IP address or the name of the Redis host
  # If Redis is deployed inside a k8s cluster, then you need to specify the FQDN name of the service
  # ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#services
  redisHost: redis-master.default.svc.cluster.local
  # connections.redisPort The Redis server port number
  redisPort: "6379"
  # connections.redisUser The Redis user name
  # ref: https://redis.io/docs/management/security/acl/
  redisUser: default
  ## connections.redisDB Number of the redis logical database to be selected
  ## ref: https://redis.io/commands/select/
  redisDB: "0"
  # connections.redisExistingSecret Name of existing secret to use for Redis password
  # Must contain the key specified in `connections.redisSecretKeyName`
  redisExistingSecret: redis
  # connections.redisSecretKeyName The name of the key that contains the Redis user password
  # If you set a password in `connections.redisPassword`, a secret will be automatically created, the key name of which will be the value set here
  redisSecretKeyName: redis-password
  # connections.redisPassword The password set for the Redis account
  # If set to, it takes priority over the `connections.redisExistingSecret`
  redisPassword: ""
  # connections.redisNoPass Defines whether to use a Redis auth without a password
  # If the connection to Redis server does not require a password, set the value to `true`
  redisNoPass: false
  # connections.brokerHost The IP address or the name of the Broker host
  brokerHost: rabbitmq
  # connections.brokerPort The port for the connection to Broker host
  brokerPort: "5672"
  # connections.brokerVhost The virtual host for the connection to Broker host
  brokerVhost: "/"
  # connections.brokerUser The username for the Broker account
  brokerUser: user
  # connections.brokerProto The protocol for the connection to Broker host
  brokerProto: amqp
  # connections.brokerUri A string containing the necessary connection parameters to Broker
  # If set to, it takes priority
  brokerUri: ""
  # connections.brokerExistingSecret The name of existing secret to use for Broker password
  # Must contain the key specified in `connections.brokerSecretKeyName`
  brokerExistingSecret: rabbitmq
  # connections.brokerSecretKeyName The name of the key that contains the Broker user password
  # If you set a password in `connections.brokerPassword`, a secret will be automatically created, the key name of which will be the value set here
  brokerSecretKeyName: rabbitmq-password
  # connections.brokerPassword Broker user password
  # If set to, it takes priority over the `connections.brokerExistingSecret`
  brokerPassword: ""
  # connections.elkSheme The protocol for the connection to Opensearch
  elkSheme: http
  # connections.elkHost The IP address or the name of the Opensearch host
  elkHost: opensearch
  # connections.elkPort The port for the connection to Opensearch
  elkPort: "9200"
  # connections.elkThreads Number of threads in Opensearch
  elkThreads: "1"
  # connections.apiHost The name of the DocSpace Api service
  apiHost: api
  # connections.apiSystemHost The name of the DocSpace Api System service
  apiSystemHost: api-system
  # connections.notifyHost The name of the DocSpace Notify service
  notifyHost: notify
  # connections.studioNotifyHost The name of the DocSpace Studio Notify service
  studioNotifyHost: studio-notify
  # connections.socketHost The name of the DocSpace Socket service
  socketHost: socket
  # connections.peopleServerHost The name of the DocSpace People Server service
  peopleServerHost: people-server
  # connections.filesHost The name of the DocSpace Files service
  filesHost: files
  # connections.filesServicesHost The name of the DocSpace Files Services service
  filesServicesHost: files-services
  # connections.studioHost The name of the DocSpace Studio service
  studioHost: studio
  # connections.backupHost The name of the DocSpace Backup service
  backupHost: backup
  # connections.ssoauthHost The name of the DocSpace SSO service
  ssoauthHost: ssoauth
  # connections.clearEventsHost The name of the DocSpace Clear Events service
  clearEventsHost: clear-events
  # connections.doceditorHost The name of the DocSpace Doceditor service
  doceditorHost: doceditor
  # connections.backupBackgroundTasksHost The name of the DocSpace Backup Background Tasks service
  backupBackgroundTasksHost: backup-background-tasks
  # connections.loginHost The name of the DocSpace Login service
  loginHost: login
  # connections.healthchecksHost The name of the DocSpace Healthchecks service
  healthchecksHost: healthchecks
  # connections.identityApiHost The name of the DocSpace Identity Api service
  identityApiHost: identity-api
  # connections.identityAuthorizationHost The name of the DocSpace Identity service
  identityAuthorizationHost: identity-authorization
  # connections.documentServerHost The name of the Document Server service
  # Used when installing a local Document Server (by default `docs.enabled=true`)
  # If you want to install DocSpace without Document Server then specify an empty value
  documentServerHost: documentserver
  # connections.documentServerUrlExternal The address of the external Document Server
  # If set, the local Document Server will not be installed
  # The address must be in the `http(s)://<documentserver-address>/` format
  documentServerUrlExternal: ""
  # connections.appUrlPortal URL for DocSpace requests
  # By default, the name of the routing (Router) service and the port on which it accepts requests are used
  appUrlPortal: "http://router:8092"
  # connections.appCoreBaseDomain The base domain on which the DocSpace will be available
  appCoreBaseDomain: localhost
  appCoreMachinekey:
    # connections.appCoreMachinekey.secretKey The secret key used in the DocSpace
    secretKey: "your_core_machinekey"
    # connections.appCoreMachinekey.existingSecret The name of an existing secret containing Core Machine Key
    # Must contain the `APP_CORE_MACHINEKEY` key
    # If not specified, a secret will be created with the value set in `connections.appCoreMachinekey.secretKey`
    existingSecret: ""
  # connections.countWorkerConnections Defines the nginx config worker_connections directive for routing (Router) service
  # ref: https://nginx.org/en/docs/ngx_core_module.html#worker_connections
  countWorkerConnections: "1024"
  # connections.nginxSnvsubstTemplateSuffix A suffix of template files for rendering nginx configs in routing (Router) service
  nginxSnvsubstTemplateSuffix: ".template"
  # connections.wrongPortalNameURL
  wrongPortalNameURL: ""
  # connections.oauthOrigin The address of the OAUTH2 server
  oauthOrigin: ""
  # connections.appKnownNetworks Defines the address ranges of known networks to accept forwarded headers from for DocSpace services
  # In particular, the networks in which the proxies that you are using in front of DocSpace services are located should be indicated here
  # Provide IP ranges using CIDR notation
  # Example:
  # appKnownNetworks: "10.244.0.0/16,10.245.0.0/16"
  appKnownNetworks: "10.244.0.0/16"
  # connections.appKnownProxies Deprecated parameter. Use `connections.appKnownNetworks` instead
  appKnownProxies: ""
  # connections.oauthRedirectURL Address of the oauth authorization server
  oauthRedirectURL: "https://service.onlyoffice.com/oauth2.aspx"
# namespaceOverride The name of the namespace in which DocSpace will be deployed
# If not set, the name will be taken from .Release.Namespace
namespaceOverride: ""
# commonLabels Defines labels that will be additionally added to all the deployed resources
# You can also use `tpl` as the value for the key
# ref: https://helm.sh/docs/chart_best_practices/labels/
# Example:
# commonLabels:
#   app.kubernetes.io/name: "{{ .Chart.Name }}"
#   helm.sh/chart: '{{ .Chart.Name }}-{{ .Chart.Version | replace "+" "_" }}'
#   app.kubernetes.io/managed-by: "{{ .Release.Service }}"
#   app.kubernetes.io/instance: "{{ .Release.Name }}"
#   app.kubernetes.io/version: "{{ .Chart.AppVersion }}"
commonLabels: {}
# commonAnnotations Defines annotations that will be additionally added to all the deployed resources
# You can also use `tpl` as the value for the key
# Some resources may override the values specified here with their own
# Example:
# commonAnnotations:
#   "key1": "value1"
#   "key2": "{{ value2 }}"
commonAnnotations: {}
# podAnnotations Map of annotations to add to the DocSpace pods
# Each Docspace application can override the values specified here with its own
podAnnotations:
  rollme: "{{ randAlphaNum 5 | quote }}"
# Service account parameters
# ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
serviceAccount:
  # serviceAccount.create Enable ServiceAccount creation
  create: false
  # serviceAccount.name Name of the ServiceAccount to be used
  # If not set and `serviceAccount.create` is `true` the name will be taken from .Release.Name
  # If not set and `serviceAccount.create` is `false` the name will be "default"
  name: ""
  # serviceAccount.annotations Map of annotations to add to the ServiceAccount
  annotations: {}
  # serviceAccount.automountServiceAccountToken Enable auto mount of ServiceAccountToken on the serviceAccount created
  # Used only if `serviceAccount.create` is `true`
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#opt-out-of-api-credential-automounting
  automountServiceAccountToken: true
# Configure a Security Context for the DocSpace application Pods
# Each Docspace application can override the values specified here with its own
# Individual values for `docs` and `elasticsearch`
# ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
podSecurityContext:
  # podSecurityContext.enabled Enable security context for the pods
  # If set to true, `podSecurityContext` is enabled for all resources describing the podTemplate
  enabled: false
  # podSecurityContext.fsGroup Defines the Group ID to which the owner and permissions for all files in volumes are changed when mounted in the DocSpace application Pods
  fsGroup: 107
# Configure a Security Context for containers in DocSpace application Pods
# Each Docspace application can override the values specified here with its own
# Individual values for `docs` and `elasticsearch`
containerSecurityContext:
  # containerSecurityContext.enabled Enable security context for containers in DocSpace application pods
  enabled: false
  # containerSecurityContext.runAsUser User ID for the DocSpace application containers
  runAsUser: 104
  # containerSecurityContext.runAsGroup Group ID for the DocSpace application containers
  runAsGroup: 107
  # containerSecurityContext.runAsNonRoot Require that the container will run with a user with UID other than 0
  runAsNonRoot: true
  # containerSecurityContext.allowPrivilegeEscalation Controls whether a process can gain more privileges than its parent process
  allowPrivilegeEscalation: false
  # containerSecurityContext.seLinuxOptions Defines SELinux labels for the DocSpace application containers
  seLinuxOptions: {}
  # containerSecurityContext.seccompProfile Defines the Seccomp profile for the DocSpace application containers
  seccompProfile:
    type: RuntimeDefault
  # containerSecurityContext.capabilities Defines the privileges granted to the process
  capabilities:
    drop: ["ALL"]
# nodeSelector Node labels for DocSpace application pods assignment
# Each Docspace application can override the values specified here with its own
# ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
nodeSelector: {}
# tolerations Tolerations for DocSpace application pods assignment
# Each Docspace application can override the values specified here with its own
# ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
tolerations: []
# imagePullSecrets Container image registry secret name
# ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: ""
# Global image parameters for for all DocSpace applications
# Does not apply to the Document Server, Elasticsearch and Proxy Frontend
images:
  # images.repoPrefix Global repository prefix for all DocSpace applications
  repoPrefix: ""
  # images.tag Global image tag for all DocSpace applications
  # Does not apply to the Document Server and Elasticsearch
  tag: 3.0.1
# JSON Web Token parameters
jwt:
  # jwt.enabled Specifies the enabling the JSON Web Token validation by the DocSpace
  enabled: true
  # jwt.secret Defines the secret key to validate the JSON Web Token in the request to the DocSpace
  secret: "jwt_secret"
  # jwt.header Defines the http header that will be used to send the JSON Web Token
  header: "AuthorizationJwt"
  # jwt.inBody Specifies the enabling the token validation in the request body to the DocSpace
  inBody: false
  # jwt.existingSecret The name of an existing secret containing variables for jwt
  # If not specified, a secret named `jwt` will be created
  existingSecret: ""
# Configs for overriding default values and additional configuration files for DocSpace
extraConf:
  # extraConf.secretName The name of the Secret containing the json files that override the default values and additional configuration files
  secretName: ""
  # extraConf.filename The name of the json files that contains custom values and name additional configuration files
  # Must be the same as the `key` name in `extraConf.secretName`
  # May contain multiple values
  fileName:
    - appsettings.test.json
# log.level Defines the type and severity of a logged event
log:
  level: "Warning"
# debug.enabled Enable debug
debug:
  enabled: "false"
# nodeOptions
nodeOptions: "--max_old_space_size=4096"
# DocSpace Init Containers parameters
# Containers that run before containers in a Pods
# ref:https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
initContainers:
  # Parameters of the Check DB initContainers
  checkDB:
    image:
      # initContainers.checkDB.image.repository check-db initContainer image repository
      repository: onlyoffice/docs-utils
      # initContainers.checkDB.image.tag check-db initContainer image tag
      tag: 8.2.2-1
      # initContainers.checkDB.image.pullPolicy check-db initContainer image pull policy
      pullPolicy: IfNotPresent
    # check-db initContainer resource requests and limits
    # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    # initContainers.checkDB.resources.requests The requested resources for the check-db initContainer
    # initContainers.checkDB.resources.limits The resources limits for the check-db initContainer
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "1Gi"
        cpu: "1000m"
  # Parameters of the Wait Storage initContainers
  waitStorage:
    image:
      # initContainers.waitStorage.image.repository app-wait-storage initContainer image repository
      repository: onlyoffice/docspace-wait-bin-share
      # initContainers.waitStorage.image.tag app-wait-storage initContainer image tag
      # If set to, it takes priority over the `images.tag`
      tag: ""
      # initContainers.waitStorage.image.pullPolicy app-wait-storage initContainer image pull policy
      pullPolicy: IfNotPresent
    # app-wait-storage initContainer resource requests and limits
    # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    # initContainers.waitStorage.resources.requests The requested resources for the app-wait-storage initContainer
    # initContainers.waitStorage.resources.limits The resources limits for the app-wait-storage initContainer
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "1Gi"
        cpu: "1000m"
  # Parameters of the Init Storage initContainers
  initStorage:
    image:
      # initContainers.initStorage.image.repository app-init-storage initContainer image repository
      repository: onlyoffice/docspace-bin-share
      # initContainers.initStorage.image.tag app-init-storage initContainer image tag
      # If set to, it takes priority over the `images.tag`
      tag: ""
      # initContainers.initStorage.image.pullPolicy app-init-storage initContainer image pull policy
      pullPolicy: IfNotPresent
    # app-init-storage initContainer resource requests and limits
    # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    # initContainers.initStorage.resources.requests The requested resources for the app-init-storage initContainer
    # initContainers.initStorage.resources.limits The resources limits for the app-init-storage initContainer
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "2Gi"
        cpu: "1000m"
  # initContainers.custom Custom initContainers parameters
  # Does not apply to the following services: `Docs`, `Router`, `Opensearch` and `Proxy Frontend`. The corresponding individual parameters are used for them
  # Example:
  # custom:
  #   - name: additional-init-container
  #     image: busybox:latest
  #     command: ['chown', '-R', '104:107', '/app/onlyoffice/data']
  #     volumeMounts:
  #     - name: docspace-data
  #       mountPath: /app/onlyoffice/data
  custom: []
# DocSpace persistence parameters
persistence:
  # persistence.storageClass PVC Storage Class for DocSpace data volume
  storageClass: "nfs"
  docspaceData:
    # persistence.docspaceData.existingClaim The name of the existing PVC for storing files common to all services
    # If not specified, a PVC named "docspace-data" will be created
    existingClaim: ""
    # persistence.docspaceData.annotations Defines annotations that will be additionally added to common files PVC
    # If set to, it takes priority over the `commonAnnotations`
    # You can also use `tpl` as the value for the key
    annotations: {}
    # persistence.docspaceData.size PVC Storage Request for common files volume
    size: 8Gi
  filesData:
    # persistence.filesData.existingClaim The name of the existing PVC for use in the Files service
    # If not specified, a PVC named "files-data" will be created
    existingClaim: ""
    # persistence.filesData.annotations Defines annotations that will be additionally added to Files PVC
    # If set to, it takes priority over the `commonAnnotations`
    # You can also use `tpl` as the value for the key
    annotations: {}
    # persistence.filesData.size PVC Storage Request for Files volume
    size: 2Gi
  peopleData:
    # persistence.peopleData.existingClaim The name of the existing PVC for use in the People Server service
    # If not specified, a PVC named "people-data" will be created
    existingClaim: ""
    # persistence.peopleData.annotations Defines annotations that will be additionally added to People Server PVC
    # If set to, it takes priority over the `commonAnnotations`
    # You can also use `tpl` as the value for the key
    annotations: {}
    # persistence.peopleData.size PVC Storage Request for People Server volume
    size: 2Gi
  routerLog:
    # persistence.routerLog.existingClaim The name of the existing PVC for storing Nginx logs of the Router service
    # If not specified, a PVC named "router-log" will be created
    existingClaim: ""
    # persistence.routerLog.annotations Defines annotations that will be additionally added to Nginx logs PVC
    # If set to, it takes priority over the `commonAnnotations`
    # You can also use `tpl` as the value for the key
    annotations: {}
    # persistence.routerLog.size PVC Storage Request for Nginx logs volume
    size: 5Gi
# Pod anti-affinity parameters
# Pod anti-affinity prohibits at all (required) or, if possible (preferred), placing a second pod with the same label on the same node
# Does not apply to `Docs` and `Opensearch`
# ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
podAntiAffinity:
  # podAntiAffinity.type Types of Pod antiaffinity. Allowed values: `preferred` or `required`
  type: "preferred"
  # podAntiAffinity.topologyKey Node label key to match
  topologyKey: kubernetes.io/hostname
  # podAntiAffinity.weight Priority when selecting node. It is in the range from 1 to 100. Used only when `podAntiAffinity.type=preferred`
  weight: "100"

# DocSpace Files application parameters
# This block defines the parameters common to all the Pods of this application
#
files:
  # files.enabled Enables Files installation
  enabled: true
  # files.kind The controller used for deploy
  # Possible values are `Deployment` (default) or `StatefulSet`
  kind: Deployment
  # files.annotations Defines annotations that will be additionally added to Files deploy
  # If set to, it takes priority over the `commonAnnotations`
  # You can also use `tpl` as the value for the key
  annotations: {}
  # files.replicaCount Number of Files replicas to deploy
  # If the `files.autoscaling.enabled` parameter is enabled, it is ignored
  replicaCount: 2
  # Update strategy used to replace old Pods by new ones
  updateStrategy:
    # files.updateStrategy.type Files update strategy type
    # Allowed values: `RollingUpdate` or `OnDelete` if `files.kind` is set to `StatefulSet` and `RollingUpdate` or `Recreate` if `files.kind` is set to `Deployment`
    type: RollingUpdate
    # files.updateStrategy.rollingUpdate Used only when `files.updateStrategy.type=RollingUpdate`
    rollingUpdate:
      # files.updateStrategy.rollingUpdate.maxUnavailable Maximum number of Files Pods unavailable during the update process
      maxUnavailable: 25%
      # files.updateStrategy.rollingUpdate.maxSurge Maximum number of Files Pods created over the desired number of Pods
      maxSurge: 25%
  # files.podManagementPolicy The Files Pods scaling operations policy
  # Used if `files.kind` is set to `StatefulSet`
  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  podManagementPolicy: OrderedReady
  # files.podAnnotations Map of annotations to add to the Files pods
  podAnnotations: {}
  # Configure a Security Context for the Files Pods
  # If set to, it takes priority over the `podSecurityContext`
  podSecurityContext:
    # files.podSecurityContext.enabled Enable security context for the Files pods
    enabled: false
  # files.customPodAntiAffinity Prohibiting the scheduling of Files Pods relative to other Pods containing the specified labels on the same node
  # Example:
  # customPodAntiAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #   - labelSelector:
  #       matchExpressions:
  #       - key: app
  #         operator: In
  #         values:
  #         - files-services
  #     topologyKey: kubernetes.io/hostname
  customPodAntiAffinity: {}
  # Pod affinity rules for Files Pods scheduling by nodes relative to other Pods
  # Pod affinity allow you to constrain which nodes Files Pods can be scheduled on based on the labels of Pods already running on that node
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # Example:
  # podAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #   - labelSelector:
  #       matchExpressions:
  #       - key: app
  #         operator: In
  #         values:
  #         - store
  #     topologyKey: topology.kubernetes.io/zone
  podAffinity: {}
  # Node affinity rules for Files Pods scheduling by nodes
  # Node affinity allow you to constrain which nodes Files Pods can be scheduled on based on node labels
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  # Example:
  # nodeAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #     - matchExpressions:
  #       - key: topology.kubernetes.io/zone
  #         operator: In
  #         values:
  #         - zone1
  #         - zone2
  nodeAffinity: {}
  # files.nodeSelector Node labels for Files pods assignment
  # If set to, it takes priority over the `nodeSelector`
  nodeSelector: {}
  # files.tolerations Tolerations for Files pods assignment
  # If set to, it takes priority over the `tolerations`
  tolerations: []
  ## Horizontal Pod Autoscaling parameters
  ## Horizontal Pod Autoscaling is used for autoscaling of the Files deployment
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## The parameters below for autoscaling are used only when `files.autoscaling.enabled=true`
  autoscaling:
    ## files.autoscaling.enabled Enable Files deployment autoscaling
    enabled: false
    ## files.autoscaling.annotations Defines annotations that will be additionally added to Files deployment HPA
    ## If set to, it takes priority over the `commonAnnotations`
    ## You can also use `tpl` as the value for the key
    annotations: {}
    ## files.autoscaling.minReplicas Files deployment autoscaling minimum number of replicas
    minReplicas: 2
    ## files.autoscaling.maxReplicas Files deployment autoscaling maximum number of replicas
    maxReplicas: 16
    targetCPU:
      ## files.autoscaling.targetCPU.enabled Enable autoscaling of Files deployment by CPU usage percentage
      enabled: true
      ## files.autoscaling.targetCPU.utilizationPercentage Files deployment autoscaling target CPU percentage
      utilizationPercentage: 70
    targetMemory:
      ## files.autoscaling.targetMemory.enabled Enable autoscaling of Files deployment by memory usage percentage
      enabled: false
      ## files.autoscaling.targetMemory.utilizationPercentage Files deployment autoscaling target memory percentage
      utilizationPercentage: 70
    ## files.autoscaling.customMetricsType Custom, additional or external autoscaling metrics for the Files deployment
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
    ## Example:
    ## customMetricsType:
    ##   - type: Pods
    ##     pods:
    ##       metric:
    ##         name: packets-per-second
    ##       target:
    ##         type: AverageValue
    ##         averageValue: 1k
    customMetricsType: []
    ## files.autoscaling.behavior Configuring Files deployment scaling behavior policies for the `scaleDown` and `scaleUp` fields
    ## If not set the default values are used:
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#default-behavior
    ## Example:
    ## behavior:
    ##   scaleDown:
    ##     stabilizationWindowSeconds: 300
    ##     policies:
    ##     - type: Percent
    ##       value: 10
    ##       periodSeconds: 60
    ##   scaleUp:
    ##     stabilizationWindowSeconds: 0
    ##     policies:
    ##     - type: Percent
    ##       value: 10
    ##       periodSeconds: 15
    ##     - type: Pods
    ##       value: 2
    ##       periodSeconds: 15
    ##     selectPolicy: Max
    behavior: {}
  # Files container image parameters
  image:
    # files.image.repository files container image repository
    repository: onlyoffice/docspace-files
    # files.image.tag files container image tag
    # If set to, it takes priority over the `images.tag`
    tag: ""
    # files.image.pullPolicy files container image pull policy
    pullPolicy: IfNotPresent
  # Configure a Security Context for containers in Files Pods
  # If set to, it takes priority over the `containerSecurityContext`
  containerSecurityContext:
    # files.containerSecurityContext.enabled Enable security context for containers in Files pods
    enabled: false
  # files.lifecycleHooks Defines the Files container lifecycle hooks
  # ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
  # It is used to trigger events to run at certain points in a container's lifecycle
  # There are two hooks that are exposed: `PostStart` and `PreStop`
  # Example:
  # lifecycleHooks:
  #   preStop:
  #     exec:
  #       command: ["/bin/sh", "-c", "sleep 25"]
  lifecycleHooks: {}
  # files.containerPorts.app files container port
  containerPorts:
    app: 5050
  # Probe used for the files container: startup, readiness and liveness probes
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # The parameters below for startup probes are used only when `files.startupProbe.enabled=true`
  startupProbe:
    # files.startupProbe.enabled Enable startupProbe for files container
    enabled: true
    httpGet:
      # files.startupProbe.httpGet.path Checking the path for startupProbe
      path: /health
      # files.startupProbe.httpGet.port Checking the port for startupProbe
      port: 5050
    # files.startupProbe.failureThreshold Failure threshold for startupProbe
    failureThreshold: 30
    # files.startupProbe.periodSeconds Period seconds for startupProbe
    periodSeconds: 10
  # The parameters below for readiness probes are used only when `files.readinessProbe.enabled=true`
  readinessProbe:
    # files.readinessProbe.enabled Enable readinessProbe for files container
    enabled: true
    # files.readinessProbe.failureThreshold Failure threshold for readinessProbe
    failureThreshold: 2
    httpGet:
      # files.readinessProbe.httpGet.path Checking the path for readinessProbe
      path: /health
      # files.readinessProbe.httpGet.port Checking the port for readinessProbe
      port: 5050
    # files.readinessProbe.periodSeconds Period seconds for readinessProbe
    periodSeconds: 20
    # files.readinessProbe.successThreshold Success threshold for readinessProbe
    successThreshold: 1
    # files.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    timeoutSeconds: 5
  # The parameters below for liveness probes are used only when `files.livenessProbe.enabled=true`
  livenessProbe:
    # files.livenessProbe.enabled Enable livenessProbe for files container
    enabled: true
    # files.livenessProbe.failureThreshold Failure threshold for livenessProbe
    failureThreshold: 3
    httpGet:
      # files.livenessProbe.httpGet.path Checking the path for livenessProbe
      path: /health
      # files.livenessProbe.httpGet.port Checking the port for livenessProbe
      port: 5050
    # files.livenessProbe.periodSeconds Period seconds for livenessProbe
    periodSeconds: 20
    # files.livenessProbe.successThreshold Success threshold for livenessProbe
    successThreshold: 1
    # files.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    timeoutSeconds: 5
  # files container resource requests and limits
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  # files.resources.requests The requested resources for the files container
  # files.resources.limits The resources limits for the files container
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "4Gi"
      cpu: "4000m"
  ## files.extraEnvVars An array with extra env variables for the files container
  ## Example:
  ## extraEnvVars:
  ##   - name: my_env
  ##     value: "my_value"
  extraEnvVars: []
  ## files.extraVolumes An array with extra volumes for the files container
  ## Example:
  ## extraVolumes:
  ##   - name: my-volume
  ##     configMap:
  ##       name: my-cm
  extraVolumes: []
  ## files.extraVolumeMounts An array with extra volume mounts for the files container
  ## Example:
  ## extraVolumeMounts:
  ##   - name: my-volume
  ##     mountPath: /my_dir/my_cm.txt
  ##     subPath: my_cm.txt
  extraVolumeMounts: []
  # files.mysqlUser Database user who will be used by the Files service to connect to the database
  # If not set, `connections.mysqlUser` will be used
  # If set to, it takes priority over the `connections.mysqlUser`
  mysqlUser: ""

# DocSpace People Server application parameters
# This block defines the parameters common to all the Pods of this application
#
peopleServer:
  # peopleServer.enabled Enables People Server installation
  enabled: true
  # peopleServer.kind The controller used for deploy
  # Possible values are `Deployment` (default) or `StatefulSet`
  kind: Deployment
  # peopleServer.annotations Defines annotations that will be additionally added to People Server deploy
  # If set to, it takes priority over the `commonAnnotations`
  # You can also use `tpl` as the value for the key
  annotations: {}
  # peopleServer.replicaCount Number of People Server replicas to deploy
  # If the `peopleServer.autoscaling.enabled` parameter is enabled, it is ignored
  replicaCount: 2
  # Update strategy used to replace old Pods by new ones
  updateStrategy:
    # peopleServer.updateStrategy.type People Server update strategy type
    # Allowed values: `RollingUpdate` or `OnDelete` if `peopleServer.kind` is set to `StatefulSet` and `RollingUpdate` or `Recreate` if `peopleServer.kind` is set to `Deployment`
    type: RollingUpdate
    # peopleServer.updateStrategy.rollingUpdate Used only when `peopleServer.updateStrategy.type=RollingUpdate`
    rollingUpdate:
      # peopleServer.updateStrategy.rollingUpdate.maxUnavailable Maximum number of People Server Pods unavailable during the update process
      maxUnavailable: 25%
      # peopleServer.updateStrategy.rollingUpdate.maxSurge Maximum number of People Server Pods created over the desired number of Pods
      maxSurge: 25%
  # peopleServer.podManagementPolicy The People Server Pods scaling operations policy
  # Used if `peopleServer.kind` is set to `StatefulSet`
  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  podManagementPolicy: OrderedReady
  # peopleServer.podAnnotations Map of annotations to add to the People Server pods
  podAnnotations: {}
  # Configure a Security Context for the People Server Pods
  # If set to, it takes priority over the `podSecurityContext`
  podSecurityContext:
    # peopleServer.podSecurityContext.enabled Enable security context for the People Server pods
    enabled: false
  # peopleServer.customPodAntiAffinity Prohibiting the scheduling of People Server Pods relative to other Pods containing the specified labels on the same node
  customPodAntiAffinity: {}
  # Pod affinity rules for People Server Pods scheduling by nodes relative to other Pods
  # Pod affinity allow you to constrain which nodes People Server Pods can be scheduled on based on the labels of Pods already running on that node
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # Example:
  # podAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #   - labelSelector:
  #       matchExpressions:
  #       - key: app
  #         operator: In
  #         values:
  #         - store
  #     topologyKey: topology.kubernetes.io/zone
  podAffinity: {}
  # Node affinity rules for People Server Pods scheduling by nodes
  # Node affinity allow you to constrain which nodes People Server Pods can be scheduled on based on node labels
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  # Example:
  # nodeAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #     - matchExpressions:
  #       - key: topology.kubernetes.io/zone
  #         operator: In
  #         values:
  #         - zone1
  #         - zone2
  nodeAffinity: {}
  # peopleServer.nodeSelector Node labels for People Server pods assignment
  # If set to, it takes priority over the `nodeSelector`
  nodeSelector: {}
  # peopleServer.tolerations Tolerations for People Server pods assignment
  # If set to, it takes priority over the `tolerations`
  tolerations: []
  ## Horizontal Pod Autoscaling parameters
  ## Horizontal Pod Autoscaling is used for autoscaling of the People Server deployment
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## The parameters below for autoscaling are used only when `peopleServer.autoscaling.enabled=true`
  autoscaling:
    ## peopleServer.autoscaling.enabled Enable People Server deployment autoscaling
    enabled: false
    ## peopleServer.autoscaling.annotations Defines annotations that will be additionally added to People Server deployment HPA
    ## If set to, it takes priority over the `commonAnnotations`
    ## You can also use `tpl` as the value for the key
    annotations: {}
    ## peopleServer.autoscaling.minReplicas People Server deployment autoscaling minimum number of replicas
    minReplicas: 2
    ## peopleServer.autoscaling.maxReplicas People Server deployment autoscaling maximum number of replicas
    maxReplicas: 16
    targetCPU:
      ## peopleServer.autoscaling.targetCPU.enabled Enable autoscaling of People Server deployment by CPU usage percentage
      enabled: true
      ## peopleServer.autoscaling.targetCPU.utilizationPercentage People Server deployment autoscaling target CPU percentage
      utilizationPercentage: 70
    targetMemory:
      ## peopleServer.autoscaling.targetMemory.enabled Enable autoscaling of People Server deployment by memory usage percentage
      enabled: false
      ## peopleServer.autoscaling.targetMemory.utilizationPercentage People Server deployment autoscaling target memory percentage
      utilizationPercentage: 70
    ## peopleServer.autoscaling.customMetricsType Custom, additional or external autoscaling metrics for the People Server deployment
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
    customMetricsType: []
    ## peopleServer.autoscaling.behavior Configuring People Server deployment scaling behavior policies for the `scaleDown` and `scaleUp` fields
    ## If not set the default values are used:
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#default-behavior
    behavior: {}
  # People Server container image parameters
  image:
    # peopleServer.image.repository People Server container image repository
    repository: onlyoffice/docspace-people-server
    # peopleServer.image.tag People Server container image tag
    # If set to, it takes priority over the `images.tag`
    tag: ""
    # peopleServer.image.pullPolicy People Server container image pull policy
    pullPolicy: IfNotPresent
  # Configure a Security Context for containers in People Server Pods
  # If set to, it takes priority over the `containerSecurityContext`
  containerSecurityContext:
    # peopleServer.containerSecurityContext.enabled Enable security context for containers in People Server pods
    enabled: false
  # peopleServer.lifecycleHooks Defines the People Server container lifecycle hooks
  # ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
  # It is used to trigger events to run at certain points in a container's lifecycle
  # There are two hooks that are exposed: `PostStart` and `PreStop`
  lifecycleHooks: {}
  # peopleServer.containerPorts.app People Server container port
  containerPorts:
    app: 5050
  # Probe used for the People Server container: startup, readiness and liveness probes
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # The parameters below for startup probes are used only when `peopleServer.startupProbe.enabled=true`
  startupProbe:
    # peopleServer.startupProbe.enabled Enable startupProbe for People Server container
    enabled: true
    httpGet:
      # peopleServer.startupProbe.httpGet.path Checking the path for startupProbe
      path: /health
      # peopleServer.startupProbe.httpGet.port Checking the port for startupProbe
      port: 5050
    # peopleServer.startupProbe.failureThreshold Failure threshold for startupProbe
    failureThreshold: 30
    # peopleServer.startupProbe.periodSeconds Period seconds for startupProbe
    periodSeconds: 10
  # The parameters below for readiness probes are used only when `peopleServer.readinessProbe.enabled=true`
  readinessProbe:
    # peopleServer.readinessProbe.enabled Enable readinessProbe for People Server container
    enabled: true
    # peopleServer.readinessProbe.failureThreshold Failure threshold for readinessProbe
    failureThreshold: 2
    httpGet:
      # peopleServer.readinessProbe.httpGet.path Checking the path for readinessProbe
      path: /health
      # peopleServer.readinessProbe.httpGet.port Checking the port for readinessProbe
      port: 5050
    # peopleServer.readinessProbe.periodSeconds Period seconds for readinessProbe
    periodSeconds: 10
    # peopleServer.readinessProbe.successThreshold Success threshold for readinessProbe
    successThreshold: 1
    # peopleServer.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    timeoutSeconds: 5
  # The parameters below for liveness probes are used only when `peopleServer.livenessProbe.enabled=true`
  livenessProbe:
    # peopleServer.livenessProbe.enabled Enable livenessProbe for People Server container
    enabled: true
    # peopleServer.livenessProbe.failureThreshold Failure threshold for livenessProbe
    failureThreshold: 3
    httpGet:
      # peopleServer.livenessProbe.httpGet.path Checking the path for livenessProbe
      path: /health
      # peopleServer.livenessProbe.httpGet.port Checking the port for livenessProbe
      port: 5050
    # peopleServer.livenessProbe.periodSeconds Period seconds for livenessProbe
    periodSeconds: 10
    # peopleServer.livenessProbe.successThreshold Success threshold for livenessProbe
    successThreshold: 1
    # peopleServer.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    timeoutSeconds: 5
  # People Server container resource requests and limits
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  # peopleServer.resources.requests The requested resources for the People Server container
  # peopleServer.resources.limits The resources limits for the People Server container
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "2Gi"
      cpu: "2000m"
  ## peopleServer.extraEnvVars An array with extra env variables for the People Server container
  extraEnvVars: []
  ## peopleServer.extraVolumes An array with extra volumes for the People Server container
  extraVolumes: []
  ## peopleServer.extraVolumeMounts An array with extra volume mounts for the People Server container
  extraVolumeMounts: []
  # peopleServer.mysqlUser Database user who will be used by the People Server service to connect to the database
  # If not set, `connections.mysqlUser` will be used
  # If set to, it takes priority over the `connections.mysqlUser`
  mysqlUser: ""

# DocSpace Router application parameters
# This block defines the parameters common to all the Pods of this application
#
router:
  # router.enabled Enables Router installation
  enabled: true
  # router.kind The controller used for deploy
  # Possible values are `Deployment` (default) or `StatefulSet`
  kind: Deployment
  # router.annotations Defines annotations that will be additionally added to Router deploy
  # If set to, it takes priority over the `commonAnnotations`
  # You can also use `tpl` as the value for the key
  annotations: {}
  # router.replicaCount Number of Router replicas to deploy
  # If the `router.autoscaling.enabled` parameter is enabled, it is ignored
  replicaCount: 2
  # Update strategy used to replace old Pods by new ones
  updateStrategy:
    # router.updateStrategy.type Router update strategy type
    # Allowed values: `RollingUpdate` or `OnDelete` if `router.kind` is set to `StatefulSet` and `RollingUpdate` or `Recreate` if `router.kind` is set to `Deployment`
    type: RollingUpdate
    # router.updateStrategy.rollingUpdate Used only when `router.updateStrategy.type=RollingUpdate`
    rollingUpdate:
      # router.updateStrategy.rollingUpdate.maxUnavailable Maximum number of Router Pods unavailable during the update process
      maxUnavailable: 25%
      # router.updateStrategy.rollingUpdate.maxSurge Maximum number of Router Pods created over the desired number of Pods
      maxSurge: 25%
  # router.podManagementPolicy The Router Pods scaling operations policy
  # Used if `router.kind` is set to `StatefulSet`
  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  podManagementPolicy: OrderedReady
  # router.podAnnotations Map of annotations to add to the Router pods
  podAnnotations: {}
  # Configure a Security Context for the Router Pods
  # If set to, it takes priority over the `podSecurityContext`
  podSecurityContext:
    # router.podSecurityContext.enabled Enable security context for the Router pods
    enabled: false
  # router.customPodAntiAffinity Prohibiting the scheduling of Router Pods relative to other Pods containing the specified labels on the same node
  customPodAntiAffinity: {}
  # Pod affinity rules for Router Pods scheduling by nodes relative to other Pods
  # Pod affinity allow you to constrain which nodes Router Pods can be scheduled on based on the labels of Pods already running on that node
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # Example:
  # podAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #   - labelSelector:
  #       matchExpressions:
  #       - key: app
  #         operator: In
  #         values:
  #         - store
  #     topologyKey: topology.kubernetes.io/zone
  podAffinity: {}
  # Node affinity rules for Router Pods scheduling by nodes
  # Node affinity allow you to constrain which nodes Router Pods can be scheduled on based on node labels
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  # Example:
  # nodeAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #     - matchExpressions:
  #       - key: topology.kubernetes.io/zone
  #         operator: In
  #         values:
  #         - zone1
  #         - zone2
  nodeAffinity: {}
  # router.nodeSelector Node labels for Router pods assignment
  # If set to, it takes priority over the `nodeSelector`
  nodeSelector: {}
  # router.tolerations Tolerations for Router pods assignment
  # If set to, it takes priority over the `tolerations`
  tolerations: []
  ## Horizontal Pod Autoscaling parameters
  ## Horizontal Pod Autoscaling is used for autoscaling of the Router deployment
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## The parameters below for autoscaling are used only when `router.autoscaling.enabled=true`
  autoscaling:
    ## router.autoscaling.enabled Enable Router deployment autoscaling
    enabled: false
    ## router.autoscaling.annotations Defines annotations that will be additionally added to Router deployment HPA
    ## If set to, it takes priority over the `commonAnnotations`
    ## You can also use `tpl` as the value for the key
    annotations: {}
    ## router.autoscaling.minReplicas Router deployment autoscaling minimum number of replicas
    minReplicas: 2
    ## router.autoscaling.maxReplicas Router deployment autoscaling maximum number of replicas
    maxReplicas: 16
    targetCPU:
      ## router.autoscaling.targetCPU.enabled Enable autoscaling of Router deployment by CPU usage percentage
      enabled: true
      ## router.autoscaling.targetCPU.utilizationPercentage Router deployment autoscaling target CPU percentage
      utilizationPercentage: 70
    targetMemory:
      ## router.autoscaling.targetMemory.enabled Enable autoscaling of Router deployment by memory usage percentage
      enabled: false
      ## router.autoscaling.targetMemory.utilizationPercentage Router deployment autoscaling target memory percentage
      utilizationPercentage: 70
    ## router.autoscaling.customMetricsType Custom, additional or external autoscaling metrics for the Router deployment
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
    customMetricsType: []
    ## router.autoscaling.behavior Configuring Router deployment scaling behavior policies for the `scaleDown` and `scaleUp` fields
    ## If not set the default values are used:
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#default-behavior
    behavior: {}
  # People Server container image parameters
  # Router initContainers parameters
  # router.initContainers Containers that run before Router container in a Pod
  # ref:https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
  # Example:
  # initContainers:
  #   - name: custom-init-container
  #     image: busybox:latest
  #     command: ['sh', '-c', 'sleep 180']
  initContainers: []
  # Router container image parameters
  image:
    # router.image.repository Router container image repository
    repository: onlyoffice/docspace-router
    # router.image.tag Router container image tag
    # If set to, it takes priority over the `images.tag`
    tag: ""
    # router.image.pullPolicy Router container image pull policy
    pullPolicy: IfNotPresent
  # Configure a Security Context for containers in Router Pods
  # If set to, it takes priority over the `containerSecurityContext`
  containerSecurityContext:
    # router.containerSecurityContext.enabled Enable security context for containers in Router pods
    enabled: false
  # router.lifecycleHooks Defines the Router container lifecycle hooks
  # ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
  # It is used to trigger events to run at certain points in a container's lifecycle
  # There are two hooks that are exposed: `PostStart` and `PreStop`
  lifecycleHooks: {}
  # router.containerPorts.external Router container port
  containerPorts:
    external: 8092
  # Probe used for the Router container: startup, readiness and liveness probes
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # The parameters below for startup probes are used only when `router.startupProbe.enabled=true`
  startupProbe:
    # router.startupProbe.enabled Enable startupProbe for Router container
    enabled: true
    httpGet:
      # router.startupProbe.httpGet.path Checking the path for startupProbe
      path: /
      # router.startupProbe.httpGet.port Checking the port for startupProbe
      port: 8092
    # router.startupProbe.failureThreshold Failure threshold for startupProbe
    failureThreshold: 30
    # router.startupProbe.periodSeconds Period seconds for startupProbe
    periodSeconds: 10
  # The parameters below for readiness probes are used only when `router.readinessProbe.enabled=true`
  readinessProbe:
    # router.readinessProbe.enabled Enable readinessProbe for Router container
    enabled: true
    # router.readinessProbe.failureThreshold Failure threshold for readinessProbe
    failureThreshold: 2
    httpGet:
      # router.readinessProbe.httpGet.path Checking the path for readinessProbe
      path: /
      # router.readinessProbe.httpGet.port Checking the port for readinessProbe
      port: 8092
    # router.readinessProbe.periodSeconds Period seconds for readinessProbe
    periodSeconds: 20
    # router.readinessProbe.successThreshold Success threshold for readinessProbe
    successThreshold: 1
    # router.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    timeoutSeconds: 5
  # The parameters below for liveness probes are used only when `router.livenessProbe.enabled=true`
  livenessProbe:
    # router.livenessProbe.enabled Enable livenessProbe for Router container
    enabled: true
    # router.livenessProbe.failureThreshold Failure threshold for livenessProbe
    failureThreshold: 5
    httpGet:
      # router.livenessProbe.httpGet.path Checking the path for livenessProbe
      path: /
      # router.livenessProbe.httpGet.port Checking the port for livenessProbe
      port: 8092
    # router.livenessProbe.periodSeconds Period seconds for livenessProbe
    periodSeconds: 20
    # router.livenessProbe.successThreshold Success threshold for livenessProbe
    successThreshold: 1
    # router.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    timeoutSeconds: 5
  # Router container resource requests and limits
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  # router.resources.requests The requested resources for the Router container
  # router.resources.limits The resources limits for the Router container
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "2Gi"
      cpu: "2000m"
  ## router.extraEnvVars An array with extra env variables for the Router container
  extraEnvVars: []
  ## router.extraVolumes An array with extra volumes for the Router container
  extraVolumes: []
  ## router.extraVolumeMounts An array with extra volume mounts for the Router container
  extraVolumeMounts: []
  # Additional configuration files for Router
  extraConf:
    customInitScripts:
      # router.extraConf.customInitScripts.configMap The name of the ConfigMap containing custom initialization scripts
      configMap: ""
      # router.extraConf.customInitScripts.fileName The names of scripts containing custom initialization scripts
      # Must be the same as the `key` names in `router.extraConf.customInitScripts.configMap`
      # May contain multiple values
      fileName:
        - 60-custom-init-scripts.sh
    templates:
      # router.extraConf.templates.configMap The name of the ConfigMap containing configuration file templates containing environment variables
      # The values of these variables will be substituted when the container is started
      configMap: ""
      # router.extraConf.templates.fileName The names of the configuration file templates containing environment variables
      # Must be the same as the `key` names in `router.extraConf.templates.configMap`
      # May contain multiple values
      fileName:
        - 10.example.conf.template
    confd:
      # router.extraConf.confd.configMap The name of the ConfigMap containing additional custom configuration files
      # These files will be map in the `/etc/nginx/conf.d/` directory of the container
      configMap: ""
      # router.extraConf.confd.fileName The names of the configuration files containing custom configuration files
      # Must be the same as the `key` names in `router.extraConf.confd.configMap`
      # May contain multiple values
      fileName:
        - example.conf
  # Router service parameters
  service:
    # router.service.existing The name of an existing service for Router. If not set, a service named `router` will be created
    # ref: https://github.com/ONLYOFFICE/Kubernetes-DocSpace/blob/master/templates/services/router.yaml
    existing: ""
    # router.service.annotations Map of annotations to add to the Router service
    annotations: {}
    port:
      # router.service.port.external Router service port
      external: 8092
    # router.service.type Router service type
    type: ClusterIP
    # router.service.sessionAffinity Session Affinity for Router service
    # If not set, `None` will be set as the default value
    # ref: https://kubernetes.io/docs/reference/networking/virtual-ips/#session-affinity
    sessionAffinity: ""
    # router.service.sessionAffinityConfig Configuration for Router service Session Affinity
    # Used if the `router.service.sessionAffinity` is set
    # ref: https://kubernetes.io/docs/reference/networking/virtual-ips/#session-stickiness-timeout
    # Example:
    # sessionAffinityConfig:
    #   clientIP:
    #     timeoutSeconds: 900
    sessionAffinityConfig: {}
    # router.service.externalTrafficPolicy Enable preservation of the client source IP
    # There are two available options: `Cluster` (default) and `Local`
    # ref: https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    # Not supported for service type - `ClusterIP`
    # ref: https://kubernetes.io/docs/tutorials/services/source-ip/
    externalTrafficPolicy: ""
  # Router openresty resolver
  # ref: https://github.com/openresty/openresty/#resolvconf-parsing
  resolver:
    # router.resolver.dns Configures name server used to resolve names of upstream servers into addresses
    # If set to, it takes priority over the `router.resolver.local`
    dns: ""
    # router.resolver.local Allows you to use the DNS configuration of the container
    # If set to `on`, the standard path "/etc/resolv.conf" will be used. You can specify an arbitrary path
    # Example:
    # local: "/tmp/resolv.conf"
    local: "on"

# DocSpace Healthchecks application parameters
# This block defines the parameters common to all the Pods of this application
#
healthchecks:
  # healthchecks.enabled Enables Healthchecks installation
  enabled: true
  # healthchecks.kind The controller used for deploy
  # Possible values are `Deployment` (default) or `StatefulSet`
  kind: Deployment
  # healthchecks.annotations Defines annotations that will be additionally added to Healthchecks deploy
  # If set to, it takes priority over the `commonAnnotations`
  # You can also use `tpl` as the value for the key
  annotations: {}
  # healthchecks.replicaCount Number of Healthchecks replicas to deploy
  # If the `healthchecks.autoscaling.enabled` parameter is enabled, it is ignored
  replicaCount: 2
  # Update strategy used to replace old Pods by new ones
  updateStrategy:
    # healthchecks.updateStrategy.type Healthchecks update strategy type
    # Allowed values: `RollingUpdate` or `OnDelete` if `healthchecks.kind` is set to `StatefulSet` and `RollingUpdate` or `Recreate` if `healthchecks.kind` is set to `Deployment`
    type: RollingUpdate
    # healthchecks.updateStrategy.rollingUpdate Used only when `healthchecks.updateStrategy.type=RollingUpdate`
    rollingUpdate:
      # healthchecks.updateStrategy.rollingUpdate.maxUnavailable Maximum number of Healthchecks Pods unavailable during the update process
      maxUnavailable: 25%
      # healthchecks.updateStrategy.rollingUpdate.maxSurge Maximum number of Healthchecks Pods created over the desired number of Pods
      maxSurge: 25%
  # healthchecks.podManagementPolicy The Healthchecks Pods scaling operations policy
  # Used if `healthchecks.kind` is set to `StatefulSet`
  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  podManagementPolicy: OrderedReady
  # healthchecks.podAnnotations Map of annotations to add to the Healthchecks pods
  podAnnotations: {}
  # Configure a Security Context for the Healthchecks Pods
  # If set to, it takes priority over the `podSecurityContext`
  podSecurityContext:
    # healthchecks.podSecurityContext.enabled Enable security context for the Healthchecks pods
    enabled: false
  # healthchecks.customPodAntiAffinity Prohibiting the scheduling of Healthchecks Pods relative to other Pods containing the specified labels on the same node
  customPodAntiAffinity: {}
  # Pod affinity rules for Healthchecks Pods scheduling by nodes relative to other Pods
  # Pod affinity allow you to constrain which nodes Healthchecks Pods can be scheduled on based on the labels of Pods already running on that node
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # Example:
  # podAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #   - labelSelector:
  #       matchExpressions:
  #       - key: app
  #         operator: In
  #         values:
  #         - store
  #     topologyKey: topology.kubernetes.io/zone
  podAffinity: {}
  # Node affinity rules for Healthchecks Pods scheduling by nodes
  # Node affinity allow you to constrain which nodes Healthchecks Pods can be scheduled on based on node labels
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  # Example:
  # nodeAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #     - matchExpressions:
  #       - key: topology.kubernetes.io/zone
  #         operator: In
  #         values:
  #         - zone1
  #         - zone2
  nodeAffinity: {}
  # healthchecks.nodeSelector Node labels for Healthchecks pods assignment
  # If set to, it takes priority over the `nodeSelector`
  nodeSelector: {}
  # healthchecks.tolerations Tolerations for Healthchecks pods assignment
  # If set to, it takes priority over the `tolerations`
  tolerations: []
  ## Horizontal Pod Autoscaling parameters
  ## Horizontal Pod Autoscaling is used for autoscaling of the Healthchecks deployment
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## The parameters below for autoscaling are used only when `healthchecks.autoscaling.enabled=true`
  autoscaling:
    ## healthchecks.autoscaling.enabled Enable Healthchecks deployment autoscaling
    enabled: false
    ## healthchecks.autoscaling.annotations Defines annotations that will be additionally added to Healthchecks deployment HPA
    ## If set to, it takes priority over the `commonAnnotations`
    ## You can also use `tpl` as the value for the key
    annotations: {}
    ## healthchecks.autoscaling.minReplicas Healthchecks deployment autoscaling minimum number of replicas
    minReplicas: 2
    ## healthchecks.autoscaling.maxReplicas Healthchecks deployment autoscaling maximum number of replicas
    maxReplicas: 16
    targetCPU:
      ## healthchecks.autoscaling.targetCPU.enabled Enable autoscaling of Healthchecks deployment by CPU usage percentage
      enabled: true
      ## healthchecks.autoscaling.targetCPU.utilizationPercentage Healthchecks deployment autoscaling target CPU percentage
      utilizationPercentage: 70
    targetMemory:
      ## healthchecks.autoscaling.targetMemory.enabled Enable autoscaling of Healthchecks deployment by memory usage percentage
      enabled: false
      ## healthchecks.autoscaling.targetMemory.utilizationPercentage Healthchecks deployment autoscaling target memory percentage
      utilizationPercentage: 70
    ## healthchecks.autoscaling.customMetricsType Custom, additional or external autoscaling metrics for the Healthchecks deployment
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
    customMetricsType: []
    ## healthchecks.autoscaling.behavior Configuring Healthchecks deployment scaling behavior policies for the `scaleDown` and `scaleUp` fields
    ## If not set the default values are used:
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#default-behavior
    behavior: {}
  # Healthchecks container image parameters
  image:
    # healthchecks.image.repository Healthchecks container image repository
    repository: onlyoffice/docspace-healthchecks
    # healthchecks.image.tag Healthchecks container image tag
    # If set to, it takes priority over the `images.tag`
    tag: ""
    # healthchecks.image.pullPolicy Healthchecks container image pull policy
    pullPolicy: IfNotPresent
  # Configure a Security Context for containers in Healthchecks Pods
  # If set to, it takes priority over the `containerSecurityContext`
  containerSecurityContext:
    # healthchecks.containerSecurityContext.enabled Enable security context for containers in Healthchecks pods
    enabled: false
  # healthchecks.lifecycleHooks Defines the Healthchecks container lifecycle hooks
  # ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
  # It is used to trigger events to run at certain points in a container's lifecycle
  # There are two hooks that are exposed: `PostStart` and `PreStop`
  lifecycleHooks: {}
  # healthchecks.containerPorts.healthcheck Healthchecks container port
  containerPorts:
    healthcheck: 5050
  # Probe used for the Healthchecks container: startup, readiness and liveness probes
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # The parameters below for startup probes are used only when `healthchecks.startupProbe.enabled=true`
  startupProbe:
    # healthchecks.startupProbe.enabled Enable startupProbe for Healthchecks container
    enabled: true
    tcpSocket:
      # healthchecks.startupProbe.tcpSocket.port Checking the port for startupProbe
      port: 5050
    # healthchecks.startupProbe.failureThreshold Failure threshold for startupProbe
    failureThreshold: 30
    # healthchecks.startupProbe.periodSeconds Period seconds for startupProbe
    periodSeconds: 10
  # The parameters below for readiness probes are used only when `healthchecks.readinessProbe.enabled=true`
  readinessProbe:
    # healthchecks.readinessProbe.enabled Enable readinessProbe for Healthchecks container
    enabled: true
    # healthchecks.readinessProbe.failureThreshold Failure threshold for readinessProbe
    failureThreshold: 2
    tcpSocket:
      # healthchecks.readinessProbe.tcpSocket.port Checking the port for readinessProbe
      port: 5050
    # healthchecks.readinessProbe.periodSeconds Period seconds for readinessProbe
    periodSeconds: 60
    # healthchecks.readinessProbe.successThreshold Success threshold for readinessProbe
    successThreshold: 1
    # healthchecks.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    timeoutSeconds: 5
  # The parameters below for liveness probes are used only when `healthchecks.livenessProbe.enabled=true`
  livenessProbe:
    # healthchecks.livenessProbe.enabled Enable livenessProbe for Healthchecks container
    enabled: true
    # healthchecks.livenessProbe.failureThreshold Failure threshold for livenessProbe
    failureThreshold: 3
    tcpSocket:
      # healthchecks.livenessProbe.tcpSocket.port Checking the port for livenessProbe
      port: 5050
    # healthchecks.livenessProbe.periodSeconds Period seconds for livenessProbe
    periodSeconds: 60
    # healthchecks.livenessProbe.successThreshold Success threshold for livenessProbe
    successThreshold: 1
    # healthchecks.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    timeoutSeconds: 5
  # Healthchecks container resource requests and limits
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  # healthchecks.resources.requests The requested resources for the Healthchecks container
  # healthchecks.resources.limits The resources limits for the Healthchecks container
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "1Gi"
      cpu: "1000m"
  ## healthchecks.extraEnvVars An array with extra env variables for the Healthchecks container
  extraEnvVars: []
  ## healthchecks.extraVolumes An array with extra volumes for the Healthchecks container
  extraVolumes: []
  ## healthchecks.extraVolumeMounts An array with extra volume mounts for the Healthchecks container
  extraVolumeMounts: []

# DocSpace statefulsets
#
# DocSpace Api System application parameters
# This block defines the parameters common to all the Pods of this application
#
apiSystem:
  # apiSystem.enabled Enables Api System installation
  enabled: true
  # apiSystem.kind The controller used for deploy
  # Possible values are `Deployment` (default) or `StatefulSet`
  kind: Deployment
  # apiSystem.annotations Defines annotations that will be additionally added to Api System deploy
  # If set to, it takes priority over the `commonAnnotations`
  # You can also use `tpl` as the value for the key
  annotations: {}
  # apiSystem.replicaCount Number of Api System replicas to deploy
  # If the `apiSystem.autoscaling.enabled` parameter is enabled, it is ignored
  replicaCount: 2
  # Update strategy used to replace old Pods by new ones
  updateStrategy:
    # apiSystem.updateStrategy.type Api System update strategy type
    # Allowed values: `RollingUpdate` or `OnDelete` if `apiSystem.kind` is set to `StatefulSet` and `RollingUpdate` or `Recreate` if `apiSystem.kind` is set to `Deployment`
    type: RollingUpdate
    # apiSystem.updateStrategy.rollingUpdate Used only when `apiSystem.updateStrategy.type=RollingUpdate`
    rollingUpdate:
      # apiSystem.updateStrategy.rollingUpdate.maxUnavailable Maximum number of Api System Pods unavailable during the update process
      maxUnavailable: 25%
      # apiSystem.updateStrategy.rollingUpdate.maxSurge Maximum number of Api System Pods created over the desired number of Pods
      maxSurge: 25%
  # apiSystem.podManagementPolicy The Api System Pods scaling operations policy
  # Used if `apiSystem.kind` is set to `StatefulSet`
  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  podManagementPolicy: OrderedReady
  # apiSystem.podAnnotations Map of annotations to add to the Api System pods
  podAnnotations: {}
  # Configure a Security Context for the Api System Pods
  # If set to, it takes priority over the `podSecurityContext`
  podSecurityContext:
    # apiSystem.podSecurityContext.enabled Enable security context for the Api System pods
    enabled: false
  # apiSystem.customPodAntiAffinity Prohibiting the scheduling of Api System Pods relative to other Pods containing the specified labels on the same node
  # Example:
  # customPodAntiAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #   - labelSelector:
  #       matchExpressions:
  #       - key: app
  #         operator: In
  #         values:
  #         - api
  #     topologyKey: kubernetes.io/hostname
  customPodAntiAffinity: {}
  # Pod affinity rules for Api System Pod scheduling by nodes relative to other Pods
  # Pod affinity allow you to constrain which nodes Api System Pods can be scheduled on based on the labels of Pods already running on that node
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # Example:
  # podAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     podAffinityTerm:
  #       labelSelector:
  #         matchExpressions:
  #         - key: app
  #           operator: NotIn
  #           values:
  #           - database
  #       topologyKey: kubernetes.io/hostname
  podAffinity: {}
  # Node affinity rules for Api System Pod scheduling by nodes
  # Node affinity allow you to constrain which nodes Api System Pods can be scheduled on based on node labels
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  # Example:
  # nodeAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     preference:
  #       matchExpressions:
  #       - key: kubernetes.io/name
  #         operator: In
  #         values:
  #         - name1
  #         - name2
  nodeAffinity: {}
  # apiSystem.nodeSelector Node labels for Api System pods assignment
  # If set to, it takes priority over the `nodeSelector`
  nodeSelector: {}
  # apiSystem.tolerations Tolerations for Api System pods assignment
  # If set to, it takes priority over the `tolerations`
  tolerations: []
  ## Horizontal Pod Autoscaling parameters
  ## Horizontal Pod Autoscaling is used for autoscaling of the Api deployment
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## The parameters below for autoscaling are used only when `apiSystem.autoscaling.enabled=true`
  autoscaling:
    ## apiSystem.autoscaling.enabled Enable Api deployment autoscaling
    enabled: false
    ## apiSystem.autoscaling.annotations Defines annotations that will be additionally added to Api deployment HPA
    ## If set to, it takes priority over the `commonAnnotations`
    ## You can also use `tpl` as the value for the key
    annotations: {}
    ## apiSystem.autoscaling.minReplicas Api deployment autoscaling minimum number of replicas
    minReplicas: 2
    ## apiSystem.autoscaling.maxReplicas Api deployment autoscaling maximum number of replicas
    maxReplicas: 16
    targetCPU:
      ## apiSystem.autoscaling.targetCPU.enabled Enable autoscaling of Api deployment by CPU usage percentage
      enabled: true
      ## apiSystem.autoscaling.targetCPU.utilizationPercentage Api deployment autoscaling target CPU percentage
      utilizationPercentage: 70
    targetMemory:
      ## apiSystem.autoscaling.targetMemory.enabled Enable autoscaling of Api deployment by memory usage percentage
      enabled: false
      ## apiSystem.autoscaling.targetMemory.utilizationPercentage Api deployment autoscaling target memory percentage
      utilizationPercentage: 70
    ## apiSystem.autoscaling.customMetricsType Custom, additional or external autoscaling metrics for the Api deployment
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
    customMetricsType: []
    ## apiSystem.autoscaling.behavior Configuring Api deployment scaling behavior policies for the `scaleDown` and `scaleUp` fields
    ## If not set the default values are used:
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#default-behavior
    behavior: {}
  # Api System container image parameters
  image:
    # apiSystem.image.repository Api System container image repository
    repository: onlyoffice/docspace-api-system
    # apiSystem.image.tag Api System container image tag
    # If set to, it takes priority over the `images.tag`
    tag: ""
    # apiSystem.image.pullPolicy Api System container image pull policy
    pullPolicy: IfNotPresent
  # Configure a Security Context for containers in Api System Pods
  # If set to, it takes priority over the `containerSecurityContext`
  containerSecurityContext:
    # apiSystem.containerSecurityContext.enabled Enable security context for containers in Api System pods
    enabled: false
  # apiSystem.lifecycleHooks Defines the Api System container lifecycle hooks
  # ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
  # It is used to trigger events to run at certain points in a container's lifecycle
  # There are two hooks that are exposed: `PostStart` and `PreStop`
  lifecycleHooks: {}
  # apiSystem.containerPorts.app Api System container port
  containerPorts:
    app: 5050
  # Probe used for the Api System container: startup, readiness and liveness probes
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # The parameters below for startup probes are used only when `apiSystem.startupProbe.enabled=true`
  startupProbe:
    # apiSystem.startupProbe.enabled Enable startupProbe for Api System container
    enabled: true
    httpGet:
      # apiSystem.startupProbe.httpGet.path Checking the path for startupProbe
      path: /health
      # apiSystem.startupProbe.httpGet.port Checking the port for startupProbe
      port: 5050
    # apiSystem.startupProbe.failureThreshold Failure threshold for startupProbe
    failureThreshold: 30
    # apiSystem.startupProbe.periodSeconds Period seconds for startupProbe
    periodSeconds: 10
  # The parameters below for readiness probes are used only when `apiSystem.readinessProbe.enabled=true`
  readinessProbe:
    # apiSystem.readinessProbe.enabled Enable readinessProbe for Api System container
    enabled: true
    # apiSystem.readinessProbe.failureThreshold Failure threshold for readinessProbe
    failureThreshold: 2
    httpGet:
      # apiSystem.readinessProbe.httpGet.path Checking the path for readinessProbe
      path: /health
      # apiSystem.readinessProbe.httpGet.port Checking the port for readinessProbe
      port: 5050
    # apiSystem.readinessProbe.periodSeconds Period seconds for readinessProbe
    periodSeconds: 10
    # apiSystem.readinessProbe.successThreshold Success threshold for readinessProbe
    successThreshold: 1
    # apiSystem.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    timeoutSeconds: 5
  # The parameters below for liveness probes are used only when `apiSystem.livenessProbe.enabled=true`
  livenessProbe:
    # apiSystem.livenessProbe.enabled Enable livenessProbe for Api System container
    enabled: true
    # apiSystem.livenessProbe.failureThreshold Failure threshold for livenessProbe
    failureThreshold: 3
    httpGet:
      # apiSystem.livenessProbe.httpGet.path Checking the path for livenessProbe
      path: /health
      # apiSystem.livenessProbe.httpGet.port Checking the port for livenessProbe
      port: 5050
    # apiSystem.livenessProbe.periodSeconds Period seconds for livenessProbe
    periodSeconds: 10
    # apiSystem.livenessProbe.successThreshold Success threshold for livenessProbe
    successThreshold: 1
    # apiSystem.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    timeoutSeconds: 5
  # Api System container resource requests and limits
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  # apiSystem.resources.requests The requested resources for the Api System container
  # apiSystem.resources.limits The resources limits for the Api System container
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "2Gi"
      cpu: "2000m"
  ## apiSystem.extraEnvVars An array with extra env variables for the Api System container
  extraEnvVars: []
  ## apiSystem.extraVolumes An array with extra volumes for the Api System container
  extraVolumes: []
  ## apiSystem.extraVolumeMounts An array with extra volume mounts for the Api System container
  extraVolumeMounts: []
  # apiSystem.mysqlUser Database user who will be used by the Api System service to connect to the database
  # If not set, `connections.mysqlUser` will be used
  # If set to, it takes priority over the `connections.mysqlUser`
  mysqlUser: ""

# DocSpace Api application parameters
# This block defines the parameters common to all the Pods of this application
#
api:
  # api.enabled Enables Api installation
  enabled: true
  # api.kind The controller used for deploy
  # Possible values are `Deployment` (default) or `StatefulSet`
  kind: Deployment
  # api.annotations Defines annotations that will be additionally added to Api deploy
  # If set to, it takes priority over the `commonAnnotations`
  # You can also use `tpl` as the value for the key
  annotations: {}
  # api.replicaCount Number of Api replicas to deploy
  # If the `api.autoscaling.enabled` parameter is enabled, it is ignored
  replicaCount: 2
  # Update strategy used to replace old Pods by new ones
  updateStrategy:
    # api.updateStrategy.type Api update strategy type
    # Allowed values: `RollingUpdate` or `OnDelete` if `api.kind` is set to `StatefulSet` and `RollingUpdate` or `Recreate` if `api.kind` is set to `Deployment`
    type: RollingUpdate
    # api.updateStrategy.rollingUpdate Used only when `api.updateStrategy.type=RollingUpdate`
    rollingUpdate:
      # api.updateStrategy.rollingUpdate.maxUnavailable Maximum number of Api Pods unavailable during the update process
      maxUnavailable: 25%
      # api.updateStrategy.rollingUpdate.maxSurge Maximum number of Api Pods created over the desired number of Pods
      maxSurge: 25%
  # api.podManagementPolicy The Api Pods scaling operations policy
  # Used if `api.kind` is set to `StatefulSet`
  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  podManagementPolicy: OrderedReady
  # api.podAnnotations Map of annotations to add to the Api pods
  podAnnotations: {}
  # Configure a Security Context for the Api Pods
  # If set to, it takes priority over the `podSecurityContext`
  podSecurityContext:
    # api.podSecurityContext.enabled Enable security context for the Api pods
    enabled: false
  # api.customPodAntiAffinity Prohibiting the scheduling of Api Pods relative to other Pods containing the specified labels on the same node
  # Example:
  # customPodAntiAffinity:
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #   - labelSelector:
  #       matchExpressions:
  #       - key: app
  #         operator: In
  #         values:
  #         - files
  #     topologyKey: kubernetes.io/hostname
  customPodAntiAffinity: {}
  # Pod affinity rules for Api Pod scheduling by nodes relative to other Pods
  # Pod affinity allow you to constrain which nodes Api Pods can be scheduled on based on the labels of Pods already running on that node
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # Example:
  # podAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     podAffinityTerm:
  #       labelSelector:
  #         matchExpressions:
  #         - key: app
  #           operator: NotIn
  #           values:
  #           - database
  #       topologyKey: kubernetes.io/hostname
  podAffinity: {}
  # Node affinity rules for Api Pod scheduling by nodes
  # Node affinity allow you to constrain which nodes Api Pods can be scheduled on based on node labels
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  # Example:
  # nodeAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     preference:
  #       matchExpressions:
  #       - key: kubernetes.io/name
  #         operator: In
  #         values:
  #         - name1
  #         - name2
  nodeAffinity: {}
  # api.nodeSelector Node labels for Api pods assignment
  # If set to, it takes priority over the `nodeSelector`
  nodeSelector: {}
  # api.tolerations Tolerations for Api pods assignment
  # If set to, it takes priority over the `tolerations`
  tolerations: []
  ## Horizontal Pod Autoscaling parameters
  ## Horizontal Pod Autoscaling is used for autoscaling of the Api deployment
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## The parameters below for autoscaling are used only when `api.autoscaling.enabled=true`
  autoscaling:
    ## api.autoscaling.enabled Enable Api System deployment autoscaling
    enabled: false
    ## api.autoscaling.annotations Defines annotations that will be additionally added to Api System deployment HPA
    ## If set to, it takes priority over the `commonAnnotations`
    ## You can also use `tpl` as the value for the key
    annotations: {}
    ## api.autoscaling.minReplicas Api System deployment autoscaling minimum number of replicas
    minReplicas: 2
    ## api.autoscaling.maxReplicas Api System deployment autoscaling maximum number of replicas
    maxReplicas: 16
    targetCPU:
      ## api.autoscaling.targetCPU.enabled Enable autoscaling of Api System deployment by CPU usage percentage
      enabled: true
      ## api.autoscaling.targetCPU.utilizationPercentage Api System deployment autoscaling target CPU percentage
      utilizationPercentage: 70
    targetMemory:
      ## api.autoscaling.targetMemory.enabled Enable autoscaling of Api System deployment by memory usage percentage
      enabled: false
      ## api.autoscaling.targetMemory.utilizationPercentage Api System deployment autoscaling target memory percentage
      utilizationPercentage: 70
    ## api.autoscaling.customMetricsType Custom, additional or external autoscaling metrics for the Api System deployment
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
    customMetricsType: []
    ## api.autoscaling.behavior Configuring Api System deployment scaling behavior policies for the `scaleDown` and `scaleUp` fields
    ## If not set the default values are used:
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#default-behavior
    behavior: {}
  # Api container image parameters
  image:
    # api.image.repository Api container image repository
    repository: onlyoffice/docspace-api
    # api.image.tag Api container image tag
    # If set to, it takes priority over the `images.tag`
    tag: ""
    # api.image.pullPolicy Api container image pull policy
    pullPolicy: IfNotPresent
  # Configure a Security Context for containers in Api Pods
  # If set to, it takes priority over the `containerSecurityContext`
  containerSecurityContext:
    # api.containerSecurityContext.enabled Enable security context for containers in Api pods
    enabled: false
  # api.lifecycleHooks Defines the Api container lifecycle hooks
  # ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
  # It is used to trigger events to run at certain points in a container's lifecycle
  # There are two hooks that are exposed: `PostStart` and `PreStop`
  # Example:
  # lifecycleHooks:
  #   preStop:
  #     exec:
  #       command: ["/bin/sh", "-c", "sleep 25"]
  lifecycleHooks: {}
  # api.containerPorts.app Api container port
  containerPorts:
    app: 5050
  # Probe used for the Api container: startup, readiness and liveness probes
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # The parameters below for startup probes are used only when `api.startupProbe.enabled=true`
  startupProbe:
    # api.startupProbe.enabled Enable startupProbe for Api container
    enabled: true
    httpGet:
      # api.startupProbe.httpGet.path Checking the path for startupProbe
      path: /health
      # api.startupProbe.httpGet.port Checking the port for startupProbe
      port: 5050
    # api.startupProbe.failureThreshold Failure threshold for startupProbe
    failureThreshold: 30
    # api.startupProbe.periodSeconds Period seconds for startupProbe
    periodSeconds: 10
  # The parameters below for readiness probes are used only when `Api.readinessProbe.enabled=true`
  readinessProbe:
    # api.readinessProbe.enabled Enable readinessProbe for Api container
    enabled: true
    # api.readinessProbe.failureThreshold Failure threshold for readinessProbe
    failureThreshold: 2
    httpGet:
      # api.readinessProbe.httpGet.path Checking the path for readinessProbe
      path: /health
      # api.readinessProbe.httpGet.port Checking the port for readinessProbe
      port: 5050
    # api.readinessProbe.periodSeconds Period seconds for readinessProbe
    periodSeconds: 20
    # api.readinessProbe.successThreshold Success threshold for readinessProbe
    successThreshold: 1
    # api.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    timeoutSeconds: 5
  # The parameters below for liveness probes are used only when `Api.livenessProbe.enabled=true`
  livenessProbe:
    # api.livenessProbe.enabled Enable livenessProbe for Api container
    enabled: true
    # api.livenessProbe.failureThreshold Failure threshold for livenessProbe
    failureThreshold: 3
    httpGet:
      # api.livenessProbe.httpGet.path Checking the path for livenessProbe
      path: /health
      # api.livenessProbe.httpGet.port Checking the port for livenessProbe
      port: 5050
    # api.livenessProbe.periodSeconds Period seconds for livenessProbe
    periodSeconds: 20
    # api.livenessProbe.successThreshold Success threshold for livenessProbe
    successThreshold: 1
    # api.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    timeoutSeconds: 5
  # Api container resource requests and limits
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  # api.resources.requests The requested resources for the Api container
  # api.resources.limits The resources limits for the Api container
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "4Gi"
      cpu: "4000m"
  ## api.extraEnvVars An array with extra env variables for the Api container
  extraEnvVars: []
  ## api.extraVolumes An array with extra volumes for the Api container
  extraVolumes: []
  ## api.extraVolumeMounts An array with extra volume mounts for the Api container
  extraVolumeMounts: []
  # api.mysqlUser Database user who will be used by the Api service to connect to the database
  # If not set, `connections.mysqlUser` will be used
  # If set to, it takes priority over the `connections.mysqlUser`
  mysqlUser: ""

# DocSpace Backup application parameters
# This block defines the parameters common to all the Pods of this application
#
backup:
  # backup.enabled Enables Backup installation
  enabled: true
  # backup.kind The controller used for deploy
  # Possible values are `Deployment` (default) or `StatefulSet`
  kind: Deployment
  # backup.annotations Defines annotations that will be additionally added to Backup deploy
  # If set to, it takes priority over the `commonAnnotations`
  # You can also use `tpl` as the value for the key
  annotations: {}
  # backup.replicaCount Number of Backup replicas to deploy
  # If the `backup.autoscaling.enabled` parameter is enabled, it is ignored
  replicaCount: 2
  # Update strategy used to replace old Pods by new ones
  updateStrategy:
    # backup.updateStrategy.type Backup update strategy type
    # Allowed values: `RollingUpdate` or `OnDelete` if `backup.kind` is set to `StatefulSet` and `RollingUpdate` or `Recreate` if `backup.kind` is set to `Deployment`
    type: RollingUpdate
    # backup.updateStrategy.rollingUpdate Used only when `backup.updateStrategy.type=RollingUpdate`
    rollingUpdate:
      # backup.updateStrategy.rollingUpdate.maxUnavailable Maximum number of Backup Pods unavailable during the update process
      maxUnavailable: 25%
      # backup.updateStrategy.rollingUpdate.maxSurge Maximum number of Backup Pods created over the desired number of Pods
      maxSurge: 25%
  # backup.podManagementPolicy The Backup Pods scaling operations policy
  # Used if `backup.kind` is set to `StatefulSet`
  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  podManagementPolicy: OrderedReady
  # backup.podAnnotations Map of annotations to add to the Backup pods
  podAnnotations: {}
  # Configure a Security Context for the Backup Pods
  # If set to, it takes priority over the `podSecurityContext`
  podSecurityContext:
    # backup.podSecurityContext.enabled Enable security context for the Backup pods
    enabled: false
  # backup.customPodAntiAffinity Prohibiting the scheduling of Backup Pods relative to other Pods containing the specified labels on the same node
  customPodAntiAffinity: {}
  # Pod affinity rules for Backup Pod scheduling by nodes relative to other Pods
  # Pod affinity allow you to constrain which nodes Backup Pods can be scheduled on based on the labels of Pods already running on that node
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # Example:
  # podAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     podAffinityTerm:
  #       labelSelector:
  #         matchExpressions:
  #         - key: app
  #           operator: NotIn
  #           values:
  #           - database
  #       topologyKey: kubernetes.io/hostname
  podAffinity: {}
  # Node affinity rules for Backup Pod scheduling by nodes
  # Node affinity allow you to constrain which nodes backup Pods can be scheduled on based on node labels
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  # Example:
  # nodeAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     preference:
  #       matchExpressions:
  #       - key: kubernetes.io/name
  #         operator: In
  #         values:
  #         - name1
  #         - name2
  nodeAffinity: {}
  # backup.nodeSelector Node labels for Backup pods assignment
  # If set to, it takes priority over the `nodeSelector`
  nodeSelector: {}
  # backup.tolerations Tolerations for Backup pods assignment
  # If set to, it takes priority over the `tolerations`
  tolerations: []
  ## Horizontal Pod Autoscaling parameters
  ## Horizontal Pod Autoscaling is used for autoscaling of the Backup deployment
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## The parameters below for autoscaling are used only when `backup.autoscaling.enabled=true`
  autoscaling:
    ## backup.autoscaling.enabled Enable Backup deployment autoscaling
    enabled: false
    ## backup.autoscaling.annotations Defines annotations that will be additionally added to Backup deployment HPA
    ## If set to, it takes priority over the `commonAnnotations`
    ## You can also use `tpl` as the value for the key
    annotations: {}
    ## backup.autoscaling.minReplicas Backup deployment autoscaling minimum number of replicas
    minReplicas: 2
    ## backup.autoscaling.maxReplicas Backup deployment autoscaling maximum number of replicas
    maxReplicas: 16
    targetCPU:
      ## backup.autoscaling.targetCPU.enabled Enable autoscaling of Backup deployment by CPU usage percentage
      enabled: true
      ## backup.autoscaling.targetCPU.utilizationPercentage Backup deployment autoscaling target CPU percentage
      utilizationPercentage: 70
    targetMemory:
      ## backup.autoscaling.targetMemory.enabled Enable autoscaling of Backup deployment by memory usage percentage
      enabled: false
      ## backup.autoscaling.targetMemory.utilizationPercentage Backup deployment autoscaling target memory percentage
      utilizationPercentage: 70
    ## backup.autoscaling.customMetricsType Custom, additional or external autoscaling metrics for the Backup deployment
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
    customMetricsType: []
    ## backup.autoscaling.behavior Configuring Backup deployment scaling behavior policies for the `scaleDown` and `scaleUp` fields
    ## If not set the default values are used:
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#default-behavior
    behavior: {}
  # Backup container image parameters
  image:
    # backup.image.repository Backup container image repository
    repository: onlyoffice/docspace-backup
    # backup.image.tag Backup container image tag
    # If set to, it takes priority over the `images.tag`
    tag: ""
    # backup.image.pullPolicy Backup container image pull policy
    pullPolicy: IfNotPresent
  # Configure a Security Context for containers in Backup Pods
  # If set to, it takes priority over the `containerSecurityContext`
  containerSecurityContext:
    # backup.containerSecurityContext.enabled Enable security context for containers in Backup pods
    enabled: false
  # backup.lifecycleHooks Defines the Backup container lifecycle hooks
  # ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
  # It is used to trigger events to run at certain points in a container's lifecycle
  # There are two hooks that are exposed: `PostStart` and `PreStop`
  lifecycleHooks: {}
  # backup.containerPorts.app Backup container port
  containerPorts:
    app: 5050
  # Probe used for the Backup container: startup, readiness and liveness probes
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # The parameters below for startup probes are used only when `backup.startupProbe.enabled=true`
  startupProbe:
    # backup.startupProbe.enabled Enable startupProbe for Backup container
    enabled: true
    httpGet:
      # backup.startupProbe.httpGet.path Checking the path for startupProbe
      path: /health
      # backup.startupProbe.httpGet.port Checking the port for startupProbe
      port: 5050
    # backup.startupProbe.failureThreshold Failure threshold for startupProbe
    failureThreshold: 30
    # backup.startupProbe.periodSeconds Period seconds for startupProbe
    periodSeconds: 10
  # The parameters below for readiness probes are used only when `backup.readinessProbe.enabled=true`
  readinessProbe:
    # backup.readinessProbe.enabled Enable readinessProbe for Backup container
    enabled: true
    # backup.readinessProbe.failureThreshold Failure threshold for readinessProbe
    failureThreshold: 2
    httpGet:
      # backup.readinessProbe.httpGet.path Checking the path for readinessProbe
      path: /health
      # backup.readinessProbe.httpGet.port Checking the port for readinessProbe
      port: 5050
    # backup.readinessProbe.periodSeconds Period seconds for readinessProbe
    periodSeconds: 10
    # backup.readinessProbe.successThreshold Success threshold for readinessProbe
    successThreshold: 1
    # backup.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    timeoutSeconds: 5
  # The parameters below for liveness probes are used only when `backup.livenessProbe.enabled=true`
  livenessProbe:
    # backup.livenessProbe.enabled Enable livenessProbe for Backup container
    enabled: true
    # backup.livenessProbe.failureThreshold Failure threshold for livenessProbe
    failureThreshold: 3
    httpGet:
      # backup.livenessProbe.httpGet.path Checking the path for livenessProbe
      path: /health
      # backup.livenessProbe.httpGet.port Checking the port for livenessProbe
      port: 5050
    # backup.livenessProbe.periodSeconds Period seconds for livenessProbe
    periodSeconds: 10
    # backup.livenessProbe.successThreshold Success threshold for livenessProbe
    successThreshold: 1
    # backup.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    timeoutSeconds: 5
  # Backup container resource requests and limits
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  # backup.resources.requests The requested resources for the Backup container
  # backup.resources.limits The resources limits for the Backup container
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "2Gi"
      cpu: "2000m"
  ## backup.extraEnvVars An array with extra env variables for the Backup container
  extraEnvVars: []
  ## backup.extraVolumes An array with extra volumes for the Backup container
  extraVolumes: []
  ## backup.extraVolumeMounts An array with extra volume mounts for the Backup container
  extraVolumeMounts: []
  # backup.mysqlUser Database user who will be used by the Backup service to connect to the database
  # If not set, `connections.mysqlUser` will be used
  # If set to, it takes priority over the `connections.mysqlUser`
  mysqlUser: ""

# DocSpace Backup Background Tasks application parameters
# This block defines the parameters common to all the Pods of this application
#
backupBackgroundTasks:
  # backupBackgroundTasks.enabled Enables Backup Background Tasks installation
  enabled: true
  # backupBackgroundTasks.kind The controller used for deploy
  # Possible values are `Deployment` (default) or `StatefulSet`
  kind: Deployment
  # backupBackgroundTasks.annotations Defines annotations that will be additionally added to Backup Background Tasks deploy
  # If set to, it takes priority over the `commonAnnotations`
  # You can also use `tpl` as the value for the key
  annotations: {}
  # backupBackgroundTasks.replicaCount Number of Backup Background Tasks replicas to deploy
  # If the `backupBackgroundTasks.autoscaling.enabled` parameter is enabled, it is ignored
  replicaCount: 2
  # Update strategy used to replace old Pods by new ones
  updateStrategy:
    # backupBackgroundTasks.updateStrategy.type Backup Background Tasks update strategy type
    # Allowed values: `RollingUpdate` or `OnDelete` if `backupBackgroundTasks.kind` is set to `StatefulSet` and `RollingUpdate` or `Recreate` if `backupBackgroundTasks.kind` is set to `Deployment`
    type: RollingUpdate
    # backupBackgroundTasks.updateStrategy.rollingUpdate Used only when `backupBackgroundTasks.updateStrategy.type=RollingUpdate`
    rollingUpdate:
      # backupBackgroundTasks.updateStrategy.rollingUpdate.maxUnavailable Maximum number of Backup Background Tasks Pods unavailable during the update process
      maxUnavailable: 25%
      # backupBackgroundTasks.updateStrategy.rollingUpdate.maxSurge Maximum number of Backup Background Tasks Pods created over the desired number of Pods
      maxSurge: 25%
  # backupBackgroundTasks.podManagementPolicy The Backup Background Tasks Pods scaling operations policy
  # Used if `backupBackgroundTasks.kind` is set to `StatefulSet`
  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  podManagementPolicy: OrderedReady
  # backupBackgroundTasks.podAnnotations Map of annotations to add to the Backup Background Tasks pods
  podAnnotations: {}
  # Configure a Security Context for the Backup Background Tasks Pods
  # If set to, it takes priority over the `podSecurityContext`
  podSecurityContext:
    # backupBackgroundTasks.podSecurityContext.enabled Enable security context for the Backup Background Tasks pods
    enabled: false
  # backupBackgroundTasks.customPodAntiAffinity Prohibiting the scheduling of Backup Background Tasks Pods relative to other Pods containing the specified labels on the same node
  customPodAntiAffinity: {}
  # Pod affinity rules for backupBackgroundTasks Pod scheduling by nodes relative to other Pods
  # Pod affinity allow you to constrain which nodes Backup Background Tasks Pods can be scheduled on based on the labels of Pods already running on that node
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # Example:
  # podAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     podAffinityTerm:
  #       labelSelector:
  #         matchExpressions:
  #         - key: app
  #           operator: NotIn
  #           values:
  #           - database
  #       topologyKey: kubernetes.io/hostname
  podAffinity: {}
  # Node affinity rules for Backup Background Tasks Pod scheduling by nodes
  # Node affinity allow you to constrain which nodes Backup Background Tasks Pods can be scheduled on based on node labels
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  # Example:
  # nodeAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     preference:
  #       matchExpressions:
  #       - key: kubernetes.io/name
  #         operator: In
  #         values:
  #         - name1
  #         - name2
  nodeAffinity: {}
  # backupBackgroundTasks.nodeSelector Node labels for Backup Background Tasks pods assignment
  # If set to, it takes priority over the `nodeSelector`
  nodeSelector: {}
  # backupBackgroundTasks.tolerations Tolerations for Backup Background Tasks pods assignment
  # If set to, it takes priority over the `tolerations`
  tolerations: []
  ## Horizontal Pod Autoscaling parameters
  ## Horizontal Pod Autoscaling is used for autoscaling of the Backup Background Tasks deployment
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## The parameters below for autoscaling are used only when `backupBackgroundTasks.autoscaling.enabled=true`
  autoscaling:
    ## backupBackgroundTasks.autoscaling.enabled Enable Backup Background Tasks deployment autoscaling
    enabled: false
    ## backupBackgroundTasks.autoscaling.annotations Defines annotations that will be additionally added to Backup Background Tasks deployment HPA
    ## If set to, it takes priority over the `commonAnnotations`
    ## You can also use `tpl` as the value for the key
    annotations: {}
    ## backupBackgroundTasks.autoscaling.minReplicas Backup Background Tasks deployment autoscaling minimum number of replicas
    minReplicas: 2
    ## backupBackgroundTasks.autoscaling.maxReplicas Backup Background Tasks deployment autoscaling maximum number of replicas
    maxReplicas: 16
    targetCPU:
      ## backupBackgroundTasks.autoscaling.targetCPU.enabled Enable autoscaling of Backup Background Tasks deployment by CPU usage percentage
      enabled: true
      ## backupBackgroundTasks.autoscaling.targetCPU.utilizationPercentage Backup Background Tasks deployment autoscaling target CPU percentage
      utilizationPercentage: 70
    targetMemory:
      ## backupBackgroundTasks.autoscaling.targetMemory.enabled Enable autoscaling of Backup Background Tasks deployment by memory usage percentage
      enabled: false
      ## backupBackgroundTasks.autoscaling.targetMemory.utilizationPercentage Backup Background Tasks deployment autoscaling target memory percentage
      utilizationPercentage: 70
    ## backupBackgroundTasks.autoscaling.customMetricsType Custom, additional or external autoscaling metrics for the Backup Background Tasks deployment
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
    customMetricsType: []
    ## backupBackgroundTasks.autoscaling.behavior Configuring Backup Background Tasks deployment scaling behavior policies for the `scaleDown` and `scaleUp` fields
    ## If not set the default values are used:
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#default-behavior
    behavior: {}
  # Backup Background Tasks container image parameters
  image:
    # backupBackgroundTasks.image.repository Backup Background Tasks container image repository
    repository: onlyoffice/docspace-backup-background
    # backupBackgroundTasks.image.tag Backup Background Tasks container image tag
    # If set to, it takes priority over the `images.tag`
    tag: ""
    # backupBackgroundTasks.image.pullPolicy Backup Background Tasks container image pull policy
    pullPolicy: IfNotPresent
  # Configure a Security Context for containers in Backup Background Tasks Pods
  # If set to, it takes priority over the `containerSecurityContext`
  containerSecurityContext:
    # backupBackgroundTasks.containerSecurityContext.enabled Enable security context for containers in Backup Background Tasks pods
    enabled: false
  # backupBackgroundTasks.lifecycleHooks Defines the Backup Background Tasks container lifecycle hooks
  # ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
  # It is used to trigger events to run at certain points in a container's lifecycle
  # There are two hooks that are exposed: `PostStart` and `PreStop`
  lifecycleHooks: {}
  # backupBackgroundTasks.containerPorts.app Backup Background Tasks container port
  containerPorts:
    app: 5050
  # Probe used for the Backup Background Tasks container: startup, readiness and liveness probes
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # The parameters below for startup probes are used only when `backupBackgroundTasks.startupProbe.enabled=true`
  startupProbe:
    # backupBackgroundTasks.startupProbe.enabled Enable startupProbe for Backup Background Tasks container
    enabled: true
    httpGet:
      # backupBackgroundTasks.startupProbe.httpGet.path Checking the path for startupProbe
      path: /health
      # backupBackgroundTasks.startupProbe.httpGet.port Checking the port for startupProbe
      port: 5050
    # backupBackgroundTasks.startupProbe.failureThreshold Failure threshold for startupProbe
    failureThreshold: 30
    # backupBackgroundTasks.startupProbe.periodSeconds Period seconds for startupProbe
    periodSeconds: 10
  # The parameters below for readiness probes are used only when `backupBackgroundTasks.readinessProbe.enabled=true`
  readinessProbe:
    # backupBackgroundTasks.readinessProbe.enabled Enable readinessProbe for Backup Background Tasks container
    enabled: true
    # backupBackgroundTasks.readinessProbe.failureThreshold Failure threshold for readinessProbe
    failureThreshold: 2
    httpGet:
      # backupBackgroundTasks.readinessProbe.httpGet.path Checking the path for readinessProbe
      path: /health
      # backupBackgroundTasks.readinessProbe.httpGet.port Checking the port for readinessProbe
      port: 5050
    # backupBackgroundTasks.readinessProbe.periodSeconds Period seconds for readinessProbe
    periodSeconds: 10
    # backupBackgroundTasks.readinessProbe.successThreshold Success threshold for readinessProbe
    successThreshold: 1
    # backupBackgroundTasks.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    timeoutSeconds: 5
  # The parameters below for liveness probes are used only when `backupBackgroundTasks.livenessProbe.enabled=true`
  livenessProbe:
    # backupBackgroundTasks.livenessProbe.enabled Enable livenessProbe for Backup Background Tasks container
    enabled: true
    # backupBackgroundTasks.livenessProbe.failureThreshold Failure threshold for livenessProbe
    failureThreshold: 3
    httpGet:
      # backupBackgroundTasks.livenessProbe.httpGet.path Checking the path for livenessProbe
      path: /health
      # backupBackgroundTasks.livenessProbe.httpGet.port Checking the port for livenessProbe
      port: 5050
    # backupBackgroundTasks.livenessProbe.periodSeconds Period seconds for livenessProbe
    periodSeconds: 10
    # backupBackgroundTasks.livenessProbe.successThreshold Success threshold for livenessProbe
    successThreshold: 1
    # backupBackgroundTasks.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    timeoutSeconds: 5
  # Backup Background Tasks container resource requests and limits
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  # backupBackgroundTasks.resources.requests The requested resources for the Backup Background Tasks container
  # backupBackgroundTasks.resources.limits The resources limits for the Backup Background Tasks container
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "2Gi"
      cpu: "2000m"
  ## backupBackgroundTasks.extraEnvVars An array with extra env variables for the Backup Background Tasks container
  extraEnvVars: []
  ## backupBackgroundTasks.extraVolumes An array with extra volumes for the Backup Background Tasks container
  extraVolumes: []
  ## backupBackgroundTasks.extraVolumeMounts An array with extra volume mounts for the Backup Background Tasks container
  extraVolumeMounts: []
  # backupBackgroundTasks.mysqlUser Database user who will be used by the Backup Background Tasks service to connect to the database
  # If not set, `connections.mysqlUser` will be used
  # If set to, it takes priority over the `connections.mysqlUser`
  mysqlUser: ""

# DocSpace Clear Events application parameters
# This block defines the parameters common to all the Pods of this application
#
clearEvents:
  # clearEvents.enabled Enables Clear Events installation
  enabled: true
  # clearEvents.kind The controller used for deploy
  # Possible values are `Deployment` (default) or `StatefulSet`
  kind: Deployment
  # clearEvents.annotations Defines annotations that will be additionally added to Clear Events deploy
  # If set to, it takes priority over the `commonAnnotations`
  # You can also use `tpl` as the value for the key
  annotations: {}
  # clearEvents.replicaCount Number of Clear Events replicas to deploy
  # If the `clearEvents.autoscaling.enabled` parameter is enabled, it is ignored
  replicaCount: 2
  # Update strategy used to replace old Pods by new ones
  updateStrategy:
    # clearEvents.updateStrategy.type Clear Events update strategy type
    # Allowed values: `RollingUpdate` or `OnDelete` if `clearEvents.kind` is set to `StatefulSet` and `RollingUpdate` or `Recreate` if `clearEvents.kind` is set to `Deployment`
    type: RollingUpdate
    # clearEvents.updateStrategy.rollingUpdate Used only when `clearEvents.updateStrategy.type=RollingUpdate`
    rollingUpdate:
      # clearEvents.updateStrategy.rollingUpdate.maxUnavailable Maximum number of Clear Events Pods unavailable during the update process
      maxUnavailable: 25%
      # clearEvents.updateStrategy.rollingUpdate.maxSurge Maximum number of Clear Events Pods created over the desired number of Pods
      maxSurge: 25%
  # clearEvents.podManagementPolicy The Clear Events Pods scaling operations policy
  # Used if `clearEvents.kind` is set to `StatefulSet`
  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  podManagementPolicy: OrderedReady
  # clearEvents.podAnnotations Map of annotations to add to the Clear Events pods
  podAnnotations: {}
  # Configure a Security Context for the Clear Events Pods
  # If set to, it takes priority over the `podSecurityContext`
  podSecurityContext:
    # clearEvents.podSecurityContext.enabled Enable security context for the Clear Events pods
    enabled: false
  # clearEvents.customPodAntiAffinity Prohibiting the scheduling of Clear Events Pods relative to other Pods containing the specified labels on the same node
  customPodAntiAffinity: {}
  # Pod affinity rules for Clear Events Pod scheduling by nodes relative to other Pods
  # Pod affinity allow you to constrain which nodes Clear Events Pods can be scheduled on based on the labels of Pods already running on that node
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # Example:
  # podAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     podAffinityTerm:
  #       labelSelector:
  #         matchExpressions:
  #         - key: app
  #           operator: NotIn
  #           values:
  #           - database
  #       topologyKey: kubernetes.io/hostname
  podAffinity: {}
  # Node affinity rules for Clear Events Pod scheduling by nodes
  # Node affinity allow you to constrain which nodes Clear Events Pods can be scheduled on based on node labels
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  # Example:
  # nodeAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     preference:
  #       matchExpressions:
  #       - key: kubernetes.io/name
  #         operator: In
  #         values:
  #         - name1
  #         - name2
  nodeAffinity: {}
  # clearEvents.nodeSelector Node labels for Clear Events pods assignment
  # If set to, it takes priority over the `nodeSelector`
  nodeSelector: {}
  # clearEvents.tolerations Tolerations for Clear Events pods assignment
  # If set to, it takes priority over the `tolerations`
  tolerations: []
  ## Horizontal Pod Autoscaling parameters
  ## Horizontal Pod Autoscaling is used for autoscaling of the Clear Events deployment
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## The parameters below for autoscaling are used only when `clearEvents.autoscaling.enabled=true`
  autoscaling:
    ## clearEvents.autoscaling.enabled Enable Clear Events deployment autoscaling
    enabled: false
    ## clearEvents.autoscaling.annotations Defines annotations that will be additionally added to Clear Events deployment HPA
    ## If set to, it takes priority over the `commonAnnotations`
    ## You can also use `tpl` as the value for the key
    annotations: {}
    ## clearEvents.autoscaling.minReplicas Clear Events deployment autoscaling minimum number of replicas
    minReplicas: 2
    ## clearEvents.autoscaling.maxReplicas Clear Events deployment autoscaling maximum number of replicas
    maxReplicas: 16
    targetCPU:
      ## clearEvents.autoscaling.targetCPU.enabled Enable autoscaling of Clear Events deployment by CPU usage percentage
      enabled: true
      ## clearEvents.autoscaling.targetCPU.utilizationPercentage Clear Events deployment autoscaling target CPU percentage
      utilizationPercentage: 70
    targetMemory:
      ## clearEvents.autoscaling.targetMemory.enabled Enable autoscaling of Clear Events deployment by memory usage percentage
      enabled: false
      ## clearEvents.autoscaling.targetMemory.utilizationPercentage Clear Events deployment autoscaling target memory percentage
      utilizationPercentage: 70
    ## clearEvents.autoscaling.customMetricsType Custom, additional or external autoscaling metrics for the Clear Events deployment
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
    customMetricsType: []
    ## clearEvents.autoscaling.behavior Configuring Clear Events deployment scaling behavior policies for the `scaleDown` and `scaleUp` fields
    ## If not set the default values are used:
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#default-behavior
    behavior: {}
  # Clear Events container image parameters
  image:
    # clearEvents.image.repository Clear Events container image repository
    repository: onlyoffice/docspace-clear-events
    # clearEvents.image.tag Clear Events container image tag
    # If set to, it takes priority over the `images.tag`
    tag: ""
    # clearEvents.image.pullPolicy Clear Events container image pull policy
    pullPolicy: IfNotPresent
  # Configure a Security Context for containers in Clear Events Pods
  # If set to, it takes priority over the `containerSecurityContext`
  containerSecurityContext:
    # clearEvents.containerSecurityContext.enabled Enable security context for containers in Clear Events pods
    enabled: false
  # clearEvents.lifecycleHooks Defines the Clear Events container lifecycle hooks
  # ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
  # It is used to trigger events to run at certain points in a container's lifecycle
  # There are two hooks that are exposed: `PostStart` and `PreStop`
  lifecycleHooks: {}
  # clearEvents.containerPorts.app Clear Events container port
  containerPorts:
    app: 5050
  # Probe used for the Clear Events container: startup, readiness and liveness probes
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # The parameters below for startup probes are used only when `clearEvents.startupProbe.enabled=true`
  startupProbe:
    # clearEvents.startupProbe.enabled Enable startupProbe for Clear Events container
    enabled: true
    httpGet:
      # clearEvents.startupProbe.httpGet.path Checking the path for startupProbe
      path: /health
      # clearEvents.startupProbe.httpGet.port Checking the port for startupProbe
      port: 5050
    # clearEvents.startupProbe.failureThreshold Failure threshold for startupProbe
    failureThreshold: 30
    # clearEvents.startupProbe.periodSeconds Period seconds for startupProbe
    periodSeconds: 10
  # The parameters below for readiness probes are used only when `clearEvents.readinessProbe.enabled=true`
  readinessProbe:
    # clearEvents.readinessProbe.enabled Enable readinessProbe for Clear Events container
    enabled: true
    # clearEvents.readinessProbe.failureThreshold Failure threshold for readinessProbe
    failureThreshold: 2
    httpGet:
      # clearEvents.readinessProbe.httpGet.path Checking the path for readinessProbe
      path: /health
      # clearEvents.readinessProbe.httpGet.port Checking the port for readinessProbe
      port: 5050
    # clearEvents.readinessProbe.periodSeconds Period seconds for readinessProbe
    periodSeconds: 10
    # clearEvents.readinessProbe.successThreshold Success threshold for readinessProbe
    successThreshold: 1
    # clearEvents.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    timeoutSeconds: 5
  # The parameters below for liveness probes are used only when `clearEvents.livenessProbe.enabled=true`
  livenessProbe:
    # clearEvents.livenessProbe.enabled Enable livenessProbe for Clear Events container
    enabled: true
    # clearEvents.livenessProbe.failureThreshold Failure threshold for livenessProbe
    failureThreshold: 3
    httpGet:
      # clearEvents.livenessProbe.httpGet.path Checking the path for livenessProbe
      path: /health
      # clearEvents.livenessProbe.httpGet.port Checking the port for livenessProbe
      port: 5050
    # clearEvents.livenessProbe.periodSeconds Period seconds for livenessProbe
    periodSeconds: 10
    # clearEvents.livenessProbe.successThreshold Success threshold for livenessProbe
    successThreshold: 1
    # clearEvents.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    timeoutSeconds: 5
  # Clear Events container resource requests and limits
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  # clearEvents.resources.requests The requested resources for the Clear Events container
  # clearEvents.resources.limits The resources limits for the Clear Events container
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "2Gi"
      cpu: "2000m"
  ## clearEvents.extraEnvVars An array with extra env variables for the Clear Events container
  extraEnvVars: []
  ## clearEvents.extraVolumes An array with extra volumes for the Clear Events container
  extraVolumes: []
  ## clearEvents.extraVolumeMounts An array with extra volume mounts for the Clear Events container
  extraVolumeMounts: []
  # clearEvents.mysqlUser Database user who will be used by the Clear Events service to connect to the database
  # If not set, `connections.mysqlUser` will be used
  # If set to, it takes priority over the `connections.mysqlUser`
  mysqlUser: ""

# DocSpace Doceditor application parameters
# This block defines the parameters common to all the Pods of this application
#
doceditor:
  # doceditor.enabled Enables Doceditor installation
  enabled: true
  # doceditor.kind The controller used for deploy
  # Possible values are `Deployment` (default) or `StatefulSet`
  kind: Deployment
  # doceditor.annotations Defines annotations that will be additionally added to Doceditor deploy
  # If set to, it takes priority over the `commonAnnotations`
  # You can also use `tpl` as the value for the key
  annotations: {}
  # doceditor.replicaCount Number of Doceditor replicas to deploy
  # If the `doceditor.autoscaling.enabled` parameter is enabled, it is ignored
  replicaCount: 2
  # Update strategy used to replace old Pods by new ones
  updateStrategy:
    # doceditor.updateStrategy.type Doceditor update strategy type
    # Allowed values: `RollingUpdate` or `OnDelete` if `doceditor.kind` is set to `StatefulSet` and `RollingUpdate` or `Recreate` if `doceditor.kind` is set to `Deployment`
    type: RollingUpdate
    # doceditor.updateStrategy.rollingUpdate Used only when `doceditor.updateStrategy.type=RollingUpdate`
    rollingUpdate:
      # doceditor.updateStrategy.rollingUpdate.maxUnavailable Maximum number of Doceditor Pods unavailable during the update process
      maxUnavailable: 25%
      # doceditor.updateStrategy.rollingUpdate.maxSurge Maximum number of Doceditor Pods created over the desired number of Pods
      maxSurge: 25%
  # doceditor.podManagementPolicy The Doceditor Pods scaling operations policy
  # Used if `doceditor.kind` is set to `StatefulSet`
  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  podManagementPolicy: OrderedReady
  # doceditor.podAnnotations Map of annotations to add to the Doceditor pods
  podAnnotations: {}
  # Configure a Security Context for the Doceditor Pods
  # If set to, it takes priority over the `podSecurityContext`
  podSecurityContext:
    # doceditor.podSecurityContext.enabled Enable security context for the Doceditor pods
    enabled: false
  # doceditor.customPodAntiAffinity Prohibiting the scheduling of Doceditor Pods relative to other Pods containing the specified labels on the same node
  customPodAntiAffinity: {}
  # Pod affinity rules for Doceditor Pod scheduling by nodes relative to other Pods
  # Pod affinity allow you to constrain which nodes Doceditor Pods can be scheduled on based on the labels of Pods already running on that node
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # Example:
  # podAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     podAffinityTerm:
  #       labelSelector:
  #         matchExpressions:
  #         - key: app
  #           operator: NotIn
  #           values:
  #           - database
  #       topologyKey: kubernetes.io/hostname
  podAffinity: {}
  # Node affinity rules for Doceditor Pod scheduling by nodes
  # Node affinity allow you to constrain which nodes Doceditor Pods can be scheduled on based on node labels
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  # Example:
  # nodeAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     preference:
  #       matchExpressions:
  #       - key: kubernetes.io/name
  #         operator: In
  #         values:
  #         - name1
  #         - name2
  nodeAffinity: {}
  # doceditor.nodeSelector Node labels for Doceditor pods assignment
  # If set to, it takes priority over the `nodeSelector`
  nodeSelector: {}
  # doceditor.tolerations Tolerations for Doceditor pods assignment
  # If set to, it takes priority over the `tolerations`
  tolerations: []
  ## Horizontal Pod Autoscaling parameters
  ## Horizontal Pod Autoscaling is used for autoscaling of the Doceditor deployment
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## The parameters below for autoscaling are used only when `doceditor.autoscaling.enabled=true`
  autoscaling:
    ## doceditor.autoscaling.enabled Enable Doceditor deployment autoscaling
    enabled: false
    ## doceditor.autoscaling.annotations Defines annotations that will be additionally added to Doceditor deployment HPA
    ## If set to, it takes priority over the `commonAnnotations`
    ## You can also use `tpl` as the value for the key
    annotations: {}
    ## doceditor.autoscaling.minReplicas Doceditor deployment autoscaling minimum number of replicas
    minReplicas: 2
    ## doceditor.autoscaling.maxReplicas Doceditor deployment autoscaling maximum number of replicas
    maxReplicas: 16
    targetCPU:
      ## doceditor.autoscaling.targetCPU.enabled Enable autoscaling of Doceditor deployment by CPU usage percentage
      enabled: true
      ## doceditor.autoscaling.targetCPU.utilizationPercentage Doceditor deployment autoscaling target CPU percentage
      utilizationPercentage: 70
    targetMemory:
      ## doceditor.autoscaling.targetMemory.enabled Enable autoscaling of Doceditor deployment by memory usage percentage
      enabled: false
      ## doceditor.autoscaling.targetMemory.utilizationPercentage Doceditor deployment autoscaling target memory percentage
      utilizationPercentage: 70
    ## doceditor.autoscaling.customMetricsType Custom, additional or external autoscaling metrics for the Doceditor deployment
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
    customMetricsType: []
    ## doceditor.autoscaling.behavior Configuring Doceditor deployment scaling behavior policies for the `scaleDown` and `scaleUp` fields
    ## If not set the default values are used:
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#default-behavior
    behavior: {}
  # Doceditor container image parameters
  image:
    # doceditor.image.repository Doceditor container image repository
    repository: onlyoffice/docspace-doceditor
    # doceditor.image.tag Doceditor container image tag
    # If set to, it takes priority over the `images.tag`
    tag: ""
    # doceditor.image.pullPolicy Doceditor container image pull policy
    pullPolicy: IfNotPresent
  # Configure a Security Context for containers in Doceditor Pods
  # If set to, it takes priority over the `containerSecurityContext`
  containerSecurityContext:
    # doceditor.containerSecurityContext.enabled Enable security context for containers in Doceditor pods
    enabled: false
  # doceditor.lifecycleHooks Defines the Doceditor container lifecycle hooks
  # ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
  # It is used to trigger events to run at certain points in a container's lifecycle
  # There are two hooks that are exposed: `PostStart` and `PreStop`
  lifecycleHooks: {}
  containerPorts:
    # doceditor.containerPorts.app Doceditor container port
    app: 5050
    # doceditor.containerPorts.doceditor Doceditor additional container port
    doceditor: 5013
  # Probe used for the Doceditor container: startup, readiness and liveness probes
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # The parameters below for startup probes are used only when `doceditor.startupProbe.enabled=true`
  startupProbe:
    # doceditor.startupProbe.enabled Enable startupProbe for Doceditor container
    enabled: true
    httpGet:
      # doceditor.startupProbe.httpGet.path Checking the path for startupProbe
      path: /doceditor/health
      # doceditor.startupProbe.httpGet.port Checking the port for startupProbe
      port: 5013
    # doceditor.startupProbe.failureThreshold Failure threshold for startupProbe
    failureThreshold: 30
    # doceditor.startupProbe.periodSeconds Period seconds for startupProbe
    periodSeconds: 10
  # The parameters below for readiness probes are used only when `doceditor.readinessProbe.enabled=true`
  readinessProbe:
    # doceditor.readinessProbe.enabled Enable readinessProbe for Doceditor container
    enabled: true
    # doceditor.readinessProbe.failureThreshold Failure threshold for readinessProbe
    failureThreshold: 2
    httpGet:
      # doceditor.readinessProbe.httpGet.path Checking the path for readinessProbe
      path: /doceditor/health
      # doceditor.readinessProbe.httpGet.port Checking the port for readinessProbe
      port: 5013
    # doceditor.readinessProbe.periodSeconds Period seconds for readinessProbe
    periodSeconds: 10
    # doceditor.readinessProbe.successThreshold Success threshold for readinessProbe
    successThreshold: 1
    # doceditor.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    timeoutSeconds: 5
  # The parameters below for liveness probes are used only when `doceditor.livenessProbe.enabled=true`
  livenessProbe:
    # doceditor.livenessProbe.enabled Enable livenessProbe for Doceditor container
    enabled: true
    # doceditor.livenessProbe.failureThreshold Failure threshold for livenessProbe
    failureThreshold: 3
    httpGet:
      # doceditor.livenessProbe.httpGet.path Checking the path for livenessProbe
      path: /doceditor/health
      # doceditor.livenessProbe.httpGet.port Checking the port for livenessProbe
      port: 5013
    # doceditor.livenessProbe.periodSeconds Period seconds for livenessProbe
    periodSeconds: 10
    # doceditor.livenessProbe.successThreshold Success threshold for livenessProbe
    successThreshold: 1
    # doceditor.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    timeoutSeconds: 5
  # Doceditor container resource requests and limits
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  # doceditor.resources.requests The requested resources for the Doceditor container
  # doceditor.resources.limits The resources limits for the Doceditor container
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "2Gi"
      cpu: "2000m"
  ## doceditor.extraEnvVars An array with extra env variables for the Doceditor container
  extraEnvVars: []
  ## doceditor.extraVolumes An array with extra volumes for the Doceditor container
  extraVolumes: []
  ## doceditor.extraVolumeMounts An array with extra volume mounts for the Doceditor container
  extraVolumeMounts: []
  # doceditor.mysqlUser Database user who will be used by the Doceditor service to connect to the database
  # If not set, `connections.mysqlUser` will be used
  # If set to, it takes priority over the `connections.mysqlUser`
  mysqlUser: ""

# DocSpace Files Services application parameters
# This block defines the parameters common to all the Pods of this application
#
filesServices:
  # filesServices.enabled Enables Files Services installation
  enabled: true
  # filesServices.kind The controller used for deploy
  # Possible values are `Deployment` (default) or `StatefulSet`
  kind: Deployment
  # filesServices.annotations Defines annotations that will be additionally added to Files Services deploy
  # If set to, it takes priority over the `commonAnnotations`
  # You can also use `tpl` as the value for the key
  annotations: {}
  # filesServices.replicaCount Number of Files Services replicas to deploy
  # If the `filesServices.autoscaling.enabled` parameter is enabled, it is ignored
  replicaCount: 2
  # Update strategy used to replace old Pods by new ones
  updateStrategy:
    # filesServices.updateStrategy.type Files Services update strategy type
    # Allowed values: `RollingUpdate` or `OnDelete` if `filesServices.kind` is set to `StatefulSet` and `RollingUpdate` or `Recreate` if `filesServices.kind` is set to `Deployment`
    type: RollingUpdate
    # filesServices.updateStrategy.rollingUpdate Used only when `filesServices.updateStrategy.type=RollingUpdate`
    rollingUpdate:
      # filesServices.updateStrategy.rollingUpdate.maxUnavailable Maximum number of Files Services Pods unavailable during the update process
      maxUnavailable: 25%
      # filesServices.updateStrategy.rollingUpdate.maxSurge Maximum number of Files Services Pods created over the desired number of Pods
      maxSurge: 25%
  # filesServices.podManagementPolicy The Files Services Pods scaling operations policy
  # Used if `filesServices.kind` is set to `StatefulSet`
  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  podManagementPolicy: OrderedReady
  # filesServices.podAnnotations Map of annotations to add to the Files Services pods
  podAnnotations: {}
  # Configure a Security Context for the Files Services Pods
  # If set to, it takes priority over the `podSecurityContext`
  podSecurityContext:
    # filesServices.podSecurityContext.enabled Enable security context for the Files Services pods
    enabled: false
  # filesServices.customPodAntiAffinity Prohibiting the scheduling of Files Services Pods relative to other Pods containing the specified labels on the same node
  customPodAntiAffinity: {}
  # Pod affinity rules for Files Services Pod scheduling by nodes relative to other Pods
  # Pod affinity allow you to constrain which nodes Files Services Pods can be scheduled on based on the labels of Pods already running on that node
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # Example:
  # podAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     podAffinityTerm:
  #       labelSelector:
  #         matchExpressions:
  #         - key: app
  #           operator: NotIn
  #           values:
  #           - database
  #       topologyKey: kubernetes.io/hostname
  podAffinity: {}
  # Node affinity rules for Files Services Pod scheduling by nodes
  # Node affinity allow you to constrain which nodes Files Services Pods can be scheduled on based on node labels
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  # Example:
  # nodeAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     preference:
  #       matchExpressions:
  #       - key: kubernetes.io/name
  #         operator: In
  #         values:
  #         - name1
  #         - name2
  nodeAffinity: {}
  # filesServices.nodeSelector Node labels for Files Services pods assignment
  # If set to, it takes priority over the `nodeSelector`
  nodeSelector: {}
  # filesServices.tolerations Tolerations for Files Services pods assignment
  # If set to, it takes priority over the `tolerations`
  tolerations: []
  ## Horizontal Pod Autoscaling parameters
  ## Horizontal Pod Autoscaling is used for autoscaling of the Files Services deployment
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## The parameters below for autoscaling are used only when `filesServices.autoscaling.enabled=true`
  autoscaling:
    ## filesServices.autoscaling.enabled Enable Files Services deployment autoscaling
    enabled: false
    ## filesServices.autoscaling.annotations Defines annotations that will be additionally added to Files Services deployment HPA
    ## If set to, it takes priority over the `commonAnnotations`
    ## You can also use `tpl` as the value for the key
    annotations: {}
    ## filesServices.autoscaling.minReplicas Files Services deployment autoscaling minimum number of replicas
    minReplicas: 2
    ## filesServices.autoscaling.maxReplicas Files Services deployment autoscaling maximum number of replicas
    maxReplicas: 16
    targetCPU:
      ## filesServices.autoscaling.targetCPU.enabled Enable autoscaling of Files Services deployment by CPU usage percentage
      enabled: true
      ## filesServices.autoscaling.targetCPU.utilizationPercentage Files Services deployment autoscaling target CPU percentage
      utilizationPercentage: 70
    targetMemory:
      ## filesServices.autoscaling.targetMemory.enabled Enable autoscaling of Files Services deployment by memory usage percentage
      enabled: false
      ## filesServices.autoscaling.targetMemory.utilizationPercentage Files Services deployment autoscaling target memory percentage
      utilizationPercentage: 70
    ## filesServices.autoscaling.customMetricsType Custom, additional or external autoscaling metrics for the Files Services deployment
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
    customMetricsType: []
    ## filesServices.autoscaling.behavior Configuring Files Services deployment scaling behavior policies for the `scaleDown` and `scaleUp` fields
    ## If not set the default values are used:
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#default-behavior
    behavior: {}
  # Files Services container image parameters
  image:
    # filesServices.image.repository Files Services container image repository
    repository: onlyoffice/docspace-files-services
    # filesServices.image.tag Files Services container image tag
    # If set to, it takes priority over the `images.tag`
    tag: ""
    # filesServices.image.pullPolicy Files Services container image pull policy
    pullPolicy: IfNotPresent
  # Configure a Security Context for containers in Files Services Pods
  # If set to, it takes priority over the `containerSecurityContext`
  containerSecurityContext:
    # filesServices.containerSecurityContext.enabled Enable security context for containers in Files Services pods
    enabled: false
  # filesServices.lifecycleHooks Defines the Files Services container lifecycle hooks
  # ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
  # It is used to trigger events to run at certain points in a container's lifecycle
  # There are two hooks that are exposed: `PostStart` and `PreStop`
  lifecycleHooks: {}
  # filesServices.containerPorts.app Files Services container port
  containerPorts:
    app: 5050
  # Probe used for the Files Services container: startup, readiness and liveness probes
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # The parameters below for startup probes are used only when `filesServices.startupProbe.enabled=true`
  startupProbe:
    # filesServices.startupProbe.enabled Enable startupProbe for Files Services container
    enabled: true
    httpGet:
      # filesServices.startupProbe.httpGet.path Checking the path for startupProbe
      path: /health
      # filesServices.startupProbe.httpGet.port Checking the port for startupProbe
      port: 5050
    # filesServices.startupProbe.failureThreshold Failure threshold for startupProbe
    failureThreshold: 30
    # filesServices.startupProbe.periodSeconds Period seconds for startupProbe
    periodSeconds: 10
  # The parameters below for readiness probes are used only when `filesServices.readinessProbe.enabled=true`
  readinessProbe:
    # filesServices.readinessProbe.enabled Enable readinessProbe for Files Services container
    enabled: true
    # filesServices.readinessProbe.failureThreshold Failure threshold for readinessProbe
    failureThreshold: 2
    httpGet:
      # filesServices.readinessProbe.httpGet.path Checking the path for readinessProbe
      path: /health
      # filesServices.readinessProbe.httpGet.port Checking the port for readinessProbe
      port: 5050
    # filesServices.readinessProbe.periodSeconds Period seconds for readinessProbe
    periodSeconds: 20
    # filesServices.readinessProbe.successThreshold Success threshold for readinessProbe
    successThreshold: 1
    # filesServices.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    timeoutSeconds: 5
  # The parameters below for liveness probes are used only when `filesServices.livenessProbe.enabled=true`
  livenessProbe:
    # filesServices.livenessProbe.enabled Enable livenessProbe for Files Services container
    enabled: true
    # filesServices.livenessProbe.failureThreshold Failure threshold for livenessProbe
    failureThreshold: 3
    httpGet:
      # filesServices.livenessProbe.httpGet.path Checking the path for livenessProbe
      path: /health
      # filesServices.livenessProbe.httpGet.port Checking the port for livenessProbe
      port: 5050
    # filesServices.livenessProbe.periodSeconds Period seconds for livenessProbe
    periodSeconds: 20
    # filesServices.livenessProbe.successThreshold Success threshold for livenessProbe
    successThreshold: 1
    # filesServices.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    timeoutSeconds: 5
  # Files Services container resource requests and limits
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  # filesServices.resources.requests The requested resources for the Files Services container
  # filesServices.resources.limits The resources limits for the Files Services container
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "2Gi"
      cpu: "2000m"
  ## filesServices.extraEnvVars An array with extra env variables for the Files Services container
  extraEnvVars: []
  ## filesServices.extraVolumes An array with extra volumes for the Files Services container
  extraVolumes: []
  ## filesServices.extraVolumeMounts An array with extra volume mounts for the Files Services container
  extraVolumeMounts: []
  # filesServices.mysqlUser Database user who will be used by the Files Services service to connect to the database
  # If not set, `connections.mysqlUser` will be used
  # If set to, it takes priority over the `connections.mysqlUser`
  mysqlUser: ""

# DocSpace Login application parameters
# This block defines the parameters common to all the Pods of this application
#
login:
  # login.enabled Enables Login installation
  enabled: true
  # login.kind The controller used for deploy
  # Possible values are `Deployment` (default) or `StatefulSet`
  kind: Deployment
  # login.annotations Defines annotations that will be additionally added to Login deploy
  # If set to, it takes priority over the `commonAnnotations`
  # You can also use `tpl` as the value for the key
  annotations: {}
  # login.replicaCount Number of Login replicas to deploy
  # If the `login.autoscaling.enabled` parameter is enabled, it is ignored
  replicaCount: 2
  # Update strategy used to replace old Pods by new ones
  updateStrategy:
    # login.updateStrategy.type Login update strategy type
    # Allowed values: `RollingUpdate` or `OnDelete` if `login.kind` is set to `StatefulSet` and `RollingUpdate` or `Recreate` if `login.kind` is set to `Deployment`
    type: RollingUpdate
    # login.updateStrategy.rollingUpdate Used only when `login.updateStrategy.type=RollingUpdate`
    rollingUpdate:
      # login.updateStrategy.rollingUpdate.maxUnavailable Maximum number of Login Pods unavailable during the update process
      maxUnavailable: 25%
      # login.updateStrategy.rollingUpdate.maxSurge Maximum number of Login Pods created over the desired number of Pods
      maxSurge: 25%
  # login.podManagementPolicy The Login Pods scaling operations policy
  # Used if `login.kind` is set to `StatefulSet`
  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  podManagementPolicy: OrderedReady
  # login.podAnnotations Map of annotations to add to the Login pods
  podAnnotations: {}
  # Configure a Security Context for the Login Pods
  # If set to, it takes priority over the `podSecurityContext`
  podSecurityContext:
    # login.podSecurityContext.enabled Enable security context for the Login pods
    enabled: false
  # login.customPodAntiAffinity Prohibiting the scheduling of Login Pods relative to other Pods containing the specified labels on the same node
  customPodAntiAffinity: {}
  # Pod affinity rules for Login Pod scheduling by nodes relative to other Pods
  # Pod affinity allow you to constrain which nodes Login Pods can be scheduled on based on the labels of Pods already running on that node
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # Example:
  # podAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     podAffinityTerm:
  #       labelSelector:
  #         matchExpressions:
  #         - key: app
  #           operator: NotIn
  #           values:
  #           - database
  #       topologyKey: kubernetes.io/hostname
  podAffinity: {}
  # Node affinity rules for Login Pod scheduling by nodes
  # Node affinity allow you to constrain which nodes Login Pods can be scheduled on based on node labels
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  # Example:
  # nodeAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     preference:
  #       matchExpressions:
  #       - key: kubernetes.io/name
  #         operator: In
  #         values:
  #         - name1
  #         - name2
  nodeAffinity: {}
  # login.nodeSelector Node labels for Login pods assignment
  # If set to, it takes priority over the `nodeSelector`
  nodeSelector: {}
  # login.tolerations Tolerations for Login pods assignment
  # If set to, it takes priority over the `tolerations`
  tolerations: []
  ## Horizontal Pod Autoscaling parameters
  ## Horizontal Pod Autoscaling is used for autoscaling of the Login deployment
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## The parameters below for autoscaling are used only when `login.autoscaling.enabled=true`
  autoscaling:
    ## login.autoscaling.enabled Enable Login deployment autoscaling
    enabled: false
    ## login.autoscaling.annotations Defines annotations that will be additionally added to Login deployment HPA
    ## If set to, it takes priority over the `commonAnnotations`
    ## You can also use `tpl` as the value for the key
    annotations: {}
    ## login.autoscaling.minReplicas Login deployment autoscaling minimum number of replicas
    minReplicas: 2
    ## login.autoscaling.maxReplicas Login deployment autoscaling maximum number of replicas
    maxReplicas: 16
    targetCPU:
      ## login.autoscaling.targetCPU.enabled Enable autoscaling of Login deployment by CPU usage percentage
      enabled: true
      ## login.autoscaling.targetCPU.utilizationPercentage Login deployment autoscaling target CPU percentage
      utilizationPercentage: 70
    targetMemory:
      ## login.autoscaling.targetMemory.enabled Enable autoscaling of Login deployment by memory usage percentage
      enabled: false
      ## login.autoscaling.targetMemory.utilizationPercentage Login deployment autoscaling target memory percentage
      utilizationPercentage: 70
    ## login.autoscaling.customMetricsType Custom, additional or external autoscaling metrics for the Login deployment
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
    customMetricsType: []
    ## login.autoscaling.behavior Configuring Login deployment scaling behavior policies for the `scaleDown` and `scaleUp` fields
    ## If not set the default values are used:
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#default-behavior
    behavior: {}
  # Login container image parameters
  image:
    # login.image.repository Login container image repository
    repository: onlyoffice/docspace-login
    # login.image.tag Login container image tag
    # If set to, it takes priority over the `images.tag`
    tag: ""
    # login.image.pullPolicy Login container image pull policy
    pullPolicy: IfNotPresent
  # Configure a Security Context for containers in Login Pods
  # If set to, it takes priority over the `containerSecurityContext`
  containerSecurityContext:
    # login.containerSecurityContext.enabled Enable security context for containers in Login pods
    enabled: false
  # login.lifecycleHooks Defines the Login container lifecycle hooks
  # ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
  # It is used to trigger events to run at certain points in a container's lifecycle
  # There are two hooks that are exposed: `PostStart` and `PreStop`
  lifecycleHooks: {}
  # login.containerPorts.login Login container port
  containerPorts:
    login: 5011
  # Probe used for the Login container: startup, readiness and liveness probes
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # The parameters below for startup probes are used only when `login.startupProbe.enabled=true`
  startupProbe:
    # login.startupProbe.enabled Enable startupProbe for Login container
    enabled: true
    httpGet:
      # login.startupProbe.httpGet.path Checking the path for startupProbe
      path: /login/health
      # login.startupProbe.httpGet.port Checking the port for startupProbe
      port: 5011
    # login.startupProbe.failureThreshold Failure threshold for startupProbe
    failureThreshold: 30
    # login.startupProbe.periodSeconds Period seconds for startupProbe
    periodSeconds: 10
  # The parameters below for readiness probes are used only when `login.readinessProbe.enabled=true`
  readinessProbe:
    # login.readinessProbe.enabled Enable readinessProbe for Login container
    enabled: true
    # login.readinessProbe.failureThreshold Failure threshold for readinessProbe
    failureThreshold: 2
    httpGet:
      # login.readinessProbe.httpGet.path Checking the path for readinessProbe
      path: /login/health
      # login.readinessProbe.httpGet.port Checking the port for readinessProbe
      port: 5011
    # login.readinessProbe.periodSeconds Period seconds for readinessProbe
    periodSeconds: 10
    # login.readinessProbe.successThreshold Success threshold for readinessProbe
    successThreshold: 1
    # login.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    timeoutSeconds: 5
  # The parameters below for liveness probes are used only when `login.livenessProbe.enabled=true`
  livenessProbe:
    # login.livenessProbe.enabled Enable livenessProbe for Login container
    enabled: true
    # login.livenessProbe.failureThreshold Failure threshold for livenessProbe
    failureThreshold: 3
    httpGet:
      # login.livenessProbe.httpGet.path Checking the path for livenessProbe
      path: /login/health
      # login.livenessProbe.httpGet.port Checking the port for livenessProbe
      port: 5011
    # login.livenessProbe.periodSeconds Period seconds for livenessProbe
    periodSeconds: 10
    # login.livenessProbe.successThreshold Success threshold for livenessProbe
    successThreshold: 1
    # login.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    timeoutSeconds: 5
  # Login container resource requests and limits
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  # login.resources.requests The requested resources for the Login container
  # login.resources.limits The resources limits for the Login container
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "2Gi"
      cpu: "2000m"
  ## login.extraEnvVars An array with extra env variables for the Login container
  extraEnvVars: []
  ## login.extraVolumes An array with extra volumes for the Login container
  extraVolumes: []
  ## login.extraVolumeMounts An array with extra volume mounts for the Login container
  extraVolumeMounts: []
  # login.mysqlUser Database user who will be used by the Login service to connect to the database
  # If not set, `connections.mysqlUser` will be used
  # If set to, it takes priority over the `connections.mysqlUser`
  mysqlUser: ""

# DocSpace Identity application parameters
# This block defines the parameters common to all the Pods of this application
#
identity:
  # identity.enabled Enables Identity installation
  enabled: false
  # identity.serviceAccount.create Enable Identity ServiceAccount creation
  # The `Role` and the `RoleBinding` required to build a cluster from identity replicas will be applied
  # If disabled, the common `serviceAccount` will be used
  serviceAccount:
    create: true
  env:
    # identity.env.springProfilesActive is the environment variable to override/pick Spring profile. Default is `dev`
    springProfilesActive: "dev"
    # identity.env.multicast.enabled Defines whether multicast discovery will be used
    multicast:
      enabled: "false"
    # identity.env.kubernetes.enabled Defines whether k8s service discovery will be used
    kubernetes:
      enabled: "true"
  # identity.brokerUriExistingSecret The name of the existing secret that contains the connection URI to the broker
  # If set to, it takes priority over those specified in `connections`
  # Must contain the `broker_uri` key
  brokerUriExistingSecret: ""
  # DocSpace Identity Authorization application parameters
  # This block defines the parameters common to all the Pods of this application
  #
  authorization:
    # identity.authorization.kind The controller used for deploy
    # Possible values are `Deployment` (default) or `StatefulSet`
    kind: Deployment
    # identity.authorization.annotations Defines annotations that will be additionally added to Identity deploy
    # If set to, it takes priority over the `commonAnnotations`
    # You can also use `tpl` as the value for the key
    annotations: {}
    # identity.authorization.replicaCount Number of Identity replicas to deploy
    # If the `identity.authorization.autoscaling.enabled` parameter is enabled, it is ignored
    replicaCount: 2
    # Update strategy used to replace old Pods by new ones
    updateStrategy:
      # identity.authorization.updateStrategy.type Identity update strategy type
      # Allowed values: `RollingUpdate` or `OnDelete` if `identity.authorization.kind` is set to `StatefulSet` and `RollingUpdate` or `Recreate` if `identity.authorization.kind` is set to `Deployment`
      type: RollingUpdate
      # identity.authorization.updateStrategy.rollingUpdate Used only when `identity.authorization.updateStrategy.type=RollingUpdate`
      rollingUpdate:
        # identity.authorization.updateStrategy.rollingUpdate.maxUnavailable Maximum number of Identity Pods unavailable during the update process
        maxUnavailable: 25%
        # identity.authorization.updateStrategy.rollingUpdate.maxSurge Maximum number of Identity Pods created over the desired number of Pods
        maxSurge: 25%
    # identity.authorization.podManagementPolicy The Identity Pods scaling operations policy
    # Used if `identity.authorization.kind` is set to `StatefulSet`
    # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
    podManagementPolicy: OrderedReady
    # identity.authorization.podAnnotations Map of annotations to add to the Identity pods
    podAnnotations: {}
    # Configure a Security Context for the Identity Pods
    # If set to, it takes priority over the `podSecurityContext`
    podSecurityContext:
      # identity.authorization.podSecurityContext.enabled Enable security context for the Identity pods
      enabled: false
    # identity.authorization.customPodAntiAffinity Prohibiting the scheduling of Identity Pods relative to other Pods containing the specified labels on the same node
    customPodAntiAffinity: {}
    # Pod affinity rules for Identity Pod scheduling by nodes relative to other Pods
    # Pod affinity allow you to constrain which nodes Identity Pods can be scheduled on based on the labels of Pods already running on that node
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    # Example:
    # podAffinity:
    #   preferredDuringSchedulingIgnoredDuringExecution:
    #   - weight: 100
    #     podAffinityTerm:
    #       labelSelector:
    #         matchExpressions:
    #         - key: app
    #           operator: NotIn
    #           values:
    #           - database
    #       topologyKey: kubernetes.io/hostname
    podAffinity: {}
    # Node affinity rules for Identity Pod scheduling by nodes
    # Node affinity allow you to constrain which nodes Identity Pods can be scheduled on based on node labels
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    # Example:
    # nodeAffinity:
    #   preferredDuringSchedulingIgnoredDuringExecution:
    #   - weight: 100
    #     preference:
    #       matchExpressions:
    #       - key: kubernetes.io/name
    #         operator: In
    #         values:
    #         - name1
    #         - name2
    nodeAffinity: {}
    # identity.authorization.nodeSelector Node labels for Identity pods assignment
    # If set to, it takes priority over the `nodeSelector`
    nodeSelector: {}
    # identity.authorization.tolerations Tolerations for Identity pods assignment
    # If set to, it takes priority over the `tolerations`
    tolerations: []
    ## Horizontal Pod Autoscaling parameters
    ## Horizontal Pod Autoscaling is used for autoscaling of the Identity deployment
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
    ## The parameters below for autoscaling are used only when `identity.authorization.autoscaling.enabled=true`
    autoscaling:
      ## identity.authorization.autoscaling.enabled Enable Identity deployment autoscaling
      enabled: false
      ## identity.authorization.autoscaling.annotations Defines annotations that will be additionally added to Identity deployment HPA
      ## If set to, it takes priority over the `commonAnnotations`
      ## You can also use `tpl` as the value for the key
      annotations: {}
      ## identity.authorization.autoscaling.minReplicas Identity deployment autoscaling minimum number of replicas
      minReplicas: 2
      ## identity.authorization.autoscaling.maxReplicas Identity deployment autoscaling maximum number of replicas
      maxReplicas: 16
      targetCPU:
        ## identity.authorization.autoscaling.targetCPU.enabled Enable autoscaling of Identity deployment by CPU usage percentage
        enabled: true
        ## identity.authorization.autoscaling.targetCPU.utilizationPercentage Identity deployment autoscaling target CPU percentage
        utilizationPercentage: 70
      targetMemory:
        ## identity.authorization.autoscaling.targetMemory.enabled Enable autoscaling of Identity deployment by memory usage percentage
        enabled: false
        ## identity.authorization.autoscaling.targetMemory.utilizationPercentage Identity deployment autoscaling target memory percentage
        utilizationPercentage: 70
      ## identity.authorization.autoscaling.customMetricsType Custom, additional or external autoscaling metrics for the Identity deployment
      ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
      customMetricsType: []
      ## identity.authorization.autoscaling.behavior Configuring Identity deployment scaling behavior policies for the `scaleDown` and `scaleUp` fields
      ## If not set the default values are used:
      ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#default-behavior
      behavior: {}
    # Identity container image parameters
    image:
      # identity.authorization.image.repository Identity container image repository
      repository: onlyoffice/docspace-identity-authorization
      # identity.authorization.image.tag Identity container image tag
      # If set to, it takes priority over the `images.tag`
      tag: ""
      # identity.authorization.image.pullPolicy Identity container image pull policy
      pullPolicy: IfNotPresent
    # Configure a Security Context for containers in Identity Pods
    # If set to, it takes priority over the `containerSecurityContext`
    containerSecurityContext:
      # identity.authorization.containerSecurityContext.enabled Enable security context for containers in Identity pods
      enabled: false
    # identity.authorization.lifecycleHooks Defines the Identity container lifecycle hooks
    # ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
    # It is used to trigger events to run at certain points in a container's lifecycle
    # There are two hooks that are exposed: `PostStart` and `PreStop`
    lifecycleHooks: {}
    # identity.authorization.containerPorts.authorization Identity container port
    containerPorts:
      authorization: 8080
    # Probe used for the Identity container: startup, readiness and liveness probes
    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
    # The parameters below for startup probes are used only when `identity.authorization.startupProbe.enabled=true`
    startupProbe:
      # identity.authorization.startupProbe.enabled Enable startupProbe for Identity container
      enabled: true
      httpGet:
        # identity.authorization.startupProbe.httpGet.path Checking the path for startupProbe
        path: /health
        # identity.authorization.startupProbe.httpGet.port Checking the port for startupProbe
        port: 8080
      # identity.authorization.startupProbe.failureThreshold Failure threshold for startupProbe
      failureThreshold: 30
      # identity.authorization.startupProbe.periodSeconds Period seconds for startupProbe
      periodSeconds: 10
    # The parameters below for readiness probes are used only when `identity.authorization.readinessProbe.enabled=true`
    readinessProbe:
      # identity.authorization.readinessProbe.enabled Enable readinessProbe for Identity container
      enabled: true
      # identity.authorization.readinessProbe.failureThreshold Failure threshold for readinessProbe
      failureThreshold: 2
      httpGet:
        # identity.authorization.readinessProbe.httpGet.path Checking the path for readinessProbe
        path: /health
        # identity.authorization.readinessProbe.httpGet.port Checking the port for readinessProbe
        port: 8080
      # identity.authorization.readinessProbe.periodSeconds Period seconds for readinessProbe
      periodSeconds: 10
      # identity.authorization.readinessProbe.successThreshold Success threshold for readinessProbe
      successThreshold: 1
      # identity.authorization.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
      timeoutSeconds: 5
    # The parameters below for liveness probes are used only when `identity.authorization.livenessProbe.enabled=true`
    livenessProbe:
      # identity.authorization.livenessProbe.enabled Enable livenessProbe for Identity container
      enabled: true
      # identity.authorization.livenessProbe.failureThreshold Failure threshold for livenessProbe
      failureThreshold: 3
      httpGet:
        # identity.authorization.livenessProbe.httpGet.path Checking the path for livenessProbe
        path: /health
        # identity.authorization.livenessProbe.httpGet.port Checking the port for livenessProbe
        port: 8080
      # identity.authorization.livenessProbe.periodSeconds Period seconds for livenessProbe
      periodSeconds: 10
      # identity.authorization.livenessProbe.successThreshold Success threshold for livenessProbe
      successThreshold: 1
      # identity.authorization.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
      timeoutSeconds: 5
    # Identity container resource requests and limits
    # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    # identity.authorization.resources.requests The requested resources for the Identity container
    # identity.authorization.resources.limits The resources limits for the Identity container
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "2Gi"
        cpu: "2000m"
    ## identity.authorization.extraEnvVars An array with extra env variables for the Identity container
    extraEnvVars: []
    ## identity.authorization.extraVolumes An array with extra volumes for the Identity container
    extraVolumes: []
    ## identity.authorization.extraVolumeMounts An array with extra volume mounts for the Identity container
    extraVolumeMounts: []
    # identity.authorization.mysqlUser Database user who will be used by the Identity to connect to the database
    # If not set, `connections.mysqlUser` will be used
    # If set to, it takes priority over the `connections.mysqlUser`
    mysqlUser: ""
  # DocSpace Identity API application parameters
  # This block defines the parameters common to all the Pods of this application
  #
  api:
    # identity.api.kind The controller used for deploy
    # Possible values are `Deployment` (default) or `StatefulSet`
    kind: Deployment
    # identity.api.annotations Defines annotations that will be additionally added to Identity Api deploy
    # If set to, it takes priority over the `commonAnnotations`
    # You can also use `tpl` as the value for the key
    annotations: {}
    # identity.api.replicaCount Number of Identity Api replicas to deploy
    # If the `identity.api.autoscaling.enabled` parameter is enabled, it is ignored
    replicaCount: 2
    # Update strategy used to replace old Pods by new ones
    updateStrategy:
      # identity.api.updateStrategy.type Identity Api update strategy type
      # Allowed values: `RollingUpdate` or `OnDelete` if `identity.api.kind` is set to `StatefulSet` and `RollingUpdate` or `Recreate` if `identity.api.kind` is set to `Deployment`
      type: RollingUpdate
      # identity.api.updateStrategy.rollingUpdate Used only when `identity.api.updateStrategy.type=RollingUpdate`
      rollingUpdate:
        # identity.api.updateStrategy.rollingUpdate.maxUnavailable Maximum number of Identity Api Pods unavailable during the update process
        maxUnavailable: 25%
        # identity.api.updateStrategy.rollingUpdate.maxSurge Maximum number of Identity Api Pods created over the desired number of Pods
        maxSurge: 25%
    # identity.api.podManagementPolicy The Identity Api Pods scaling operations policy
    # Used if `identity.api.kind` is set to `StatefulSet`
    # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
    podManagementPolicy: OrderedReady
    # identity.api.podAnnotations Map of annotations to add to the Identity Api pods
    podAnnotations: {}
    # Configure a Security Context for the Identity Api Pods
    # If set to, it takes priority over the `podSecurityContext`
    podSecurityContext:
      # identity.api.podSecurityContext.enabled Enable security context for the Identity Api pods
      enabled: false
    # identity.api.customPodAntiAffinity Prohibiting the scheduling of Identity Api Pods relative to other Pods containing the specified labels on the same node
    customPodAntiAffinity: {}
    # Pod affinity rules for Identity Api Pod scheduling by nodes relative to other Pods
    # Pod affinity allow you to constrain which nodes Identity Api Pods can be scheduled on based on the labels of Pods already running on that node
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    # Example:
    # podAffinity:
    #   preferredDuringSchedulingIgnoredDuringExecution:
    #   - weight: 100
    #     podAffinityTerm:
    #       labelSelector:
    #         matchExpressions:
    #         - key: app
    #           operator: NotIn
    #           values:
    #           - database
    #       topologyKey: kubernetes.io/hostname
    podAffinity: {}
    # Node affinity rules for Identity Api Pod scheduling by nodes
    # Node affinity allow you to constrain which nodes Identity Api Pods can be scheduled on based on node labels
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    # Example:
    # nodeAffinity:
    #   preferredDuringSchedulingIgnoredDuringExecution:
    #   - weight: 100
    #     preference:
    #       matchExpressions:
    #       - key: kubernetes.io/name
    #         operator: In
    #         values:
    #         - name1
    #         - name2
    nodeAffinity: {}
    # identity.api.nodeSelector Node labels for Identity Api pods assignment
    # If set to, it takes priority over the `nodeSelector`
    nodeSelector: {}
    # identity.api.tolerations Tolerations for Identity Api pods assignment
    # If set to, it takes priority over the `tolerations`
    tolerations: []
    ## Horizontal Pod Autoscaling parameters
    ## Horizontal Pod Autoscaling is used for autoscaling of the Identity Api deployment
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
    ## The parameters below for autoscaling are used only when `identity.api.autoscaling.enabled=true`
    autoscaling:
      ## identity.api.autoscaling.enabled Enable Identity Api deployment autoscaling
      enabled: false
      ## identity.api.autoscaling.annotations Defines annotations that will be additionally added to Identity Api deployment HPA
      ## If set to, it takes priority over the `commonAnnotations`
      ## You can also use `tpl` as the value for the key
      annotations: {}
      ## identity.api.autoscaling.minReplicas Identity Api deployment autoscaling minimum number of replicas
      minReplicas: 2
      ## identity.api.autoscaling.maxReplicas Identity Api deployment autoscaling maximum number of replicas
      maxReplicas: 16
      targetCPU:
        ## identity.api.autoscaling.targetCPU.enabled Enable autoscaling of Identity Api deployment by CPU usage percentage
        enabled: true
        ## identity.api.autoscaling.targetCPU.utilizationPercentage Identity Api deployment autoscaling target CPU percentage
        utilizationPercentage: 70
      targetMemory:
        ## identity.api.autoscaling.targetMemory.enabled Enable autoscaling of Identity Api deployment by memory usage percentage
        enabled: false
        ## identity.api.autoscaling.targetMemory.utilizationPercentage Identity Api deployment autoscaling target memory percentage
        utilizationPercentage: 70
      ## identity.api.autoscaling.customMetricsType Custom, additional or external autoscaling metrics for the Identity Api deployment
      ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
      customMetricsType: []
      ## identity.api.autoscaling.behavior Configuring Identity Api deployment scaling behavior policies for the `scaleDown` and `scaleUp` fields
      ## If not set the default values are used:
      ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#default-behavior
      behavior: {}
    # Identity Api container image parameters
    image:
      # identity.api.image.repository Identity Api container image repository
      repository: onlyoffice/docspace-identity-api
      # identity.api.image.tag Identity Api container image tag
      # If set to, it takes priority over the `images.tag`
      tag: ""
      # identity.api.image.pullPolicy Identity Api container image pull policy
      pullPolicy: IfNotPresent
    # Configure a Security Context for containers in Identity Api Pods
    # If set to, it takes priority over the `containerSecurityContext`
    containerSecurityContext:
      # identity.api.containerSecurityContext.enabled Enable security context for containers in Identity Api pods
      enabled: false
    # identity.api.lifecycleHooks Defines the Identity Api container lifecycle hooks
    # ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
    # It is used to trigger events to run at certain points in a container's lifecycle
    # There are two hooks that are exposed: `PostStart` and `PreStop`
    lifecycleHooks: {}
    # identity.api.containerPorts.api Identity Api container port
    containerPorts:
      api: 9090
    # Probe used for the Identity Api container: startup, readiness and liveness probes
    # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
    # The parameters below for startup probes are used only when `identity.api.startupProbe.enabled=true`
    startupProbe:
      # identity.api.startupProbe.enabled Enable startupProbe for Identity Api container
      enabled: true
      httpGet:
        # identity.api.startupProbe.httpGet.path Checking the path for startupProbe
        path: /health
        # identity.api.startupProbe.httpGet.port Checking the port for startupProbe
        port: 9090
      # identity.api.startupProbe.failureThreshold Failure threshold for startupProbe
      failureThreshold: 30
      # identity.api.startupProbe.periodSeconds Period seconds for startupProbe
      periodSeconds: 10
    # The parameters below for readiness probes are used only when `identity.api.readinessProbe.enabled=true`
    readinessProbe:
      # identity.api.readinessProbe.enabled Enable readinessProbe for Identity Api container
      enabled: true
      # identity.api.readinessProbe.failureThreshold Failure threshold for readinessProbe
      failureThreshold: 2
      httpGet:
        # identity.api.readinessProbe.httpGet.path Checking the path for readinessProbe
        path: /health
        # identity.api.readinessProbe.httpGet.port Checking the port for readinessProbe
        port: 9090
      # identity.api.readinessProbe.periodSeconds Period seconds for readinessProbe
      periodSeconds: 10
      # identity.api.readinessProbe.successThreshold Success threshold for readinessProbe
      successThreshold: 1
      # identity.api.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
      timeoutSeconds: 5
    # The parameters below for liveness probes are used only when `identity.api.livenessProbe.enabled=true`
    livenessProbe:
      # identity.api.livenessProbe.enabled Enable livenessProbe for Identity Api container
      enabled: true
      # identity.api.livenessProbe.failureThreshold Failure threshold for livenessProbe
      failureThreshold: 3
      httpGet:
        # identity.api.livenessProbe.httpGet.path Checking the path for livenessProbe
        path: /health
        # identity.api.livenessProbe.httpGet.port Checking the port for livenessProbe
        port: 9090
      # identity.api.livenessProbe.periodSeconds Period seconds for livenessProbe
      periodSeconds: 10
      # identity.api.livenessProbe.successThreshold Success threshold for livenessProbe
      successThreshold: 1
      # identity.api.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
      timeoutSeconds: 5
    # Identity Api container resource requests and limits
    # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    # identity.api.resources.requests The requested resources for the Identity Api container
    # identity.api.resources.limits The resources limits for the Identity Api container
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "2Gi"
        cpu: "2000m"
    ## identity.api.extraEnvVars An array with extra env variables for the Identity Api container
    extraEnvVars: []
    ## identity.api.extraVolumes An array with extra volumes for the Identity Api container
    extraVolumes: []
    ## identity.api.extraVolumeMounts An array with extra volume mounts for the Identity Api container
    extraVolumeMounts: []
    # identity.api.mysqlUser Database user who will be used by the Identity Api to connect to the database
    # If not set, `connections.mysqlUser` will be used
    # If set to, it takes priority over the `connections.mysqlUser`
    mysqlUser: ""

# DocSpace Notify application parameters
# This block defines the parameters common to all the Pods of this application
#
notify:
  # notify.enabled Enables Notify installation
  enabled: true
  # notify.kind The controller used for deploy
  # Possible values are `Deployment` (default) or `StatefulSet`
  kind: Deployment
  # notify.annotations Defines annotations that will be additionally added to Notify deploy
  # If set to, it takes priority over the `commonAnnotations`
  # You can also use `tpl` as the value for the key
  annotations: {}
  # notify.replicaCount Number of Notify replicas to deploy
  # If the `notify.autoscaling.enabled` parameter is enabled, it is ignored
  replicaCount: 2
  # Update strategy used to replace old Pods by new ones
  updateStrategy:
    # notify.updateStrategy.type Notify update strategy type
    # Allowed values: `RollingUpdate` or `OnDelete` if `notify.kind` is set to `StatefulSet` and `RollingUpdate` or `Recreate` if `notify.kind` is set to `Deployment`
    type: RollingUpdate
    # notify.updateStrategy.rollingUpdate Used only when `notify.updateStrategy.type=RollingUpdate`
    rollingUpdate:
      # notify.updateStrategy.rollingUpdate.maxUnavailable Maximum number of Notify Pods unavailable during the update process
      maxUnavailable: 25%
      # notify.updateStrategy.rollingUpdate.maxSurge Maximum number of Notify Pods created over the desired number of Pods
      maxSurge: 25%
  # notify.podManagementPolicy The Notify Pods scaling operations policy
  # Used if `notify.kind` is set to `StatefulSet`
  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  podManagementPolicy: OrderedReady
  # notify.podAnnotations Map of annotations to add to the Notify pods
  podAnnotations: {}
  # Configure a Security Context for the Notify Pods
  # If set to, it takes priority over the `podSecurityContext`
  podSecurityContext:
    # notify.podSecurityContext.enabled Enable security context for the Notify pods
    enabled: false
  # notify.customPodAntiAffinity Prohibiting the scheduling of Notify Pods relative to other Pods containing the specified labels on the same node
  customPodAntiAffinity: {}
  # Pod affinity rules for Notify Pod scheduling by nodes relative to other Pods
  # Pod affinity allow you to constrain which nodes Notify Pods can be scheduled on based on the labels of Pods already running on that node
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # Example:
  # podAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     podAffinityTerm:
  #       labelSelector:
  #         matchExpressions:
  #         - key: app
  #           operator: NotIn
  #           values:
  #           - database
  #       topologyKey: kubernetes.io/hostname
  podAffinity: {}
  # Node affinity rules for Notify Pod scheduling by nodes
  # Node affinity allow you to constrain which nodes Notify Pods can be scheduled on based on node labels
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  # Example:
  # nodeAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     preference:
  #       matchExpressions:
  #       - key: kubernetes.io/name
  #         operator: In
  #         values:
  #         - name1
  #         - name2
  nodeAffinity: {}
  # notify.nodeSelector Node labels for Notify pods assignment
  # If set to, it takes priority over the `nodeSelector`
  nodeSelector: {}
  # notify.tolerations Tolerations for Notify pods assignment
  # If set to, it takes priority over the `tolerations`
  tolerations: []
  ## Horizontal Pod Autoscaling parameters
  ## Horizontal Pod Autoscaling is used for autoscaling of the Notify deployment
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## The parameters below for autoscaling are used only when `notify.autoscaling.enabled=true`
  autoscaling:
    ## notify.autoscaling.enabled Enable Notify deployment autoscaling
    enabled: false
    ## notify.autoscaling.annotations Defines annotations that will be additionally added to Notify deployment HPA
    ## If set to, it takes priority over the `commonAnnotations`
    ## You can also use `tpl` as the value for the key
    annotations: {}
    ## notify.autoscaling.minReplicas Notify deployment autoscaling minimum number of replicas
    minReplicas: 2
    ## notify.autoscaling.maxReplicas Notify deployment autoscaling maximum number of replicas
    maxReplicas: 16
    targetCPU:
      ## notify.autoscaling.targetCPU.enabled Enable autoscaling of Notify deployment by CPU usage percentage
      enabled: true
      ## notify.autoscaling.targetCPU.utilizationPercentage Notify deployment autoscaling target CPU percentage
      utilizationPercentage: 70
    targetMemory:
      ## notify.autoscaling.targetMemory.enabled Enable autoscaling of Notify deployment by memory usage percentage
      enabled: false
      ## notify.autoscaling.targetMemory.utilizationPercentage Notify deployment autoscaling target memory percentage
      utilizationPercentage: 70
    ## notify.autoscaling.customMetricsType Custom, additional or external autoscaling metrics for the Notify deployment
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
    customMetricsType: []
    ## notify.autoscaling.behavior Configuring Notify deployment scaling behavior policies for the `scaleDown` and `scaleUp` fields
    ## If not set the default values are used:
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#default-behavior
    behavior: {}
  # Notify container image parameters
  image:
    # notify.image.repository Notify container image repository
    repository: onlyoffice/docspace-notify
    # notify.image.tag Notify container image tag
    # If set to, it takes priority over the `images.tag`
    tag: ""
    # notify.image.pullPolicy Notify container image pull policy
    pullPolicy: IfNotPresent
  # Configure a Security Context for containers in Notify Pods
  # If set to, it takes priority over the `containerSecurityContext`
  containerSecurityContext:
    # notify.containerSecurityContext.enabled Enable security context for containers in Notify pods
    enabled: false
  # notify.lifecycleHooks Defines the Notify container lifecycle hooks
  # ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
  # It is used to trigger events to run at certain points in a container's lifecycle
  # There are two hooks that are exposed: `PostStart` and `PreStop`
  lifecycleHooks: {}
  # notify.containerPorts.app Notify container port
  containerPorts:
    app: 5050
  # Probe used for the Notify container: startup, readiness and liveness probes
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # The parameters below for startup probes are used only when `notify.startupProbe.enabled=true`
  startupProbe:
    # notify.startupProbe.enabled Enable startupProbe for Notify container
    enabled: true
    httpGet:
      # notify.startupProbe.httpGet.path Checking the path for startupProbe
      path: /health
      # notify.startupProbe.httpGet.port Checking the port for startupProbe
      port: 5050
    # notify.startupProbe.failureThreshold Failure threshold for startupProbe
    failureThreshold: 30
    # notify.startupProbe.periodSeconds Period seconds for startupProbe
    periodSeconds: 10
  # The parameters below for readiness probes are used only when `notify.readinessProbe.enabled=true`
  readinessProbe:
    # notify.readinessProbe.enabled Enable readinessProbe for Notify container
    enabled: true
    # notify.readinessProbe.failureThreshold Failure threshold for readinessProbe
    failureThreshold: 2
    httpGet:
      # notify.readinessProbe.httpGet.path Checking the path for readinessProbe
      path: /health
      # notify.readinessProbe.httpGet.port Checking the port for readinessProbe
      port: 5050
    # notify.readinessProbe.periodSeconds Period seconds for readinessProbe
    periodSeconds: 10
    # notify.readinessProbe.successThreshold Success threshold for readinessProbe
    successThreshold: 1
    # notify.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    timeoutSeconds: 5
  # The parameters below for liveness probes are used only when `notify.livenessProbe.enabled=true`
  livenessProbe:
    # notify.livenessProbe.enabled Enable livenessProbe for Notify container
    enabled: true
    # notify.livenessProbe.failureThreshold Failure threshold for livenessProbe
    failureThreshold: 3
    httpGet:
      # notify.livenessProbe.httpGet.path Checking the path for livenessProbe
      path: /health
      # notify.livenessProbe.httpGet.port Checking the port for livenessProbe
      port: 5050
    # notify.livenessProbe.periodSeconds Period seconds for livenessProbe
    periodSeconds: 10
    # notify.livenessProbe.successThreshold Success threshold for livenessProbe
    successThreshold: 1
    # notify.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    timeoutSeconds: 5
  # Notify container resource requests and limits
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  # notify.resources.requests The requested resources for the Notify container
  # notify.resources.limits The resources limits for the Notify container
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "2Gi"
      cpu: "2000m"
  ## notify.extraEnvVars An array with extra env variables for the Notify container
  extraEnvVars: []
  ## notify.extraVolumes An array with extra volumes for the Notify container
  extraVolumes: []
  ## notify.extraVolumeMounts An array with extra volume mounts for the Notify container
  extraVolumeMounts: []
  # notify.mysqlUser Database user who will be used by the Notify service to connect to the database
  # If not set, `connections.mysqlUser` will be used
  # If set to, it takes priority over the `connections.mysqlUser`
  mysqlUser: ""

# DocSpace Socket application parameters
# This block defines the parameters common to all the Pods of this application
#
socket:
  # socket.enabled Enables Socket installation
  enabled: true
  # socket.kind The controller used for deploy
  # Possible values are `Deployment` (default) or `StatefulSet`
  kind: Deployment
  # socket.annotations Defines annotations that will be additionally added to Socket deploy
  # If set to, it takes priority over the `commonAnnotations`
  # You can also use `tpl` as the value for the key
  annotations: {}
  # socket.replicaCount Number of Socket replicas to deploy
  # If the `socket.autoscaling.enabled` parameter is enabled, it is ignored
  replicaCount: 2
  # Update strategy used to replace old Pods by new ones
  updateStrategy:
    # socket.updateStrategy.type Socket update strategy type
    # Allowed values: `RollingUpdate` or `OnDelete` if `socket.kind` is set to `StatefulSet` and `RollingUpdate` or `Recreate` if `socket.kind` is set to `Deployment`
    type: RollingUpdate
    # socket.updateStrategy.rollingUpdate Used only when `socket.updateStrategy.type=RollingUpdate`
    rollingUpdate:
      # socket.updateStrategy.rollingUpdate.maxUnavailable Maximum number of Socket Pods unavailable during the update process
      maxUnavailable: 25%
      # socket.updateStrategy.rollingUpdate.maxSurge Maximum number of Socket Pods created over the desired number of Pods
      maxSurge: 25%
  # socket.podManagementPolicy The Socket Pods scaling operations policy
  # Used if `socket.kind` is set to `StatefulSet`
  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  podManagementPolicy: OrderedReady
  # socket.podAnnotations Map of annotations to add to the Socket pods
  podAnnotations: {}
  # Configure a Security Context for the Socket Pods
  # If set to, it takes priority over the `podSecurityContext`
  podSecurityContext:
    # socket.podSecurityContext.enabled Enable security context for the Socket pods
    enabled: false
  # socket.customPodAntiAffinity Prohibiting the scheduling of Socket Pods relative to other Pods containing the specified labels on the same node
  customPodAntiAffinity: {}
  # Pod affinity rules for Socket Pod scheduling by nodes relative to other Pods
  # Pod affinity allow you to constrain which nodes Socket Pods can be scheduled on based on the labels of Pods already running on that node
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # Example:
  # podAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     podAffinityTerm:
  #       labelSelector:
  #         matchExpressions:
  #         - key: app
  #           operator: NotIn
  #           values:
  #           - database
  #       topologyKey: kubernetes.io/hostname
  podAffinity: {}
  # Node affinity rules for Socket Pod scheduling by nodes
  # Node affinity allow you to constrain which nodes Socket Pods can be scheduled on based on node labels
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  # Example:
  # nodeAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     preference:
  #       matchExpressions:
  #       - key: kubernetes.io/name
  #         operator: In
  #         values:
  #         - name1
  #         - name2
  nodeAffinity: {}
  # socket.nodeSelector Node labels for Socket pods assignment
  # If set to, it takes priority over the `nodeSelector`
  nodeSelector: {}
  # socket.tolerations Tolerations for Socket pods assignment
  # If set to, it takes priority over the `tolerations`
  tolerations: []
  ## Horizontal Pod Autoscaling parameters
  ## Horizontal Pod Autoscaling is used for autoscaling of the Socket deployment
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## The parameters below for autoscaling are used only when `socket.autoscaling.enabled=true`
  autoscaling:
    ## socket.autoscaling.enabled Enable Socket deployment autoscaling
    enabled: false
    ## socket.autoscaling.annotations Defines annotations that will be additionally added to Socket deployment HPA
    ## If set to, it takes priority over the `commonAnnotations`
    ## You can also use `tpl` as the value for the key
    annotations: {}
    ## socket.autoscaling.minReplicas Socket deployment autoscaling minimum number of replicas
    minReplicas: 2
    ## socket.autoscaling.maxReplicas Socket deployment autoscaling maximum number of replicas
    maxReplicas: 16
    targetCPU:
      ## socket.autoscaling.targetCPU.enabled Enable autoscaling of Socket deployment by CPU usage percentage
      enabled: true
      ## socket.autoscaling.targetCPU.utilizationPercentage Socket deployment autoscaling target CPU percentage
      utilizationPercentage: 70
    targetMemory:
      ## socket.autoscaling.targetMemory.enabled Enable autoscaling of Socket deployment by memory usage percentage
      enabled: false
      ## socket.autoscaling.targetMemory.utilizationPercentage Socket deployment autoscaling target memory percentage
      utilizationPercentage: 70
    ## socket.autoscaling.customMetricsType Custom, additional or external autoscaling metrics for the Socket deployment
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
    customMetricsType: []
    ## socket.autoscaling.behavior Configuring Socket deployment scaling behavior policies for the `scaleDown` and `scaleUp` fields
    ## If not set the default values are used:
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#default-behavior
    behavior: {}
  # Socket container image parameters
  image:
    # socket.image.repository Socket container image repository
    repository: onlyoffice/docspace-socket
    # socket.image.tag Socket container image tag
    # If set to, it takes priority over the `images.tag`
    tag: ""
    # socket.image.pullPolicy Socket container image pull policy
    pullPolicy: IfNotPresent
  # Configure a Security Context for containers in Socket Pods
  # If set to, it takes priority over the `containerSecurityContext`
  containerSecurityContext:
    # socket.containerSecurityContext.enabled Enable security context for containers in Socket pods
    enabled: false
  # socket.lifecycleHooks Defines the Socket container lifecycle hooks
  # ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
  # It is used to trigger events to run at certain points in a container's lifecycle
  # There are two hooks that are exposed: `PostStart` and `PreStop`
  lifecycleHooks: {}
  containerPorts:
    # socket.containerPorts.app Socket container port
    app: 5050
    # socket.containerPorts.socket Socket additional container port
    socket: 9899
  # Probe used for the Socket container: startup, readiness and liveness probes
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # The parameters below for startup probes are used only when `socket.startupProbe.enabled=true`
  startupProbe:
    # socket.startupProbe.enabled Enable startupProbe for Socket container
    enabled: true
    httpGet:
      # socket.startupProbe.httpGet.path Checking the path for startupProbe
      path: /health
      # socket.startupProbe.httpGet.port Checking the port for startupProbe
      port: 5050
    # socket.startupProbe.failureThreshold Failure threshold for startupProbe
    failureThreshold: 30
    # socket.startupProbe.periodSeconds Period seconds for startupProbe
    periodSeconds: 10
  # The parameters below for readiness probes are used only when `socket.readinessProbe.enabled=true`
  readinessProbe:
    # socket.readinessProbe.enabled Enable readinessProbe for Socket container
    enabled: true
    # socket.readinessProbe.failureThreshold Failure threshold for readinessProbe
    failureThreshold: 2
    httpGet:
      # socket.readinessProbe.httpGet.path Checking the path for readinessProbe
      path: /health
      # socket.readinessProbe.httpGet.port Checking the port for readinessProbe
      port: 5050
    # socket.readinessProbe.periodSeconds Period seconds for readinessProbe
    periodSeconds: 10
    # socket.readinessProbe.successThreshold Success threshold for readinessProbe
    successThreshold: 1
    # socket.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    timeoutSeconds: 5
  # The parameters below for liveness probes are used only when `socket.livenessProbe.enabled=true`
  livenessProbe:
    # socket.livenessProbe.enabled Enable livenessProbe for Socket container
    enabled: true
    # socket.livenessProbe.failureThreshold Failure threshold for livenessProbe
    failureThreshold: 3
    httpGet:
      # socket.livenessProbe.httpGet.path Checking the path for livenessProbe
      path: /health
      # socket.livenessProbe.httpGet.port Checking the port for livenessProbe
      port: 5050
    # socket.livenessProbe.periodSeconds Period seconds for livenessProbe
    periodSeconds: 10
    # socket.livenessProbe.successThreshold Success threshold for livenessProbe
    successThreshold: 1
    # socket.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    timeoutSeconds: 5
  # Socket container resource requests and limits
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  # socket.resources.requests The requested resources for the Socket container
  # socket.resources.limits The resources limits for the Socket container
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "2Gi"
      cpu: "2000m"
  ## socket.extraEnvVars An array with extra env variables for the Socket container
  extraEnvVars: []
  ## socket.extraVolumes An array with extra volumes for the Socket container
  extraVolumes: []
  ## socket.extraVolumeMounts An array with extra volume mounts for the Socket container
  extraVolumeMounts: []
  # socket.mysqlUser Database user who will be used by the Socket service to connect to the database
  # If not set, `connections.mysqlUser` will be used
  # If set to, it takes priority over the `connections.mysqlUser`
  mysqlUser: ""

# DocSpace ssoauth application parameters
# This block defines the parameters common to all the Pods of this application
#
ssoauth:
  # ssoauth.enabled Enables ssoauth installation
  enabled: true
  # ssoauth.kind The controller used for deploy
  # Possible values are `Deployment` (default) or `StatefulSet`
  kind: Deployment
  # ssoauth.annotations Defines annotations that will be additionally added to ssoauth deploy
  # If set to, it takes priority over the `commonAnnotations`
  # You can also use `tpl` as the value for the key
  annotations: {}
  # ssoauth.replicaCount Number of ssoauth replicas to deploy
  # If the `ssoauth.autoscaling.enabled` parameter is enabled, it is ignored
  replicaCount: 2
  # Update strategy used to replace old Pods by new ones
  updateStrategy:
    # ssoauth.updateStrategy.type ssoauth update strategy type
    # Allowed values: `RollingUpdate` or `OnDelete` if `ssoauth.kind` is set to `StatefulSet` and `RollingUpdate` or `Recreate` if `ssoauth.kind` is set to `Deployment`
    type: RollingUpdate
    # ssoauth.updateStrategy.rollingUpdate Used only when `ssoauth.updateStrategy.type=RollingUpdate`
    rollingUpdate:
      # ssoauth.updateStrategy.rollingUpdate.maxUnavailable Maximum number of ssoauth Pods unavailable during the update process
      maxUnavailable: 25%
      # ssoauth.updateStrategy.rollingUpdate.maxSurge Maximum number of ssoauth Pods created over the desired number of Pods
      maxSurge: 25%
  # ssoauth.podManagementPolicy The ssoauth Pods scaling operations policy
  # Used if `ssoauth.kind` is set to `StatefulSet`
  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  podManagementPolicy: OrderedReady
  # ssoauth.podAnnotations Map of annotations to add to the ssoauth pods
  podAnnotations: {}
  # Configure a Security Context for the ssoauth Pods
  # If set to, it takes priority over the `podSecurityContext`
  podSecurityContext:
    # ssoauth.podSecurityContext.enabled Enable security context for the ssoauth pods
    enabled: false
  # ssoauth.customPodAntiAffinity Prohibiting the scheduling of ssoauth Pods relative to other Pods containing the specified labels on the same node
  customPodAntiAffinity: {}
  # Pod affinity rules for ssoauth Pod scheduling by nodes relative to other Pods
  # Pod affinity allow you to constrain which nodes ssoauth Pods can be scheduled on based on the labels of Pods already running on that node
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # Example:
  # podAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     podAffinityTerm:
  #       labelSelector:
  #         matchExpressions:
  #         - key: app
  #           operator: NotIn
  #           values:
  #           - database
  #       topologyKey: kubernetes.io/hostname
  podAffinity: {}
  # Node affinity rules for ssoauth Pod scheduling by nodes
  # Node affinity allow you to constrain which nodes ssoauth Pods can be scheduled on based on node labels
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  # Example:
  # nodeAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     preference:
  #       matchExpressions:
  #       - key: kubernetes.io/name
  #         operator: In
  #         values:
  #         - name1
  #         - name2
  nodeAffinity: {}
  # ssoauth.nodeSelector Node labels for ssoauth pods assignment
  # If set to, it takes priority over the `nodeSelector`
  nodeSelector: {}
  # ssoauth.tolerations Tolerations for ssoauth pods assignment
  # If set to, it takes priority over the `tolerations`
  tolerations: []
  ## Horizontal Pod Autoscaling parameters
  ## Horizontal Pod Autoscaling is used for autoscaling of the ssoauth deployment
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## The parameters below for autoscaling are used only when `ssoauth.autoscaling.enabled=true`
  autoscaling:
    ## ssoauth.autoscaling.enabled Enable ssoauth deployment autoscaling
    enabled: false
    ## ssoauth.autoscaling.annotations Defines annotations that will be additionally added to ssoauth deployment HPA
    ## If set to, it takes priority over the `commonAnnotations`
    ## You can also use `tpl` as the value for the key
    annotations: {}
    ## ssoauth.autoscaling.minReplicas ssoauth deployment autoscaling minimum number of replicas
    minReplicas: 2
    ## ssoauth.autoscaling.maxReplicas ssoauth deployment autoscaling maximum number of replicas
    maxReplicas: 16
    targetCPU:
      ## ssoauth.autoscaling.targetCPU.enabled Enable autoscaling of ssoauth deployment by CPU usage percentage
      enabled: true
      ## ssoauth.autoscaling.targetCPU.utilizationPercentage ssoauth deployment autoscaling target CPU percentage
      utilizationPercentage: 70
    targetMemory:
      ## ssoauth.autoscaling.targetMemory.enabled Enable autoscaling of ssoauth deployment by memory usage percentage
      enabled: false
      ## ssoauth.autoscaling.targetMemory.utilizationPercentage ssoauth deployment autoscaling target memory percentage
      utilizationPercentage: 70
    ## ssoauth.autoscaling.customMetricsType Custom, additional or external autoscaling metrics for the ssoauth deployment
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
    customMetricsType: []
    ## ssoauth.autoscaling.behavior Configuring ssoauth deployment scaling behavior policies for the `scaleDown` and `scaleUp` fields
    ## If not set the default values are used:
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#default-behavior
    behavior: {}
  # ssoauth container image parameters
  image:
    # ssoauth.image.repository ssoauth container image repository
    repository: onlyoffice/docspace-ssoauth
    # ssoauth.image.tag ssoauth container image tag
    # If set to, it takes priority over the `images.tag`
    tag: ""
    # ssoauth.image.pullPolicy ssoauth container image pull policy
    pullPolicy: IfNotPresent
  # Configure a Security Context for containers in ssoauth Pods
  # If set to, it takes priority over the `containerSecurityContext`
  containerSecurityContext:
    # ssoauth.containerSecurityContext.enabled Enable security context for containers in ssoauth pods
    enabled: false
  # ssoauth.lifecycleHooks Defines the ssoauth container lifecycle hooks
  # ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
  # It is used to trigger events to run at certain points in a container's lifecycle
  # There are two hooks that are exposed: `PostStart` and `PreStop`
  lifecycleHooks: {}
  containerPorts:
    # ssoauth.containerPorts.app ssoauth container port
    app: 5050
    # ssoauth.containerPorts.sso ssoauth additional container port
    sso: 9834
  # Probe used for the ssoauth container: startup, readiness and liveness probes
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # The parameters below for startup probes are used only when `ssoauth.startupProbe.enabled=true`
  startupProbe:
    # ssoauth.startupProbe.enabled Enable startupProbe for ssoauth container
    enabled: true
    httpGet:
      # ssoauth.startupProbe.httpGet.path Checking the path for startupProbe
      path: /health
      # ssoauth.startupProbe.httpGet.port Checking the port for startupProbe
      port: 5050
    # ssoauth.startupProbe.failureThreshold Failure threshold for startupProbe
    failureThreshold: 30
    # ssoauth.startupProbe.periodSeconds Period seconds for startupProbe
    periodSeconds: 10
  # The parameters below for readiness probes are used only when `ssoauth.readinessProbe.enabled=true`
  readinessProbe:
    # ssoauth.readinessProbe.enabled Enable readinessProbe for ssoauth container
    enabled: true
    # ssoauth.readinessProbe.failureThreshold Failure threshold for readinessProbe
    failureThreshold: 2
    httpGet:
      # ssoauth.readinessProbe.httpGet.path Checking the path for readinessProbe
      path: /health
      # ssoauth.readinessProbe.httpGet.port Checking the port for readinessProbe
      port: 5050
    # ssoauth.readinessProbe.periodSeconds Period seconds for readinessProbe
    periodSeconds: 10
    # ssoauth.readinessProbe.successThreshold Success threshold for readinessProbe
    successThreshold: 1
    # ssoauth.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    timeoutSeconds: 5
  # The parameters below for liveness probes are used only when `ssoauth.livenessProbe.enabled=true`
  livenessProbe:
    # ssoauth.livenessProbe.enabled Enable livenessProbe for ssoauth container
    enabled: true
    # ssoauth.livenessProbe.failureThreshold Failure threshold for livenessProbe
    failureThreshold: 3
    httpGet:
      # ssoauth.livenessProbe.httpGet.path Checking the path for livenessProbe
      path: /health
      # ssoauth.livenessProbe.httpGet.port Checking the port for livenessProbe
      port: 5050
    # ssoauth.livenessProbe.periodSeconds Period seconds for livenessProbe
    periodSeconds: 10
    # ssoauth.livenessProbe.successThreshold Success threshold for livenessProbe
    successThreshold: 1
    # ssoauth.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    timeoutSeconds: 5
  # ssoauth container resource requests and limits
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  # ssoauth.resources.requests The requested resources for the ssoauth container
  # ssoauth.resources.limits The resources limits for the ssoauth container
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "2Gi"
      cpu: "2000m"
  ## ssoauth.extraEnvVars An array with extra env variables for the ssoauth container
  extraEnvVars: []
  ## ssoauth.extraVolumes An array with extra volumes for the ssoauth container
  extraVolumes: []
  ## ssoauth.extraVolumeMounts An array with extra volume mounts for the ssoauth container
  extraVolumeMounts: []
  # ssoauth.mysqlUser Database user who will be used by the ssoauth service to connect to the database
  # If not set, `connections.mysqlUser` will be used
  # If set to, it takes priority over the `connections.mysqlUser`
  mysqlUser: ""

# DocSpace Studio application parameters
# This block defines the parameters common to all the Pods of this application
#
studio:
  # studio.enabled Enables Studio installation
  enabled: true
  # studio.kind The controller used for deploy
  # Possible values are `Deployment` (default) or `StatefulSet`
  kind: Deployment
  # studio.annotations Defines annotations that will be additionally added to Studio deploy
  # If set to, it takes priority over the `commonAnnotations`
  # You can also use `tpl` as the value for the key
  annotations: {}
  # studio.replicaCount Number of Studio replicas to deploy
  # If the `studio.autoscaling.enabled` parameter is enabled, it is ignored
  replicaCount: 2
  # Update strategy used to replace old Pods by new ones
  updateStrategy:
    # studio.updateStrategy.type Studio update strategy type
    # Allowed values: `RollingUpdate` or `OnDelete` if `studio.kind` is set to `StatefulSet` and `RollingUpdate` or `Recreate` if `studio.kind` is set to `Deployment`
    type: RollingUpdate
    # studio.updateStrategy.rollingUpdate Used only when `studio.updateStrategy.type=RollingUpdate`
    rollingUpdate:
      # studio.updateStrategy.rollingUpdate.maxUnavailable Maximum number of Studio Pods unavailable during the update process
      maxUnavailable: 25%
      # studio.updateStrategy.rollingUpdate.maxSurge Maximum number of Studio Pods created over the desired number of Pods
      maxSurge: 25%
  # studio.podManagementPolicy The Studio Pods scaling operations policy
  # Used if `studio.kind` is set to `StatefulSet`
  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  podManagementPolicy: OrderedReady
  # studio.podAnnotations Map of annotations to add to the Studio pods
  podAnnotations: {}
  # Configure a Security Context for the Studio Pods
  # If set to, it takes priority over the `podSecurityContext`
  podSecurityContext:
    # studio.podSecurityContext.enabled Enable security context for the Studio pods
    enabled: false
  # studio.customPodAntiAffinity Prohibiting the scheduling of Studio Pods relative to other Pods containing the specified labels on the same node
  customPodAntiAffinity: {}
  # Pod affinity rules for Studio Pod scheduling by nodes relative to other Pods
  # Pod affinity allow you to constrain which nodes Studio Pods can be scheduled on based on the labels of Pods already running on that node
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # Example:
  # podAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     podAffinityTerm:
  #       labelSelector:
  #         matchExpressions:
  #         - key: app
  #           operator: NotIn
  #           values:
  #           - database
  #       topologyKey: kubernetes.io/hostname
  podAffinity: {}
  # Node affinity rules for Studio Pod scheduling by nodes
  # Node affinity allow you to constrain which nodes Studio Pods can be scheduled on based on node labels
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  # Example:
  # nodeAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     preference:
  #       matchExpressions:
  #       - key: kubernetes.io/name
  #         operator: In
  #         values:
  #         - name1
  #         - name2
  nodeAffinity: {}
  # studio.nodeSelector Node labels for Studio pods assignment
  # If set to, it takes priority over the `nodeSelector`
  nodeSelector: {}
  # studio.tolerations Tolerations for Studio pods assignment
  # If set to, it takes priority over the `tolerations`
  tolerations: []
  ## Horizontal Pod Autoscaling parameters
  ## Horizontal Pod Autoscaling is used for autoscaling of the Studio deployment
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## The parameters below for autoscaling are used only when `studio.autoscaling.enabled=true`
  autoscaling:
    ## studio.autoscaling.enabled Enable Studio deployment autoscaling
    enabled: false
    ## studio.autoscaling.annotations Defines annotations that will be additionally added to Studio deployment HPA
    ## If set to, it takes priority over the `commonAnnotations`
    ## You can also use `tpl` as the value for the key
    annotations: {}
    ## studio.autoscaling.minReplicas Studio deployment autoscaling minimum number of replicas
    minReplicas: 2
    ## studio.autoscaling.maxReplicas Studio deployment autoscaling maximum number of replicas
    maxReplicas: 16
    targetCPU:
      ## studio.autoscaling.targetCPU.enabled Enable autoscaling of Studio deployment by CPU usage percentage
      enabled: true
      ## studio.autoscaling.targetCPU.utilizationPercentage Studio deployment autoscaling target CPU percentage
      utilizationPercentage: 70
    targetMemory:
      ## studio.autoscaling.targetMemory.enabled Enable autoscaling of Studio deployment by memory usage percentage
      enabled: false
      ## studio.autoscaling.targetMemory.utilizationPercentage Studio deployment autoscaling target memory percentage
      utilizationPercentage: 70
    ## studio.autoscaling.customMetricsType Custom, additional or external autoscaling metrics for the Studio deployment
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
    customMetricsType: []
    ## studio.autoscaling.behavior Configuring Studio deployment scaling behavior policies for the `scaleDown` and `scaleUp` fields
    ## If not set the default values are used:
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#default-behavior
    behavior: {}
  # Studio container image parameters
  image:
    # studio.image.repository Studio container image repository
    repository: onlyoffice/docspace-studio
    # studio.image.tag Studio container image tag
    # If set to, it takes priority over the `images.tag`
    tag: ""
    # studio.image.pullPolicy Studio container image pull policy
    pullPolicy: IfNotPresent
  # Configure a Security Context for containers in Studio Pods
  # If set to, it takes priority over the `containerSecurityContext`
  containerSecurityContext:
    # studio.containerSecurityContext.enabled Enable security context for containers in Studio pods
    enabled: false
  # studio.lifecycleHooks Defines the Studio container lifecycle hooks
  # ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
  # It is used to trigger events to run at certain points in a container's lifecycle
  # There are two hooks that are exposed: `PostStart` and `PreStop`
  lifecycleHooks: {}
  # studio.containerPorts.app Studio container port
  containerPorts:
    app: 5050
  # Probe used for the Studio container: startup, readiness and liveness probes
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # The parameters below for startup probes are used only when `studio.startupProbe.enabled=true`
  startupProbe:
    # studio.startupProbe.enabled Enable startupProbe for Studio container
    enabled: true
    httpGet:
      # studio.startupProbe.httpGet.path Checking the path for startupProbe
      path: /health
      # studio.startupProbe.httpGet.port Checking the port for startupProbe
      port: 5050
    # studio.startupProbe.failureThreshold Failure threshold for startupProbe
    failureThreshold: 30
    # studio.startupProbe.periodSeconds Period seconds for startupProbe
    periodSeconds: 10
  # The parameters below for readiness probes are used only when `studio.readinessProbe.enabled=true`
  readinessProbe:
    # studio.readinessProbe.enabled Enable readinessProbe for Studio container
    enabled: true
    # studio.readinessProbe.failureThreshold Failure threshold for readinessProbe
    failureThreshold: 2
    httpGet:
      # studio.readinessProbe.httpGet.path Checking the path for readinessProbe
      path: /health
      # studio.readinessProbe.httpGet.port Checking the port for readinessProbe
      port: 5050
    # studio.readinessProbe.periodSeconds Period seconds for readinessProbe
    periodSeconds: 10
    # studio.readinessProbe.successThreshold Success threshold for readinessProbe
    successThreshold: 1
    # studio.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    timeoutSeconds: 5
  # The parameters below for liveness probes are used only when `studio.livenessProbe.enabled=true`
  livenessProbe:
    # studio.livenessProbe.enabled Enable livenessProbe for Studio container
    enabled: true
    # studio.livenessProbe.failureThreshold Failure threshold for livenessProbe
    failureThreshold: 3
    httpGet:
      # studio.livenessProbe.httpGet.path Checking the path for livenessProbe
      path: /health
      # studio.livenessProbe.httpGet.port Checking the port for livenessProbe
      port: 5050
    # studio.livenessProbe.periodSeconds Period seconds for livenessProbe
    periodSeconds: 10
    # studio.livenessProbe.successThreshold Success threshold for livenessProbe
    successThreshold: 1
    # studio.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    timeoutSeconds: 5
  # Studio container resource requests and limits
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  # studio.resources.requests The requested resources for the Studio container
  # studio.resources.limits The resources limits for the Studio container
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "2Gi"
      cpu: "2000m"
  ## studio.extraEnvVars An array with extra env variables for the Studio container
  extraEnvVars: []
  ## studio.extraVolumes An array with extra volumes for the Studio container
  extraVolumes: []
  ## studio.extraVolumeMounts An array with extra volume mounts for the Studio container
  extraVolumeMounts: []
  # studio.mysqlUser Database user who will be used by the Studio service to connect to the database
  # If not set, `connections.mysqlUser` will be used
  # If set to, it takes priority over the `connections.mysqlUser`
  mysqlUser: ""

# DocSpace Studio Notify application parameters
# This block defines the parameters common to all the Pods of this application
#
studioNotify:
  # studioNotify.enabled Enables Studio Notify installation
  enabled: true
  # studioNotify.kind The controller used for deploy
  # Possible values are `Deployment` (default) or `StatefulSet`
  kind: Deployment
  # studioNotify.annotations Defines annotations that will be additionally added to Studio Notify deploy
  # If set to, it takes priority over the `commonAnnotations`
  # You can also use `tpl` as the value for the key
  annotations: {}
  # studioNotify.replicaCount Number of Studio Notify replicas to deploy
  # If the `studioNotify.autoscaling.enabled` parameter is enabled, it is ignored
  replicaCount: 2
  # Update strategy used to replace old Pods by new ones
  updateStrategy:
    # studioNotify.updateStrategy.type Studio Notify update strategy type
    # Allowed values: `RollingUpdate` or `OnDelete` if `studioNotify.kind` is set to `StatefulSet` and `RollingUpdate` or `Recreate` if `studioNotify.kind` is set to `Deployment`
    type: RollingUpdate
    # studioNotify.updateStrategy.rollingUpdate Used only when `studioNotify.updateStrategy.type=RollingUpdate`
    rollingUpdate:
      # studioNotify.updateStrategy.rollingUpdate.maxUnavailable Maximum number of Studio Notify Pods unavailable during the update process
      maxUnavailable: 25%
      # studioNotify.updateStrategy.rollingUpdate.maxSurge Maximum number of Studio Notify Pods created over the desired number of Pods
      maxSurge: 25%
  # studioNotify.podManagementPolicy The Studio Notify Pods scaling operations policy
  # Used if `studioNotify.kind` is set to `StatefulSet`
  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  podManagementPolicy: OrderedReady
  # studioNotify.podAnnotations Map of annotations to add to the Studio Notify pods
  podAnnotations: {}
  # Configure a Security Context for the Studio Notify Pods
  # If set to, it takes priority over the `podSecurityContext`
  podSecurityContext:
    # studioNotify.podSecurityContext.enabled Enable security context for the Studio Notify pods
    enabled: false
  # studioNotify.customPodAntiAffinity Prohibiting the scheduling of Studio Notify Pods relative to other Pods containing the specified labels on the same node
  customPodAntiAffinity: {}
  # Pod affinity rules for Studio Notify Pod scheduling by nodes relative to other Pods
  # Pod affinity allow you to constrain which nodes Studio Notify Pods can be scheduled on based on the labels of Pods already running on that node
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # Example:
  # podAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     podAffinityTerm:
  #       labelSelector:
  #         matchExpressions:
  #         - key: app
  #           operator: NotIn
  #           values:
  #           - database
  #       topologyKey: kubernetes.io/hostname
  podAffinity: {}
  # Node affinity rules for Studio Notify Pod scheduling by nodes
  # Node affinity allow you to constrain which nodes Studio Notify Pods can be scheduled on based on node labels
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  # Example:
  # nodeAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     preference:
  #       matchExpressions:
  #       - key: kubernetes.io/name
  #         operator: In
  #         values:
  #         - name1
  #         - name2
  nodeAffinity: {}
  # studioNotify.nodeSelector Node labels for Studio Notify pods assignment
  # If set to, it takes priority over the `nodeSelector`
  nodeSelector: {}
  # studioNotify.tolerations Tolerations for Studio Notify pods assignment
  # If set to, it takes priority over the `tolerations`
  tolerations: []
  ## Horizontal Pod Autoscaling parameters
  ## Horizontal Pod Autoscaling is used for autoscaling of the Studio Notify deployment
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## The parameters below for autoscaling are used only when `studioNotify.autoscaling.enabled=true`
  autoscaling:
    ## studioNotify.autoscaling.enabled Enable Studio Notify deployment autoscaling
    enabled: false
    ## studioNotify.autoscaling.annotations Defines annotations that will be additionally added to Studio Notify deployment HPA
    ## If set to, it takes priority over the `commonAnnotations`
    ## You can also use `tpl` as the value for the key
    annotations: {}
    ## studioNotify.autoscaling.minReplicas Studio Notify deployment autoscaling minimum number of replicas
    minReplicas: 2
    ## studioNotify.autoscaling.maxReplicas Studio Notify deployment autoscaling maximum number of replicas
    maxReplicas: 16
    targetCPU:
      ## studioNotify.autoscaling.targetCPU.enabled Enable autoscaling of Studio Notify deployment by CPU usage percentage
      enabled: true
      ## studioNotify.autoscaling.targetCPU.utilizationPercentage Studio Notify deployment autoscaling target CPU percentage
      utilizationPercentage: 70
    targetMemory:
      ## studioNotify.autoscaling.targetMemory.enabled Enable autoscaling of Studio Notify deployment by memory usage percentage
      enabled: false
      ## studioNotify.autoscaling.targetMemory.utilizationPercentage Studio Notify deployment autoscaling target memory percentage
      utilizationPercentage: 70
    ## studioNotify.autoscaling.customMetricsType Custom, additional or external autoscaling metrics for the Studio Notify deployment
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
    customMetricsType: []
    ## studioNotify.autoscaling.behavior Configuring Studio Notify deployment scaling behavior policies for the `scaleDown` and `scaleUp` fields
    ## If not set the default values are used:
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#default-behavior
    behavior: {}
  # Studio Notify container image parameters
  image:
    # studioNotify.image.repository Studio Notify container image repository
    repository: onlyoffice/docspace-studio-notify
    # studioNotify.image.tag Studio Notify container image tag
    # If set to, it takes priority over the `images.tag`
    tag: ""
    # studioNotify.image.pullPolicy Studio Notify container image pull policy
    pullPolicy: IfNotPresent
  # Configure a Security Context for containers in Studio Notify Pods
  # If set to, it takes priority over the `containerSecurityContext`
  containerSecurityContext:
    # studioNotify.containerSecurityContext.enabled Enable security context for containers in Studio Notify pods
    enabled: false
  # studioNotify.lifecycleHooks Defines the Studio Notify container lifecycle hooks
  # ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
  # It is used to trigger events to run at certain points in a container's lifecycle
  # There are two hooks that are exposed: `PostStart` and `PreStop`
  lifecycleHooks: {}
  # studioNotify.containerPorts.app Studio Notify container port
  containerPorts:
    app: 5050
  # Probe used for the Studio Notify container: startup, readiness and liveness probes
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # The parameters below for startup probes are used only when `studioNotify.startupProbe.enabled=true`
  startupProbe:
    # studioNotify.startupProbe.enabled Enable Startup Probe for studioNotify container
    enabled: true
    httpGet:
      # studioNotify.startupProbe.httpGet.path Checking the path for startupProbe
      path: /health
      # studioNotify.startupProbe.httpGet.port Checking the port for startupProbe
      port: 5050
    # studioNotify.startupProbe.failureThreshold Failure threshold for startupProbe
    failureThreshold: 30
    # studioNotify.startupProbe.periodSeconds Period seconds for startupProbe
    periodSeconds: 10
  # The parameters below for readiness probes are used only when `studioNotify.readinessProbe.enabled=true`
  readinessProbe:
    # studioNotify.readinessProbe.enabled Enable readinessProbe for Studio Notify container
    enabled: true
    # studioNotify.readinessProbe.failureThreshold Failure threshold for readinessProbe
    failureThreshold: 2
    httpGet:
      # studioNotify.readinessProbe.httpGet.path Checking the path for readinessProbe
      path: /health
      # studioNotify.readinessProbe.httpGet.port Checking the port for readinessProbe
      port: 5050
    # studioNotify.readinessProbe.periodSeconds Period seconds for readinessProbe
    periodSeconds: 10
    # studioNotify.readinessProbe.successThreshold Success threshold for readinessProbe
    successThreshold: 1
    # studioNotify.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    timeoutSeconds: 5
  # The parameters below for liveness probes are used only when `studioNotify.livenessProbe.enabled=true`
  livenessProbe:
    # studioNotify.livenessProbe.enabled Enable livenessProbe for Studio Notify container
    enabled: true
    # studioNotify.livenessProbe.failureThreshold Failure threshold for livenessProbe
    failureThreshold: 3
    httpGet:
      # studioNotify.livenessProbe.httpGet.path Checking the path for livenessProbe
      path: /health
      # studioNotify.livenessProbe.httpGet.port Checking the port for livenessProbe
      port: 5050
    # studioNotify.livenessProbe.periodSeconds Period seconds for livenessProbe
    periodSeconds: 10
    # studioNotify.livenessProbe.successThreshold Success threshold for livenessProbe
    successThreshold: 1
    # studioNotify.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    timeoutSeconds: 5
  # Studio Notify container resource requests and limits
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  # studioNotify.resources.requests The requested resources for the Studio Notify container
  # studioNotify.resources.limits The resources limits for the Studio Notify container
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "2Gi"
      cpu: "2000m"
  ## studioNotify.extraEnvVars An array with extra env variables for the Studio Notify container
  extraEnvVars: []
  ## studioNotify.extraVolumes An array with extra volumes for the Studio Notify container
  extraVolumes: []
  ## studioNotify.extraVolumeMounts An array with extra volume mounts for the Studio Notify container
  extraVolumeMounts: []
  # studioNotify.mysqlUser Database user who will be used by the Studio Notify service to connect to the database
  # If not set, `connections.mysqlUser` will be used
  # If set to, it takes priority over the `connections.mysqlUser`
  mysqlUser: ""

# DocSpace Proxy Frontend application parameters
# This block defines the parameters common to all the Pods of this application
#
proxyFrontend:
  # proxyFrontend.enabled Enables Proxy Frontend installation
  enabled: false
  # proxyFrontend.kind The controller used for deploy
  # Possible values are `Deployment` (default) or `StatefulSet`
  kind: Deployment
  # proxyFrontend.annotations Defines annotations that will be additionally added to Proxy Frontend deploy
  # If set to, it takes priority over the `commonAnnotations`
  # You can also use `tpl` as the value for the key
  annotations: {}
  # proxyFrontend.replicaCount Number of Proxy Frontend replicas to deploy
  # If the `proxyFrontend.autoscaling.enabled` parameter is enabled, it is ignored
  replicaCount: 2
  # Update strategy used to replace old Pods by new ones
  updateStrategy:
    # proxyFrontend.updateStrategy.type Proxy Frontend update strategy type
    # Allowed values: `RollingUpdate` or `OnDelete` if `proxyFrontend.kind` is set to `StatefulSet` and `RollingUpdate` or `Recreate` if `proxyFrontend.kind` is set to `Deployment`
    type: RollingUpdate
    # proxyFrontend.updateStrategy.rollingUpdate Used only when `proxyFrontend.updateStrategy.type=RollingUpdate`
    rollingUpdate:
      # proxyFrontend.updateStrategy.rollingUpdate.maxUnavailable Maximum number of Proxy Frontend Pods unavailable during the update process
      maxUnavailable: 25%
      # proxyFrontend.updateStrategy.rollingUpdate.maxSurge Maximum number of Proxy Frontend Pods created over the desired number of Pods
      maxSurge: 25%
  # proxyFrontend.podManagementPolicy The Proxy Frontend Pods scaling operations policy
  # Used if `proxyFrontend.kind` is set to `StatefulSet`
  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies
  podManagementPolicy: OrderedReady
  # proxyFrontend.podAnnotations Map of annotations to add to the Proxy Frontend pods
  podAnnotations: {}
  # Configure a Security Context for the Proxy Frontend Pods
  # If set to, it takes priority over the `podSecurityContext`
  podSecurityContext:
    # proxyFrontend.podSecurityContext.enabled Enable security context for the Proxy Frontend pods
    enabled: false
  # proxyFrontend.customPodAntiAffinity Prohibiting the scheduling of Proxy Frontend Pods relative to other Pods containing the specified labels on the same node
  customPodAntiAffinity: {}
  # Pod affinity rules for Proxy Frontend Pod scheduling by nodes relative to other Pods
  # Pod affinity allow you to constrain which nodes Proxy Frontend Pods can be scheduled on based on the labels of Pods already running on that node
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # Example:
  # podAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     podAffinityTerm:
  #       labelSelector:
  #         matchExpressions:
  #         - key: app
  #           operator: NotIn
  #           values:
  #           - database
  #       topologyKey: kubernetes.io/hostname
  podAffinity: {}
  # Node affinity rules for Proxy Frontend Pod scheduling by nodes
  # Node affinity allow you to constrain which nodes Proxy Frontend Pods can be scheduled on based on node labels
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  # Example:
  # nodeAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     preference:
  #       matchExpressions:
  #       - key: kubernetes.io/name
  #         operator: In
  #         values:
  #         - name1
  #         - name2
  nodeAffinity: {}
  # proxyFrontend.nodeSelector Node labels for Proxy Frontend pods assignment
  # If set to, it takes priority over the `nodeSelector`
  nodeSelector: {}
  # proxyFrontend.tolerations Tolerations for Proxy Frontend pods assignment
  # If set to, it takes priority over the `tolerations`
  tolerations: []
  ## Horizontal Pod Autoscaling parameters
  ## Horizontal Pod Autoscaling is used for autoscaling of the Proxy Frontend deployment
  ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
  ## The parameters below for autoscaling are used only when `proxyFrontend.autoscaling.enabled=true`
  autoscaling:
    ## proxyFrontend.autoscaling.enabled Enable Proxy Frontend deployment autoscaling
    enabled: false
    ## proxyFrontend.autoscaling.annotations Defines annotations that will be additionally added to Proxy Frontend deployment HPA
    ## If set to, it takes priority over the `commonAnnotations`
    ## You can also use `tpl` as the value for the key
    annotations: {}
    ## proxyFrontend.autoscaling.minReplicas Proxy Frontend deployment autoscaling minimum number of replicas
    minReplicas: 2
    ## proxyFrontend.autoscaling.maxReplicas Proxy Frontend deployment autoscaling maximum number of replicas
    maxReplicas: 16
    targetCPU:
      ## proxyFrontend.autoscaling.targetCPU.enabled Enable autoscaling of Proxy Frontend deployment by CPU usage percentage
      enabled: true
      ## proxyFrontend.autoscaling.targetCPU.utilizationPercentage Proxy Frontend deployment autoscaling target CPU percentage
      utilizationPercentage: 70
    targetMemory:
      ## proxyFrontend.autoscaling.targetMemory.enabled Enable autoscaling of Proxy Frontend deployment by memory usage percentage
      enabled: false
      ## proxyFrontend.autoscaling.targetMemory.utilizationPercentage Proxy Frontend deployment autoscaling target memory percentage
      utilizationPercentage: 70
    ## proxyFrontend.autoscaling.customMetricsType Custom, additional or external autoscaling metrics for the Proxy Frontend deployment
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#autoscaling-on-multiple-metrics-and-custom-metrics
    customMetricsType: []
    ## proxyFrontend.autoscaling.behavior Configuring Proxy Frontend deployment scaling behavior policies for the `scaleDown` and `scaleUp` fields
    ## If not set the default values are used:
    ## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#default-behavior
    behavior: {}
  # Proxy Frontend initContainers parameters
  # proxyFrontend.initContainers Containers that run before Proxy Frontend container in a Pod
  # ref:https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
  # Example:
  # initContainers:
  #   - name: custom-init-container
  #     image: busybox:latest
  #     command: ['sh', '-c', 'sleep 180']
  initContainers: []
  # Proxy Frontend container image parameters
  image:
    # proxyFrontend.image.repository Proxy Frontend container image repository
    repository: nginxinc/nginx-unprivileged
    # proxyFrontend.image.tag Proxy Frontend container image tag
    tag: latest
    # proxyFrontend.image.pullPolicy Proxy Frontend container image pull policy
    pullPolicy: IfNotPresent
  # Configure a Security Context for containers in Proxy Frontend Pods
  # If set to, it takes priority over the `containerSecurityContext`
  containerSecurityContext:
    # proxyFrontend.containerSecurityContext.enabled Enable security context for containers in Proxy Frontend pods
    enabled: false
  # proxyFrontend.lifecycleHooks Defines the Proxy Frontend container lifecycle hooks
  # ref: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/
  # It is used to trigger events to run at certain points in a container's lifecycle
  # There are two hooks that are exposed: `PostStart` and `PreStop`
  lifecycleHooks: {}
  containerPorts:
    # proxyFrontend.containerPorts.http Proxy Frontend HTTP container port
    http: 8080
    # proxyFrontend.containerPorts.https Proxy Frontend HTTPS container port
    https: 8443
  # Probe used for the proxyFrontend container: startup, readiness and liveness probes
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # The parameters below for startup probes are used only when `proxyFrontend.startupProbe.enabled=true`
  startupProbe:
    # proxyFrontend.startupProbe.enabled Enable startupProbe for Proxy Frontend container
    enabled: true
    httpGet:
      # proxyFrontend.startupProbe.httpGet.path Checking the path for startupProbe
      path: /health
      # proxyFrontend.startupProbe.httpGet.port Checking the port for startupProbe
      port: 8080
    # proxyFrontend.startupProbe.failureThreshold Failure threshold for startupProbe
    failureThreshold: 30
    # proxyFrontend.startupProbe.periodSeconds Period seconds for startupProbe
    periodSeconds: 10
  # The parameters below for readiness probes are used only when `proxyFrontend.readinessProbe.enabled=true`
  readinessProbe:
    # proxyFrontend.readinessProbe.enabled Enable readinessProbe for Proxy Frontend container
    enabled: true
    # proxyFrontend.readinessProbe.failureThreshold Failure threshold for readinessProbe
    failureThreshold: 2
    httpGet:
      # proxyFrontend.readinessProbe.httpGet.path Checking the path for readinessProbe
      path: /health
      # proxyFrontend.readinessProbe.httpGet.port Checking the port for readinessProbe
      port: 8080
    # proxyFrontend.readinessProbe.periodSeconds Period seconds for readinessProbe
    periodSeconds: 20
    # proxyFrontend.readinessProbe.successThreshold Success threshold for readinessProbe
    successThreshold: 1
    # proxyFrontend.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    timeoutSeconds: 5
  # The parameters below for liveness probes are used only when `proxyFrontend.livenessProbe.enabled=true`
  livenessProbe:
    # proxyFrontend.livenessProbe.enabled Enable livenessProbe for Proxy Frontend container
    enabled: true
    # proxyFrontend.livenessProbe.failureThreshold Failure threshold for livenessProbe
    failureThreshold: 3
    httpGet:
      # proxyFrontend.livenessProbe.httpGet.path Checking the path for livenessProbe
      path: /health
      # proxyFrontend.livenessProbe.httpGet.port Checking the port for livenessProbe
      port: 8080
    # proxyFrontend.livenessProbe.periodSeconds Period seconds for livenessProbe
    periodSeconds: 20
    # proxyFrontend.livenessProbe.successThreshold Success threshold for livenessProbe
    successThreshold: 1
    # proxyFrontend.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    timeoutSeconds: 5
  # Proxy Frontend container resource requests and limits
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  # proxyFrontend.resources.requests The requested resources for the Proxy Frontend container
  # proxyFrontend.resources.limits The resources limits for the Proxy Frontend container
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "2Gi"
      cpu: "2000m"
  ## proxyFrontend.extraEnvVars An array with extra env variables for the Proxy Frontend container
  extraEnvVars: []
  ## proxyFrontend.extraVolumes An array with extra volumes for the Proxy Frontend container
  extraVolumes: []
  ## proxyFrontend.extraVolumeMounts An array with extra volume mounts for the Proxy Frontend container
  extraVolumeMounts: []
  # Additional configuration files for Proxy Frontend
  extraConf:
    confd:
      # proxyFrontend.extraConf.confd.configMap The name of the ConfigMap containing custom configuration files
      # These files will be map in the `/etc/nginx/conf.d/` directory of the container
      configMap: default-conf
      # proxyFrontend.extraConf.confd.fileName The names of the configuration files containing custom configuration files
      # Must be the same as the `key` names in `proxyFrontend.extraConf.confd.configMap`
      # May contain multiple values
      fileName:
        - default.conf
    customConfd:
      # proxyFrontend.extraConf.customConfd.configMap The name of the ConfigMap containing additional custom configuration files
      # These files will be map in the `/etc/nginx/conf.d/` directory of the container
      configMap: ""
      # proxyFrontend.extraConf.customConfd.fileName The names of the configuration files containing additional custom configuration files
      # Must be the same as the `key` names in `proxyFrontend.extraConf.customConfd.configMap`
      # May contain multiple values
      fileName:
        - example.conf
  # proxyFrontend.hostname The hostname (domainname) by which the DocSpace will be available
  hostname: ""
  # Proxy Frontend TLS parameters
  tls:
    # proxyFrontend.tls.secretName The name of the TLS secret containing the certificate and its associated key
    # If not set, the parameters below are ignored
    secretName: tls
    # proxyFrontend.tls.mountPath The path where the certificate and key will be mounted
    mountPath: /etc/nginx/ssl
    # proxyFrontend.tls.crtName Name of the key containing the certificate
    crtName: cert.crt
    # proxyFrontend.tls.keyName Name of the key containing the key
    keyName: cert.key
  # Proxy Frontend service parameters
  service:
    # proxyFrontend.service.existing The name of an existing service for Proxy Frontend. If not set, a service named `proxy-frontend` will be created
    # ref: https://github.com/ONLYOFFICE/Kubernetes-DocSpace/blob/master/templates/services/proxy-frontend.yaml
    existing: ""
    # proxyFrontend.service.annotations Map of annotations to add to the Proxy Frontend service
    annotations: {}
    port:
      # proxyFrontend.service.port.http Proxy Frontend http service port
      http: 80
      # proxyFrontend.service.port.https Proxy Frontend https service port
      https: 443
    # proxyFrontend.service.type Proxy Frontend service type
    type: LoadBalancer

# DocSpace ingress parameters
# NGINX Ingress Controller must be installed
# ref: https://github.com/ONLYOFFICE/Kubernetes-DocSpace#121-installing-the-kubernetes-nginx-ingress-controller
#
ingress:
  # ingress.enabled Enable the creation of an ingress for the DocSpace
  enabled: false
  # ingress.annotations Map of annotations to add to the Ingress
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: 100m
  # ingress.ingressClassName Used to reference the IngressClass that should be used to implement this Ingress
  # ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#the-ingress-resource/
  ingressClassName: nginx
  tls:
    # ingress.tls.enabled Enable TLS for the DocSpace
    enabled: false
    # ingress.tls.secretName Secret name for TLS to mount into the Ingress
    # Used only when `ingress.tls.enabled=true`
    secretName: tls
  # ingress.host Ingress hostname for the DocSpace
  host: ""

# DocSpace jobs parameters
#
# Job by install has a pre-install hook and executes before any resources are created in Kubernetes
# ref: https://helm.sh/docs/topics/charts_hooks/#the-available-hooks
# He creates tables, initializes the database in the `connections.mysqlDatabase` database and initializes the storage for DocSpace
install:
  job:
    # install.job.enabled Enable the execution of job pre-install before installing DocSpace
    enabled: true
    # install.job.annotations Defines annotations that will be additionally added to Install Job
    # If set to, it takes priority over the `commonAnnotations`
    # You can also use `tpl` as the value for the key
    annotations: {}
    # install.job.podAnnotations Map of annotations to add to the Install Job Pod
    podAnnotations: {}
    # Configure a Security Context for the Install Job Pod
    # If set to, it takes priority over the `podSecurityContext`
    podSecurityContext:
      # install.job.podSecurityContext.enabled Enable security context for the Install Job pod
      enabled: false
    # install.job.customPodAntiAffinity Prohibiting the scheduling of Install Job Pod relative to other Pods containing the specified labels on the same node
    customPodAntiAffinity: {}
    # Pod affinity rules for Install Job Pod scheduling by nodes relative to other Pods
    # Pod affinity allow you to constrain which nodes Install Job Pod can be scheduled on based on the labels of Pods already running on that node
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    # Example:
    # podAffinity:
    #   preferredDuringSchedulingIgnoredDuringExecution:
    #   - weight: 100
    #     podAffinityTerm:
    #       labelSelector:
    #         matchExpressions:
    #         - key: app
    #           operator: NotIn
    #           values:
    #           - database
    #       topologyKey: kubernetes.io/hostname
    podAffinity: {}
    # Node affinity rules for Install Job Pod scheduling by nodes
    # Node affinity allow you to constrain which nodes Install Job Pod can be scheduled on based on node labels
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    # Example:
    # nodeAffinity:
    #   preferredDuringSchedulingIgnoredDuringExecution:
    #   - weight: 100
    #     preference:
    #       matchExpressions:
    #       - key: kubernetes.io/name
    #         operator: In
    #         values:
    #         - name1
    #         - name2
    nodeAffinity: {}
    # install.job.nodeSelector Node labels for Install Job pod assignment
    # If set to, it takes priority over the `nodeSelector`
    nodeSelector: {}
    # install.job.tolerations Tolerations for Install Job pod assignment
    # If set to, it takes priority over the `tolerations`
    tolerations: []
    # Configure a Security Context for containers in Install Job Pod
    # If set to, it takes priority over the `containerSecurityContext`
    containerSecurityContext:
      # install.job.containerSecurityContext.enabled Enable security context for containers in Install Job pod
      enabled: false
    initContainers:
      migrationRunner:
        # install.job.initContainers.migrationRunner.enabled Enable database initialization
        enabled: true
        image:
          # install.job.initContainers.migrationRunner.image.repository Job by pre-install Migration Runner container image repository
          repository: onlyoffice/docspace-migration-runner
          # install.job.initContainers.migrationRunner.image.tag Job by pre-install Migration Runner container image tag
          # If set to, it takes priority over the `images.tag`
          tag: ""
          # install.job.initContainers.migrationRunner.image.pullPolicy Job by pre-install Migration Runner container image pull policy
          pullPolicy: IfNotPresent
        # Job pre-install Migration Runner container resource requests and limits
        # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        # install.job.initContainers.migrationRunner.resources.requests The requested resources for the Job pre-install Migration Runner container
        # install.job.initContainers.migrationRunner.resources.limits The resources limits for the Job pre-install Migration Runner container
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "1Gi"
            cpu: "1000m"

# Job by upgrade has a pre-upgrade hook and executes on an upgrade request before all of the release's resources have been upgraded
# ref: https://helm.sh/docs/topics/charts_hooks/#the-available-hooks
# He updates the `connections.mysqlDatabase` database and reinitializes the storage for DocSpace
upgrade:
  job:
    # upgrade.job.enabled Enable the execution of job pre-upgrade before upgrading DocSpace
    enabled: true
    # upgrade.job.annotations Defines annotations that will be additionally added to Upgrade Job
    # If set to, it takes priority over the `commonAnnotations`
    # You can also use `tpl` as the value for the key
    annotations: {}
    # upgrade.job.podAnnotations Map of annotations to add to the Upgrade Job Pod
    podAnnotations: {}
    # Configure a Security Context for the Upgrade Job Pod
    # If set to, it takes priority over the `podSecurityContext`
    podSecurityContext:
      # upgrade.job.podSecurityContext.enabled Enable security context for the Upgrade Job pod
      enabled: false
    # upgrade.job.customPodAntiAffinity Prohibiting the scheduling of Upgrade Job Pod relative to other Pods containing the specified labels on the same node
    customPodAntiAffinity: {}
    # Pod affinity rules for Upgrade Job Pod scheduling by nodes relative to other Pods
    # Pod affinity allow you to constrain which nodes Upgrade Job Pod can be scheduled on based on the labels of Pods already running on that node
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    # Example:
    # podAffinity:
    #   preferredDuringSchedulingIgnoredDuringExecution:
    #   - weight: 100
    #     podAffinityTerm:
    #       labelSelector:
    #         matchExpressions:
    #         - key: app
    #           operator: NotIn
    #           values:
    #           - database
    #       topologyKey: kubernetes.io/hostname
    podAffinity: {}
    # Node affinity rules for Upgrade Job Pod scheduling by nodes
    # Node affinity allow you to constrain which nodes Upgrade Job Pod can be scheduled on based on node labels
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    # Example:
    # nodeAffinity:
    #   preferredDuringSchedulingIgnoredDuringExecution:
    #   - weight: 100
    #     preference:
    #       matchExpressions:
    #       - key: kubernetes.io/name
    #         operator: In
    #         values:
    #         - name1
    #         - name2
    nodeAffinity: {}
    # upgrade.job.nodeSelector Node labels for Upgrade Job pod assignment
    # If set to, it takes priority over the `nodeSelector`
    nodeSelector: {}
    # upgrade.job.tolerations Tolerations for Upgrade Job pod assignment
    # If set to, it takes priority over the `tolerations`
    tolerations: []
    # Configure a Security Context for containers in Upgrade Job Pod
    # If set to, it takes priority over the `containerSecurityContext`
    containerSecurityContext:
      # upgrade.job.containerSecurityContext.enabled Enable security context for containers in Upgrade Job pod
      enabled: false
    initContainers:
      rootless:
        # upgrade.job.initContainers.rootless.enabled Enable the rootless initContainer to change file ownership
        enabled: true
        image:
          # upgrade.job.initContainers.rootless.image.repository Rootless initContainer image repository
          repository: onlyoffice/docs-utils
          # upgrade.job.initContainers.rootless.image.tag Rootless initContainer image tag
          # If set to, it takes priority over the `images.tag`
          tag: 8.2.2-1
          # upgrade.job.initContainers.rootless.image.pullPolicy Rootless initContainer image pull policy
          pullPolicy: IfNotPresent
        # Rootless initContainer resource requests and limits
        # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        # upgrade.job.initContainers.rootless.resources.requests The requested resources for the rootless initContainer
        # upgrade.job.initContainers.rootless.resources.limits The resources limits for the rootless initContainer
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        containerSecurityContext:
          # upgrade.job.initContainers.rootless.containerSecurityContext.enabled Enable security context for the rootless initContainer
          enabled: true
          # upgrade.job.initContainers.rootless.containerSecurityContext.runAsUser User ID for the DocSpace rootless initContainer
          # When performing a clean install of 3.0+ images or upgrading from version 3.0+ to a later version, you can specify uid=104
          runAsUser: 0
          # upgrade.job.initContainers.rootless.containerSecurityContext.runAsGroup Group ID for the DocSpace rootless initContainer
          # When performing a clean install of 3.0+ images or upgrading from version 3.0+ to a later version, you can specify gid=107
          runAsGroup: 0
          # upgrade.job.initContainers.rootless.containerSecurityContext.allowPrivilegeEscalation Controls whether a process can gain more privileges than its parent process
          allowPrivilegeEscalation: false
          # upgrade.job.initContainers.rootless.containerSecurityContext.seLinuxOptions Defines SELinux labels for the DocSpace rootless initContainer
          seLinuxOptions: {}
          # upgrade.job.initContainers.rootless.containerSecurityContext.seccompProfile Defines the Seccomp profile for the DocSpace rootless initContainer
          seccompProfile:
            type: RuntimeDefault
          # upgrade.job.initContainers.rootless.containerSecurityContext.capabilities Defines the privileges granted to the process
          capabilities:
            drop: ["ALL"]
      migrationRunner:
        # upgrade.job.initContainers.migrationRunner.enabled Enable database update
        enabled: true
        image:
          # upgrade.job.initContainers.migrationRunner.image.repository Job by pre-upgrade Migration Runner container image repository
          repository: onlyoffice/docspace-migration-runner
          # upgrade.job.initContainers.migrationRunner.image.tag Job by pre-upgrade Migration Runner container image tag
          # If set to, it takes priority over the `images.tag`
          tag: ""
          # upgrade.job.initContainers.migrationRunner.image.pullPolicy Job by pre-upgrade Migration Runner container image pull policy
          pullPolicy: IfNotPresent
        # Job pre-upgrade Migration Runner container resource requests and limits
        # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
        # upgrade.job.initContainers.migrationRunner.resources.requests The requested resources for the Job pre-upgrade Migration Runner container
        # upgrade.job.initContainers.migrationRunner.resources.limits The resources limits for the Job pre-upgrade Migration Runner container
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
    docsInitDB:
      # upgrade.job.docsInitDB.enabled Enable the Init DB for Docs container to create tables required for Docs to work
      # Set to `true` when upgrading to version `3.0.0` from earlier
      # When installing `3.0.0` and also when upgrading from version `3.0.0` to a later one, this parameter should be set to `false`
      enabled: false
      image:
        # upgrade.job.docsInitDB.image.repository Init DB for Docs container image repository
        repository: onlyoffice/docs-utils
        # upgrade.job.docsInitDB.image.tag Init DB for Docs container image tag
        tag: 8.2.2-1
        # upgrade.job.docsInitDB.image.pullPolicy Init DB for Docs container image pull policy
        pullPolicy: IfNotPresent
      # Init DB for Docs resource requests and limits
      # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
      # upgrade.job.docsInitDB.resources.requests The requested resources for the Init DB for Docs container
      # upgrade.job.docsInitDB.resources.limits The resources limits for the Init DB for Docs container
      resources:
        requests:
          memory: "256Mi"
          cpu: "100m"
        limits:
          memory: "1Gi"
          cpu: "1000m"

# Job by elasticsearchClearIndexes has a pre-upgrade hook and executes on an upgrade request before the Upgrade job is launched
# He checks if Elasticsearch is installed with this chart and if it is installed, it clears the table in the database and indexes in Elasticsearch
elasticsearchClearIndexes:
  job:
    # elasticsearchClearIndexes.job.enabled Enable the execution of job elasticsearchClearIndexes before upgrading DocSpace
    # If you are using Opensearch or an external Elasticsearch, you can set it to `false`
    enabled: true
    # Elasticsearch-clear-indexes container image parameters
    image:
      # elasticsearchClearIndexes.job.image.repository elasticsearchClearIndexes container image repository
      repository: onlyoffice/docs-utils
      # elasticsearchClearIndexes.job.image.tag elasticsearchClearIndexes container image tag
      tag: 8.2.2-1
      # elasticsearchClearIndexes.job.image.pullPolicy elasticsearchClearIndexes container image pull policy
      pullPolicy: IfNotPresent
    # elasticsearchClearIndexes.job.annotations Defines annotations that will be additionally added to elasticsearchClearIndexes Job
    # If set to, it takes priority over the `commonAnnotations`
    # You can also use `tpl` as the value for the key
    annotations: {}
    # elasticsearchClearIndexes.job.podAnnotations Map of annotations to add to the elasticsearchClearIndexes Job Pod
    podAnnotations: {}
    # Configure a Security Context for the elasticsearchClearIndexes Job Pod
    # If set to, it takes priority over the `podSecurityContext`
    podSecurityContext:
      # elasticsearchClearIndexes.job.podSecurityContext.enabled Enable security context for the elasticsearchClearIndexes Job pod
      enabled: false
    # elasticsearchClearIndexes.job.customPodAntiAffinity Prohibiting the scheduling of elasticsearchClearIndexes Job Pod relative to other Pods containing the specified labels on the same node
    customPodAntiAffinity: {}
    # Pod affinity rules for elasticsearchClearIndexes Job Pod scheduling by nodes relative to other Pods
    # Pod affinity allow you to constrain which nodes elasticsearchClearIndexes Job Pod can be scheduled on based on the labels of Pods already running on that node
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
    podAffinity: {}
    # Node affinity rules for elasticsearchClearIndexes Job Pod scheduling by nodes
    # Node affinity allow you to constrain which nodes elasticsearchClearIndexes Job Pod can be scheduled on based on node labels
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
    nodeAffinity: {}
    # elasticsearchClearIndexes.job.nodeSelector Node labels for elasticsearchClearIndexes Job pod assignment
    # If set to, it takes priority over the `nodeSelector`
    nodeSelector: {}
    # elasticsearchClearIndexes.job.tolerations Tolerations for elasticsearchClearIndexes Job pod assignment
    # If set to, it takes priority over the `tolerations`
    tolerations: []
    # Configure a Security Context for containers in elasticsearchClearIndexes Job Pod
    # If set to, it takes priority over the `containerSecurityContext`
    containerSecurityContext:
      # elasticsearchClearIndexes.job.containerSecurityContext.enabled Enable security context for containers in elasticsearchClearIndexes Job pod
      enabled: false
    # Job elasticsearchClearIndexes container resource requests and limits
    # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    # elasticsearchClearIndexes.job.resources.requests The requested resources for the Job elasticsearchClearIndexes container
    # elasticsearchClearIndexes.job.resources.limits The resources limits for the Job elasticsearchClearIndexes container
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "1Gi"
        cpu: "1000m"

# DocSpace Opensearch StatefulSet parameters
#
opensearch:
  # opensearch.enabled Enables Opensearch installation
  enabled: true
  # opensearch.annotations Defines annotations that will be additionally added to Opensearch deploy
  # If set to, it takes priority over the `commonAnnotations`
  # You can also use `tpl` as the value for the key
  annotations: {}
  # Update strategy used to replace old Pod by new ones
  updateStrategy:
    # opensearch.updateStrategy.type Opensearch update strategy type
    # Allowed values: `RollingUpdate` or `OnDelete`
    type: RollingUpdate
  # opensearch.podAnnotations Map of annotations to add to the Opensearch pod
  podAnnotations: {}
  # Configure a Security Context for the Opensearch Pod
  podSecurityContext:
    # opensearch.podSecurityContext.enabled Enable security context for the Opensearch Pod
    enabled: false
    # opensearch.podSecurityContext.fsGroup Defines the Group ID to which the owner and permissions for all files in volumes are changed when mounted in the Opensearch Pod
    fsGroup: 1000
  # opensearch.customPodAntiAffinity Prohibiting the scheduling of Opensearch Pod relative to other Pods containing the specified labels on the same node
  customPodAntiAffinity: {}
  # Pod affinity rules for Opensearch Pod scheduling by nodes relative to other Pods
  # Pod affinity allow you to constrain which nodes Opensearch Pod can be scheduled on based on the labels of Pods already running on that node
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  # Example:
  # podAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     podAffinityTerm:
  #       labelSelector:
  #         matchExpressions:
  #         - key: app
  #           operator: NotIn
  #           values:
  #           - database
  #       topologyKey: kubernetes.io/hostname
  podAffinity: {}
  # Node affinity rules for Opensearch Pod scheduling by nodes
  # Node affinity allow you to constrain which nodes Opensearch Pod can be scheduled on based on node labels
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  # Example:
  # nodeAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 100
  #     preference:
  #       matchExpressions:
  #       - key: kubernetes.io/name
  #         operator: In
  #         values:
  #         - name1
  #         - name2
  nodeAffinity: {}
  # opensearch.nodeSelector Node labels for Opensearch pod assignment
  # If set to, it takes priority over the `nodeSelector`
  nodeSelector: {}
  # opensearch.tolerations Tolerations for Opensearch pod assignment
  # If set to, it takes priority over the `tolerations`
  tolerations: []
  initContainers:
    # Enable security context for Opensearch change-volume-owner initContainer container in pod
    changeVolumeOwner:
      enabled: true
      # Change volume owner container image parameters
      image:
        # opensearch.initContainers.changeVolumeOwner.image.repository Change volume owner container image repository
        repository: onlyoffice/docs-utils
        # opensearch.initContainers.changeVolumeOwner.image.tag Change volume owner container image tag
        tag: 8.2.2-1
        # opensearch.initContainers.changeVolumeOwner.image.pullPolicy Change volume owner container image pull policy
        pullPolicy: IfNotPresent
      # Configure a Security Context for Opensearch change-volume-owner initContainer container in Pod
      securityContext:
        # opensearch.initContainers.changeVolumeOwner.securityContext.enabled Enable security context for Opensearch change-volume-owner initContainer container in pod
        enabled: true
        # opensearch.initContainers.changeVolumeOwner.securityContext.runAsUser User ID for the Opensearch change-volume-owner initContainer container
        runAsUser: 0
        # opensearch.initContainers.changeVolumeOwner.securityContext.allowPrivilegeEscalation Controls whether a process can gain more privileges than its parent process
        allowPrivilegeEscalation: false
        # opensearch.initContainers.changeVolumeOwner.securityContext.seLinuxOptions Defines SELinux labels for the Opensearch change-volume-owner initContainer container
        seLinuxOptions: {}
        # opensearch.initContainers.changeVolumeOwner.securityContext.seccompProfile Defines the Seccomp profile for the Opensearch change-volume-owner initContainer container
        seccompProfile:
          type: RuntimeDefault
        # opensearch.initContainers.changeVolumeOwner.securityContext.capabilities Defines the privileges granted to the process
        capabilities:
          drop: ["ALL"]
      # Opensearch change-volume-owner initContainer resource requests and limits
      # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
      # opensearch.initContainers.changeVolumeOwner.resources.requests The requested resources for the Opensearch change-volume-owner initContainer
      # opensearch.initContainers.changeVolumeOwner.resources.limits The resources limits for the Opensearch change-volume-owner initContainer
      resources:
        requests:
          memory: "256Mi"
          cpu: "100m"
        limits:
          memory: "1Gi"
          cpu: "1000m"
    # opensearch.initContainers.custom Custom Opensearch initContainers parameters
    # Additional containers that run before Opensearch container in a Pod
    # Example:
    # custom:
    #   - name: additional-init-container
    #     image: busybox:latest
    #     command: ['sh', '-c', 'sleep 180']
    custom: []
  # Opensearch container image parameters
  image:
    # opensearch.image.repository Opensearch container image repository
    repository: onlyoffice/opensearch
    # opensearch.image.tag Opensearch container image tag
    tag: 2.11.1
    # opensearch.image.pullPolicy Opensearch container image pull policy
    pullPolicy: IfNotPresent
  # Configure a Security Context for Opensearch container in Pod
  containerSecurityContext:
    # opensearch.containerSecurityContext.enabled Enable security context for Opensearch container in pod
    enabled: false
    # opensearch.containerSecurityContext.runAsUser User ID for the Opensearch container
    runAsUser: 1000
    # opensearch.containerSecurityContext.runAsGroup Group ID for the Opensearch container
    runAsGroup: 1000
    # opensearch.containerSecurityContext.runAsNonRoot Require that the container will run with a user with UID other than 0
    runAsNonRoot: true
    # opensearch.containerSecurityContext.allowPrivilegeEscalation Controls whether a process can gain more privileges than its parent process
    allowPrivilegeEscalation: false
    # opensearch.containerSecurityContext.seLinuxOptions Defines SELinux labels for the Opensearch container
    seLinuxOptions: {}
    # opensearch.containerSecurityContext.seccompProfile Defines the Seccomp profile for the Opensearch container
    seccompProfile:
      type: RuntimeDefault
    # opensearch.containerSecurityContext.capabilities Defines the privileges granted to the process
    capabilities:
      drop: ["ALL"]
  # Probe used for the Opensearch container: startup, readiness and liveness probes
  # ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
  # The parameters below for startup probes are used only when `opensearch.startupProbe.enabled=true`
  startupProbe:
    # opensearch.startupProbe.enabled Enable startupProbe for Opensearch container
    enabled: true
    tcpSocket:
      # opensearch.startupProbe.tcpSocket.port Checking the port for startupProbe
      port: 9200
    # opensearch.startupProbe.failureThreshold Failure threshold for startupProbe
    failureThreshold: 30
    # opensearch.startupProbe.periodSeconds Period seconds for startupProbe
    periodSeconds: 10
  # The parameters below for readiness probes are used only when `opensearch.readinessProbe.enabled=true`
  readinessProbe:
    # opensearch.readinessProbe.enabled Enable readinessProbe for Opensearch container
    enabled: true
    # opensearch.readinessProbe.failureThreshold Failure threshold for readinessProbe
    failureThreshold: 2
    tcpSocket:
      # opensearch.readinessProbe.tcpSocket.port Checking the port for readinessProbe
      port: 9200
    # opensearch.readinessProbe.periodSeconds Period seconds for readinessProbe
    periodSeconds: 20
    # opensearch.readinessProbe.successThreshold Success threshold for readinessProbe
    successThreshold: 1
    # opensearch.readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
    timeoutSeconds: 5
  # The parameters below for liveness probes are used only when `opensearch.livenessProbe.enabled=true`
  livenessProbe:
    # opensearch.livenessProbe.enabled Enable livenessProbe for Opensearch container
    enabled: true
    # opensearch.livenessProbe.failureThreshold Failure threshold for livenessProbe
    failureThreshold: 3
    tcpSocket:
      # opensearch.livenessProbe.tcpSocket.port Checking the port for livenessProbe
      port: 9200
    # opensearch.livenessProbe.periodSeconds Period seconds for livenessProbe
    periodSeconds: 20
    # opensearch.livenessProbe.successThreshold Success threshold for livenessProbe
    successThreshold: 1
    # opensearch.livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
    timeoutSeconds: 5
  # Opensearch container resource requests and limits
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  # opensearch.resources.requests The requested resources for the opensearch container
  # opensearch.resources.limits The resources limits for the opensearch container
  resources:
    requests:
      memory: "512Mi"
      cpu: "500m"
    limits:
      memory: "4Gi"
      cpu: "4000m"
  ## opensearch.extraEnvVars An array with extra env variables for the opensearch container
  extraEnvVars: []
  ## opensearch.extraVolumes An array with extra volumes for the opensearch container
  extraVolumes: []
  ## opensearch.extraVolumeMounts An array with extra volume mounts for the opensearch container
  extraVolumeMounts: []
  env:
    # opensearch.env.discoveryType determines the cluster discovery type. Set to "single-node" for a single-node cluster
    discoveryType: "single-node"
    # opensearch.env.disableSecurityPlugin disables the security plugin
    disableSecurityPlugin: true
    # opensearch.env.disableInstallDemoConfig disables the installation of demo configuration
    disableInstallDemoConfig: true
    # opensearch.env.bootstrapMemoryLock determines whether JVM should lock memory
    bootstrapMemoryLock: true
    # opensearch.env.ESJAVAOPTS defines JVM options
    ESJAVAOPTS: "-Xms2g -Xmx2g -Dlog4j2.formatMsgNoLookups=true"
    # opensearch.env.indicesFieldDataCacheSize sets the size of the index field data cache
    indicesFieldDataCacheSize: "30%"
    # opensearch.env.indicesMemoryIndexBufferSize sets the size of the in-memory index buffer
    indicesMemoryIndexBufferSize: "30%"
  persistence:
    # opensearch.persistence.annotations Defines annotations that will be additionally added to Opensearch PVC
    # If set to, it takes priority over the `commonAnnotations`
    # You can also use `tpl` as the value for the key
    annotations: {}
    # opensearch.persistence.storageClass PVC Storage Class for Opensearch volume
    storageClass: "nfs"
    # opensearch.persistence.accessModes Opensearch Persistent Volume access modes
    accessModes:
      - ReadWriteOnce
    # opensearch.persistence.size PVC Storage Request for Opensearch volume
    size: 30Gi

# DocSpace tests parameters
#
tests:
  # tests.enabled Enable the resources creation necessary for DocSpace launch testing and connected dependencies availability testing
  # These resources will be used when running the `helm test` command
  enabled: true
  # tests.annotations Defines annotations that will be additionally added to Test Pod
  # If set to, it takes priority over the `commonAnnotations`
  # You can also use `tpl` as the value for the key
  annotations: {}
  # Configure a Security Context for the Test Pod
  podSecurityContext:
    # tests.podSecurityContext.enabled Enable security context for the Test pod
    enabled: false
    # tests.podSecurityContext.fsGroup Defines the Group ID to which the owner and permissions for all files in volumes are changed when mounted in the Test Pod
    fsGroup: 101
  # tests.customPodAntiAffinity Prohibiting the scheduling of Test Pod relative to other Pods containing the specified labels on the same node
  customPodAntiAffinity: {}
  # Pod affinity rules for Test Pod scheduling by nodes relative to other Pods
  # Pod affinity allow you to constrain which nodes Test Pod can be scheduled on based on the labels of Pods already running on that node
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  podAffinity: {}
  # Node affinity rules for Test Pod scheduling by nodes
  # Node affinity allow you to constrain which nodes Test Pod can be scheduled on based on node labels
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  nodeAffinity: {}
  # tests.nodeSelector Node labels for Test pod assignment
  # If set to, it takes priority over the `nodeSelector`
  nodeSelector: {}
  # tests.tolerations Tolerations for Test pod assignment
  # If set to, it takes priority over the `tolerations`
  tolerations: []
  # Test container image parameters
  image:
    # tests.image.repository test container image name
    repository: onlyoffice/docs-utils
    # tests.image.tag test container image tag
    tag: 8.2.2-1
    # tests.image.pullPolicy test container image pull policy
    pullPolicy: IfNotPresent
  # Configure a Security Context for the Test container
  containerSecurityContext:
    # tests.containerSecurityContext.enabled Enable security context for the Test container
    enabled: false
    # tests.containerSecurityContext.runAsUser User ID for the Test container
    runAsUser: 101
    # tests.containerSecurityContext.runAsGroup Group ID for the Test container
    runAsGroup: 101
    # tests.containerSecurityContext.runAsNonRoot Require that the container will run with a user with UID other than 0
    runAsNonRoot: true
    # tests.containerSecurityContext.allowPrivilegeEscalation Controls whether a process can gain more privileges than its parent process
    allowPrivilegeEscalation: false
    # tests.containerSecurityContext.seccompProfile Defines the Seccomp profile for the Test container
    seccompProfile:
      type: RuntimeDefault
    # tests.containerSecurityContext.capabilities Defines the privileges granted to the process
    capabilities:
      drop: ["ALL"]
  # test container resource requests and limits
  # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  # tests.resources.requests The requested resources for the test container
  # tests.resources.limits The resources limits for the test container
  resources:
    requests:
      memory: "256Mi"
      cpu: "200m"
    limits:
      memory: "1Gi"
      cpu: "1000m"
